<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>KLUE - Relation Extraction</title>
    <link href="/KLUE_RE/"/>
    <url>/KLUE_RE/</url>
    
    <content type="html"><![CDATA[<h1 id="청계산셰르파의-KLUE-RE-등반일지"><a href="#청계산셰르파의-KLUE-RE-등반일지" class="headerlink" title="청계산셰르파의 KLUE-RE 등반일지"></a>청계산셰르파의 KLUE-RE 등반일지</h1><p>약 12일동안 참여했던 P Stage의 <code>KLUE - Relation Extraction</code> 대회의 Wrap up Report이자 회고록입니다. 철저한 <code>기록</code>과 <code>공유</code>에 공감한 7명의 팀원이 처음 합을 맞추었기 때문에 어설픈 점도 있었지만, 그간의 생생한 기록들을 바탕으로 어떻게 서로가 서로의 <code>셰르파</code>로써 등반을 완료할 수 있었는지 저희의 경험을 나누고자 합니다.</p><p>물론 미흡한 점도 있었고, 여러 시행착오를 겪었지만 결과적으로는 팀원 모두가 만족한 협업이었다는 점에서 지난 대회보다 많은 성장을 할 수 있었다고 느낄 수 있었습니다. 저희의 경험이 어떠한 형태로든 도움이 되고, 좋은 레퍼런스가 되기를 바라며 시작해보도록 하겠습니다.</p><br><p><strong>목차</strong></p><ul><li><a href="#%E2%9A%94%EF%B8%8F-%ED%8C%80-%EC%86%8C%EA%B0%9C">팀소개</a><ul><li><a href="#%F0%9F%A4%9C-%EB%B6%80%EB%A1%9D-%ED%8C%80%EC%9B%90%EB%93%A4-%ED%95%9C%EB%A7%88%EB%94%94">청계산셰르파</a></li><li><a href="#%F0%9F%91%A8%E2%80%8D%F0%9F%91%A8%E2%80%8D%F0%9F%91%A6%E2%80%8D%F0%9F%91%A6-%EC%B2%AD%EA%B3%84%EC%82%B0%EC%85%B0%EB%A5%B4%ED%8C%8C%EB%93%A4">청계산셰르파들</a></li></ul></li><li><a href="#%F0%9F%94%8E-%EB%8C%80%ED%9A%8C-%EA%B0%9C%EC%9A%94">대회개요</a></li><li><a href="#%F0%9F%A4%9D-%ED%98%91%EC%97%85">협업</a><ul><li><a href="#%F0%9F%A5%BE-%EC%82%AC%EC%A0%84%EB%85%BC%EC%9D%98">사전논의</a></li><li><a href="#%E2%9A%99%EF%B8%8F-%ED%98%91%EC%97%85%ED%88%B4">협업툴</a></li><li><a href="#%F0%9F%92%BB-%EC%BD%94%EB%93%9C-%EA%B4%80%EB%A6%AC">코드관리</a></li><li><a href="#%F0%9F%A7%91%E2%80%8D%F0%9F%94%AC-%EC%B2%B4%EA%B3%84%EC%A0%81%EC%9D%B8-%EC%8B%A4%ED%97%98">체계적인 실험</a></li></ul></li><li><a href="#%F0%9F%9B%8B-Data-Experiments">Data Experiments</a><ul><li><a href="#%F0%9F%91%81-Data-EDA">Data EDA</a></li><li><a href="#%F0%9F%92%AA-Data-Augmentation">Data Augmentation</a></li><li><a href="#%F0%9F%94%A7-Data-Preprocessing-amp-Tokenizer">Data Preprocessing</a></li></ul></li><li><a href="#%F0%9F%A7%98-Modeling">Modeling</a></li><li><a href="#%F0%9F%8D%9C-Ensemble">Ensemble</a><ul><li><a href="#%F0%9F%91%A8%E2%80%8D%F0%9F%8E%A8-%EC%95%99%EC%83%81%EB%B8%94-%EA%B9%8E%EB%8A%94-%EB%85%B8%EC%9D%B8%EA%B3%BC-%EA%B8%B0%EB%8F%84%EB%A9%94%ED%83%80">앙상블 깎는 노인과 기도메타</a></li></ul></li><li><a href="#%F0%9F%92%AF-Good-Practice">Good Practice</a></li><li><a href="#%F0%9F%92%8C-Thanks-to">Thanks to</a></li><li><a href="#%EB%A7%88%EC%A7%80%EB%A7%89%EC%9C%BC%EB%A1%9C">마지막으로</a></li><li><a href="#%F0%9F%A4%9C-%EB%B6%80%EB%A1%9D-%ED%8C%80%EC%9B%90%EB%93%A4-%ED%95%9C%EB%A7%88%EB%94%94">부록: 팀원들 한마디</a></li></ul><h2 id="⚔️-팀-소개"><a href="#⚔️-팀-소개" class="headerlink" title="⚔️ 팀 소개"></a>⚔️ 팀 소개</h2><h3 id="⛰-청계산셰르파"><a href="#⛰-청계산셰르파" class="headerlink" title="⛰ 청계산셰르파"></a>⛰ 청계산셰르파</h3><p align="center">    <img src="/images/profile.png" style="display: inline" height="120px">    <img src="/images/2.png" style="display: inline" height="120px"></p><p>저희는 캠프 기간동안 모든 것을 생생하게 기억하고 나누는 <code>기록</code>과 <code>공유</code>라는 가치에 공감한 7명이 모여 팀을 구성했고, 서로가 서로의 가이드로서 좋은 영향을 주고받을 수 있는 셰르파가 되기를 원했습니다.</p><p>Naver에서 가장 가까운 산은 청계산입니다. 따라서 저희는 청계산을 등반하며 생생하게 기록한 <code>등반일지</code>가 저희를 모일 수 있게 도와준 네이버에 닿을 수 있도록, <code>청계산셰르파</code>라는 이름을 사용하게 되었습니다.</p><h3 id="👨‍👨‍👦‍👦-청계산셰르파들"><a href="#👨‍👨‍👦‍👦-청계산셰르파들" class="headerlink" title="👨‍👨‍👦‍👦 청계산셰르파들"></a>👨‍👨‍👦‍👦 청계산셰르파들</h3><ul><li>이요한</li><li>문하겸</li><li>전준영</li><li>정진원</li><li>김민수</li><li>정희영</li><li>곽진성</li></ul><h2 id="🔎-대회-개요"><a href="#🔎-대회-개요" class="headerlink" title="🔎 대회 개요"></a>🔎 대회 개요</h2><p>이번에 참여한 대회의 과제는 문장 내 개체간 관계 추출 과제로, 문장의 단어(Entity)에 대한 속성과 관계를 예측하는 자연어 처리 과제입니다. 관계 추출은 비구조적인 자연어 문장에서 구조화된 정보를 추출하는 데에 목적을 두고 있습니다.</p><p>저희가 사용한 데이터셋은 <a href="https://arxiv.org/pdf/2105.09680.pdf">「KLUE: Korean Language Understanding Evaluation」(Park et al., 2021)</a>을 통해 공개된 KLUE-RE 데이터셋으로, KLUE 데이터셋을 사전학습한 RoBERTa를 fine-tuning하여 베이스모델로 사용하였으며, private leaderboard에서 f1 score 기준 19팀 중 5등, auprc 기준 19팀 중 2등을 기록하였습니다.</p><p>🥈 Final Score<br><img src="/images/final.png"></p><ul><li>micro F1 score: 73.732 (19팀 중 <strong>5등</strong>)</li><li>AUPRC score: 82.964 (19팀 중 <strong>2등</strong>)</li></ul><p><img src="/images/kakaotalk.png" alt="묘비아님"></p><h2 id="🤝-협업"><a href="#🤝-협업" class="headerlink" title="🤝 협업"></a>🤝 협업</h2><h3 id="🥾-사전논의"><a href="#🥾-사전논의" class="headerlink" title="🥾 사전논의"></a>🥾 사전논의</h3><p>RE 대회가 시작하기 1주일 전부터, 저희는 미리 대회를 위한 전략들을 구상했습니다. 서로의 스타일을 모르기에 당장은 결정이 안나더라도, 세부적인 사항들에 대한 논의를 단 한번이라도 거쳤던 것은 추후에 의사결정속도와 팀원들의 만족에 있어서 큰 영향을 끼쳤습니다.</p><details>    <summary>    대회 전 논의사항들    </summary><br><ul><li><p>프로젝트관리 툴/채널 정하기</p><ul><li>ex) Github project의 kanban board, notion, slack, zoom, google meet, git page, github action 등등</li><li>카카오톡 (슬랙보다 많이 접함.) 슬랙처럼 스레드 형식의 대화를 할 수 없음.<ul><li>중요한 이슈가 생기면 슬랙에도 같이 이야기하자.</li></ul></li></ul></li><li><p>코드명세서 or 컨벤션</p><ul><li>naming<ul><li>클래스, Static Vars = CamelCase</li><li>변수명, 함수명 = snake_case</li></ul></li><li>formatting (&amp; auto formatter)<ul><li>autopep8</li><li>black, yap…</li></ul></li><li>annotation<ul><li>‘’’ docstring ‘’’</li><li>VSCODE CODE_ANCHORE<ul><li>TODO, NOTE,</li></ul></li><li>docstring을 알아서 해주는 게 있는지 서치해보기!</li><li>필요한 내용만 작성할 수 있도록 룰을 추가로 정할 것.<br>주석은 영문으로!!</li></ul></li><li><del>indentation (tab 1 or space 4)</del></li><li>그 외 vscode extensions<ul><li>git graph, git lens</li><li>Live share</li></ul></li></ul></li><li><p>가상환경이나 환경관리 전략 (zsh, dotenv, … conda, pip …)</p><ul><li><strong>conda 사용!</strong></li></ul></li><li><p><strong>미정</strong></p><ul><li>브랜치를 어떻게 만들어놓을것인가 (실험전략까지 고려)<br>  다른사람들이 checkout만으로 동일한 실험을 할 수 있게 하기 위함<br>  MAIN<ul><li>DEVELOP<pre><code>  유저편의성, 코드기능개선, 버그픽스  접두어/feature</code></pre></li><li>BASELINE<br>  접두어/feature x 실험 o<br>다른 태스크에 대한 것들이나 (실험위주)<br>  baseline/qa/1<br>  baseline/ner<br>  baseline/sentence classification</li></ul></li><li>너무 브랜치가 많아질 수 있다.<ul><li>config를 변경하면 브랜치를 분기하지 않을 수 있다.</li></ul></li></ul></li><li><p>(선택사항) 여유가 되면 자세하게 써놓기</p></li><li><p>커밋전략</p><ul><li>commit message 규칙이나 템플릿 정하기</li></ul></li><li><p>(작업단위와 리뷰에 대해서 더 생각해보기)</p></li><li><p>PR 템플릿 및 PR/Review 전략 구체화</p><ul><li>PR 올리는 타이밍(시점)<ul><li>성능향상에 의한 PR은 모두가 리뷰</li><li>자잘한 변화들은 책임자만 리뷰</li></ul></li><li>리뷰를 어떻게 할 건지? 리뷰어는 몇 명</li><li>속도? Merge속도에 대해서 데드라인이 있었음 좋겠다.</li></ul></li><li><p>그라운드룰 및 빠른 의사결정을 위한 협업가치 리스트업 및 우선순위 정하기</p><ul><li>ex) 안정성(예외처리), 구성원의만족, 가독성, 일관성, 객체지향성, 단순성, 외부유저의경험, 신속성(작업속도), 통제가능성, 학습가능성, 취업적용가능성, 실험가능성 등등</li></ul></li></ul></details><p>또한 이러한 사전논의를 통해 대회가 진행될수록 팀원들이 무엇을 원하는지 확실히 알 수 있었으며, 모두가 같은 그림을 그리고 같은 방향을 가지고 대회에 임할 수 있었습니다.</p><p>이러한 노력을 토대로 저희는 빈틈없이 기록을 이어가고, 실험을 공유하며 항상 새로운 실험아이디어를 얻을 수 있었습니다. 그 결과 하루에 10번씩 제출가능한 대회에서 12일 동안 다른 팀들 대비 압도적인 횟수인 <em><strong>99번의 실험결과를 제출</strong></em> 해볼 수 있었습니다.</p><h3 id="⚙️-협업툴"><a href="#⚙️-협업툴" class="headerlink" title="⚙️ 협업툴"></a>⚙️ 협업툴</h3><p>지난 대회 때 다양한 협업 채널을 구성하였을 때 오히려 혼란이 가중되고 모든 채널을 사용하기 힘들다는 점을 고려하여, 대부분의 협업을 <code>Notion</code>으로 진행했습니다. 또한 불가피한 상황에 대비한 연락수단으로 <code>kakaotalk</code>과 <code>slack</code>을 사용하였고, <code>git</code>은 사용가능한 기능을 최소한으로 하고 코드관리 수단으로만 사용하였습니다.</p><p><code>Notion</code>에서 일정관리, 문서관리, 실험관리 등 많은 기능을 사용하기 위해 엄밀하게 Template을 찾아보며 Dashboard를 구성하였고 그렇게 아래와 같은 페이지를 구성할 수 있었습니다.</p><p align="center">    <img src="/images/notion_main.png" style="display: inline" height="220px" width="32%">    <img src="/images/notion_kanban.png" style="display: inline" height="220px" width="32%">    <img src="/images/notion_schedule.png" style="display: inline" height="220px" width="32%"></p><p><strong>Main Dashboard</strong></p><ul><li><p>sub page들의 link와 zoom, wandb, github, drive 링크들</p></li><li><p>TODO와 실험관리를 위한 kanban 보드</p><ul><li><p>실험을 위한 Process별 Tag 부착</p>  <img src="/images/no_tag.png" height="30%" width="30%"></li><li><p>Assignee 할당</p></li></ul></li><li><p>일정관리를 위한 schedule</p><ul><li>알림기능 활성화</li><li>용도별 Tag 부착</li></ul></li><li><p>Reference와 Docs 링크들</p><ul><li>회의록</li><li>멘토링</li><li>연구일지 등</li></ul></li></ul><h3 id="💻-코드-관리"><a href="#💻-코드-관리" class="headerlink" title="💻 코드 관리"></a>💻 코드 관리</h3><p><img src="/images/git1.png"></p><p>초반에는 전체 코드를 관리하기 위해 PR-Merge 방법으로 진행하다가 Review가 늦어지거나, 작업시간이 오래걸리면 다른 팀원이 같은 작업을 하는 등 예상치 못한 병목이 발생하고 오히려 개인 실험에 방해요소로 작용할 수 있다는 판단을 하게 되었습니다.</p><style>  .linear_highlight {      background: linear-gradient(to top, #778899 10%, transparent 10%);  }</style><span class="linear_highlight">따라서 baseline으로 사용할 수 있는 코드에서 각자의 이름 혹은 실험 이름으로 분기를 나누어 개인 작업을 진행하면서, 사전에 논의했던 것처럼 score가 올랐을 경우에만 baseline 코드에 PR-Merge를 하고, 해당 score를 재현가능할 수 있게끔 버전업하기로 했습니다.</span><p><img src="/images/bran.png"></p><p>branch가 많아지긴 했지만, 실험에 실패했을 때 빠르게 Rollback할 수 있었고, 각자의 실험에서 확실한 성능향상 요소만을 합칠 수 있었습니다. Competition이라는 플랫폼의 특성상 7명의 팀원이 각자 작성한 모든 코드들을 Review하고 합치면서 작업을 이어나가기에는 많은 시간과 노력을 필요로 했습니다. 하지만 기준을 두고 필요할 때만 코드를 병합하니 실험은 실험대로 잘 이루어지고, 실험에 실패하더라도 가장 최신버전의 코드를 모두가 사용할 수 있었다는 점에서 많은 이점을 얻을 수 있었습니다.</p><h3 id="🧑‍🔬-체계적인-실험"><a href="#🧑‍🔬-체계적인-실험" class="headerlink" title="🧑‍🔬 체계적인 실험"></a>🧑‍🔬 체계적인 실험</h3><p><img src="/images/notion_kanban.png"></p><p>저희는 Kanban board를 사용하여 실험을 관리하였습니다.</p><p align="center">    <img src="/images/exp_tag.png" height="30%" width="30%"></p><p>Backlog, TO-DO, In progress, Completed 네 단계로 나누어 서로가 어떤 실험을 진행하고 있는지, 어떤 실험을 해야하는지 파악할 수 있게 하였으며 실험이 끝날 때마다 그때그때 갱신하는 작업을 진행하였습니다.</p><p>각각의 카테고리 별 책임은 이렇습니다.</p><ul><li>Backlog: 단순 실험 아이디어 및 건의사항, 수정사항</li><li>TO-DO: 꼭 적용해봐야 하는 실험</li><li>In progress: 현재 진행중인 실험</li><li>Completed: 완료된 실험</li></ul><p>Backlog에 진행해보고 싶은 실험카드가 생겼거나, 다른 실험 아이디어가 생긴 경우에는 Notion의 <code>Comment</code> 기능을 이용하여 이미 진행중인 실험이면 해당 실험의 진행상황이나 주의사항들을 더 자세하게 공유할 수 있게 하였습니다.</p><p align="center">    <img src="/images/notion_comm2.png">    <img src="/images/notion_comm.png"></p><p>실험카드가 <code>Completed</code>로 이동하게 되면 아래와 같이 실험기록표에 결과를 작성하고, 실험의 성공여부와 관계없이 그 실험에 대한 평가와 그런 결과가 나온 이유 혹은 주의사항 등을 기록하게 하였습니다.</p><p align="center">    <img src="/images/exp1.png">    <img src="/images/notion_exp.png"></p><span class="linear_highlight">이러한 시도는 팀원들 간 기술부채를 최대한 줄어들게 하였고 실패한 실험을 반복적으로 하지 않을 수 있게 하여 효율적으로 실험을 계획할 수 있게 해주었습니다.</span><h2 id="🛋-Data-Experiments"><a href="#🛋-Data-Experiments" class="headerlink" title="🛋 Data Experiments"></a>🛋 Data Experiments</h2><h3 id="👁-Data-EDA"><a href="#👁-Data-EDA" class="headerlink" title="👁 Data EDA"></a>👁 Data EDA</h3><p><img src="/images/eda.png"></p><p>데이터는 위와 같이 매우 불균형하게 분포되어 있었고, 9,000개가 넘는 no_relation과 달리 <code>per:place_of_death</code>처럼 약 40개 정도만 존재하는 label도 있었습니다.</p><p>이렇게 극단적인 Data Imbalancing을 잘 잡는 것이 이번 대회의 핵심이었습니다.</p><p><img src="/images/dup.png" alt="김채은 캠퍼님의 토론게시판 글 중"></p><p>또한 sentence와 subject_entity, object_entity까지 전부 동일한 문장이 53개가 있는 등 중복된 데이터와 mislabeled 데이터들이 존재하였고, 이것들을 전부 제거하고 수정하여 데이터셋을 재구성하였습니다.</p><h3 id="💪-Data-Augmentation"><a href="#💪-Data-Augmentation" class="headerlink" title="💪 Data Augmentation"></a>💪 Data Augmentation</h3><p>이후에는 Data Imbalancing을 해결하기 위하여 여러 Augmentation 기법들로 실험을 이어나갔습니다.</p><ol><li><p><a href="https://github.com/toriving/KoEDA">EDA &amp; AEDA</a><br> KoEDA 라이브러리를 사용하여 EDA, AEDA 각각 전체 데이터셋에 대해 <code>n_aug=[1, 2, 4]</code> 비율로 augmentation 진행</p><ul><li>두 방법 모두 아무것도 하지 않았을 때보다 validation score가 낮았음.</li></ul></li><li><p><a href="https://imbalanced-learn.org/stable/">Undersampling &amp; Oversampling</a><br> imblearn 라이브러리를 사용하여 SMOTE로 Sampling 진행<br> <img src="/images/under.png" alt="Undersampling"></p><p> <img src="/images/over.png" alt="Oversampling"></p><ul><li>두 방법 모두 아무것도 하지 않았을 때보다 validation score가 낮았음.</li></ul></li><li><p>Back Translation<br> Crawler를 사용하여 papago 번역기 사용.<br> <code>klue/roberta-small</code> 모델 기준으로 score 상승이 있었지만 너무 늦게 시도해서 best 모델에 적용하지 못했음.</p><ul><li><code>ko -&gt; en -&gt; ja -&gt; ko</code>: 약 0.1 LB Score 하락</li><li><code>ko -&gt; ja -&gt; ko</code>: 약 0.5 LB Score 상승</li></ul></li><li><p>Target Augmentation subject &lt;-&gt; object label changing<br> kfold로 학습을 진행할 때 한 번이라도 틀린 data에 대해서 subject와 object entity를 변경함으로써 augmentation 진행</p><ul><li>약 0.05 LB Score 상승</li></ul></li></ol><p>하지만 이러한 augmentation 기법들에 대해서 많은 효과를 볼 수가 없었는데, <code>confusion matrix</code>를 통해 원인을 유추해볼 수 있었습니다.</p><p><img src="/images/conf.png"></p><p>처음 예상과 다르게 적은 label의 데이터를 생각보다 잘 맞추고 있었고, <em>오히려 데이터 수가 가장 많았던 <code>no_relation</code> 예측에서 많이 틀리고 있었기 때문</em> 이었습니다. 따라서 전체 데이터셋에 대해서 augmentation을 진행한 방식, 그리고 sampling 방식으로는 효과를 보지 못했다는 것을 알 수 있었습니다.</p><p>처음부터 이렇게 Confusion Matrix를 도입하여 현재 모델이 어떤 예측을 잘 수행하지 못하는지 등을 파악하여, <code>no_relation</code>에 대해서만 augmentation을 시도하는 등 디테일하게 augmentation 전략을 세웠으면 좋았을 것이란 아쉬움이 남습니다. 또한 4번 실험에서 <code>sub &lt;-&gt; obj</code> label만 변경하는 방식 말고 다른 augmentation 방법도 써봤으면 어땠을까 하는 아쉬움이 남습니다.</p><h3 id="🔧-Data-Preprocessing-amp-Tokenizer"><a href="#🔧-Data-Preprocessing-amp-Tokenizer" class="headerlink" title="🔧 Data Preprocessing &amp; Tokenizer"></a>🔧 Data Preprocessing &amp; Tokenizer</h3><ol><li><p>Dynamic Padding<br> Huggingface의 Tokenizer는 <code>max_length</code> 인자를 통해 기본적으로 <code>fixed padding</code> 방식을 사용합니다. 저희는 더 빠른 실험을 통해 <code>dynamic padding</code> 방식으로 변경하였고 그 결과 약 30%의 속도를 향상시킬 수 있었습니다.</p><p> <img src="/images/fixed.png" alt="fixed padding"></p><p> <img src="/images/dynamic.png" alt="dynamic padding"></p></li><li><p><a href="https://arxiv.org/pdf/2102.01373.pdf">An Improved Baseline for Sentence-level Relation Extraction</a><br> 문장의 Subject, Object Entity의 NER Type을 명시해주고, Entity의 위치를 사전학습에서 사용된 특수문자를 이용하여 표기하는 Typed Entity Marker를 적용했습니다.</p><p> <img src="/images/token.png"></p><ol><li>vanilla : 기본 베이스라인 input</li><li>special_ent : <code>기본 베이스라인 input + [sbj][sbj/] + [obj][obj/]</code></li><li>special_ent_without_prefix : 기본 베이스라인 input의 앞에있는 <code>subject [sep] object [sep] 부분을 제거</code>하고 <code>special token</code>을 사용 (4, 5번 역시 prefix를 제거함) </li><li>punct_ent : <code>@sbj@ #obj#</code> 식으로 special token 없이 entity 표현 </li><li>punct_typing_ent : <code>@*sbj_type*sbj@ #^obj_type^obj#</code> 식으로 entity type을 알려주며 표현결과: 3번 5번이 비교적 가장 우수한 성능을 보임, 동일 조건 하 validation f1 기준 1 정도의 성능 차이를 보임.</li></ol></li></ol><p>위의 두 방법을 사용하여 실험과 검증은 조금 더 빠르게 진행할 수 있었고, 성능 향상을 이끌어낼 수 있었습니다.</p><h2 id="🧘-Modeling"><a href="#🧘-Modeling" class="headerlink" title="🧘 Modeling"></a>🧘 Modeling</h2><p>Backbone이 되는 Model은 <code>klue/roberta-large</code>를 사용하였으며 Base 성능은 <code>avg. 71 (micro f1)</code> 정도를 기록하였습니다.</p><p><em><strong>여러가지 실험들</strong></em></p><ul><li><p>Entity Embedding<br>  아무것도 하지 않았을 때보다 validation score가 낮았음.</p><details>  <summary>  코드 보기  </summary>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RobertaEmbeddingsWithTokenEmbedding</span>(<span class="hljs-params">nn.Module</span>):</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">edit by 곽진성_T2011</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, model, config, pre_model_state_dict=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.word_embeddings = model.roberta.embeddings.word_embeddings<br>    self.position_embeddings = model.roberta.embeddings.position_embeddings<br>    self.token_type_embeddings = model.roberta.embeddings.token_type_embeddings<br><br>    self.entity_embeddings = nn.Embedding(<span class="hljs-number">9</span>, config.hidden_size, padding_idx=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">if</span> pre_model_state_dict:<br>        pre_weight = pre_model_state_dict[<span class="hljs-string">&#x27;roberta.embeddings.entity_embeddings.weight&#x27;</span>]<br>        self.entity_embeddings.weight = torch.nn.parameter.Parameter(pre_weight, requires_grad=<span class="hljs-literal">True</span>)<br><br>    self.LayerNorm = model.roberta.embeddings.LayerNorm<br>    self.dropout = model.roberta.embeddings.dropout<br>    self.position_embedding_type = <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">&quot;position_embedding_type&quot;</span>, <span class="hljs-string">&quot;absolute&quot;</span>)<br>    self.register_buffer(<span class="hljs-string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="hljs-string">&quot;1.6.0&quot;</span>):<br>        self.register_buffer(<br>            <span class="hljs-string">&quot;token_type_ids&quot;</span>,<br>            torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),<br>            persistent=<span class="hljs-literal">False</span>,<br>        )<br>    self.padding_idx = config.pad_token_id<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">    self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span></span></span><br><span class="hljs-params"><span class="hljs-function"></span>):</span><br>    <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            position_ids = self.create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)<br>        <span class="hljs-keyword">else</span>:<br>            position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)<br><br>    <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        input_shape = input_ids.size()<br>    <span class="hljs-keyword">else</span>:<br>        input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br><br>    seq_length = input_shape[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">if</span> token_type_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&quot;token_type_ids&quot;</span>):<br>            buffered_token_type_ids = self.token_type_ids[:, :seq_length]<br>            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[<span class="hljs-number">0</span>], seq_length)<br>            token_type_ids = buffered_token_type_ids_expanded<br>        <span class="hljs-keyword">else</span>:<br>            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)<br><br>    <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        inputs_embeds = self.word_embeddings(input_ids)<br>    token_type_embeddings = self.token_type_embeddings(token_type_ids)<br><br>    entity_ids = self.create_entity_ids_from_input_ids(input_ids)<br>    entity_embeddings = self.entity_embeddings(entity_ids)<br><br>    embeddings = inputs_embeds + token_type_embeddings<br>    <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>        position_embeddings = self.position_embeddings(position_ids)<br>        embeddings += position_embeddings<br>    <br>    embeddings += entity_embeddings<br>    <br>    embeddings = self.LayerNorm(embeddings)<br>    embeddings = self.dropout(embeddings)<br>    <span class="hljs-keyword">return</span> embeddings<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_position_ids_from_inputs_embeds</span>(<span class="hljs-params">self, inputs_embeds</span>):</span><br>    input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br>    sequence_length = input_shape[<span class="hljs-number">1</span>]<br><br>    position_ids = torch.arange(<br>        self.padding_idx + <span class="hljs-number">1</span>, sequence_length + self.padding_idx + <span class="hljs-number">1</span>, dtype=torch.long, device=inputs_embeds.device<br>    )<br>    <span class="hljs-keyword">return</span> position_ids.unsqueeze(<span class="hljs-number">0</span>).expand(input_shape)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_entity_ids_from_input_ids</span>(<span class="hljs-params">self, input_ids</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    map index 1~8 to the token that is related to sbj, obj entities</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    s_ids = torch.nonzero((input_ids == <span class="hljs-number">36</span>)) <span class="hljs-comment"># subject</span><br>    o_ids = torch.nonzero((input_ids == <span class="hljs-number">7</span>)) <span class="hljs-comment"># object</span><br>    <span class="hljs-comment"># entity type mapped into index 3 ~ 8</span><br>    type_map = &#123;<span class="hljs-number">4410</span> : <span class="hljs-number">3</span>, <span class="hljs-number">7119</span> : <span class="hljs-number">4</span>, <span class="hljs-number">3860</span> : <span class="hljs-number">5</span>, <span class="hljs-number">5867</span> : <span class="hljs-number">6</span>, <span class="hljs-number">12395</span> : <span class="hljs-number">7</span>, <span class="hljs-number">9384</span> : <span class="hljs-number">8</span>&#125;<br><br>    entity_ids = torch.zeros_like(input_ids)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s_ids)):<br>        s_id = s_ids[i]<br>        o_id = o_ids[i]<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>            entity_ids[s_id[<span class="hljs-number">0</span>], s_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>] = type_map[input_ids[s_id[<span class="hljs-number">0</span>], s_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>].item()]<br>            entity_ids[o_id[<span class="hljs-number">0</span>], o_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>] = type_map[input_ids[o_id[<span class="hljs-number">0</span>], o_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>].item()]<br>        <span class="hljs-keyword">else</span>:<br>            prev_s_id = s_ids[i-<span class="hljs-number">1</span>]<br>            prev_o_id = o_ids[i-<span class="hljs-number">1</span>]<br>            entity_ids[s_id[<span class="hljs-number">0</span>], prev_s_id[<span class="hljs-number">1</span>]+<span class="hljs-number">4</span>:s_id[<span class="hljs-number">1</span>]] = <span class="hljs-number">1</span><br>            entity_ids[o_id[<span class="hljs-number">0</span>], prev_o_id[<span class="hljs-number">1</span>]+<span class="hljs-number">4</span>:o_id[<span class="hljs-number">1</span>]] = <span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> entity_ids<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_position_ids_from_input_ids</span>(<span class="hljs-params">self, input_ids, padding_idx, past_key_values_length=<span class="hljs-number">0</span></span>):</span><br>    mask = input_ids.ne(padding_idx).<span class="hljs-built_in">int</span>()<br>    incremental_indices = (torch.cumsum(mask, dim=<span class="hljs-number">1</span>).type_as(mask) + past_key_values_length) * mask<br>    <span class="hljs-keyword">return</span> incremental_indices.long() + padding_idx<br></code></pre></td></tr></table></figure></details></li><li><p><a href="https://github.com/monologg/R-BERT">R-BERT</a><br>  본 구조에서 BERT를 RoBERTa로 변경. LB 기준 71.362의 micro f1 score 달성.</p><details>  <summary>  코드 보기  </summary>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RBERT</span>(<span class="hljs-params">RobertaPreTrainedModel</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    orgin code: https://github.com/monologg/R-BERT</span><br><span class="hljs-string">    edit by 문하겸_T2076</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, config, model_name</span>):</span><br>        <span class="hljs-built_in">super</span>(RBERT, self).__init__(config)<br>        self.roberta = RobertaModel.from_pretrained(<br>            model_name)  <span class="hljs-comment"># Load pretrained bert</span><br><br>        self.num_labels = config.num_labels<br><br>        self.cls_fc_layer = FCLayer(<br>            config.hidden_size, config.hidden_size, <span class="hljs-number">0.1</span>)<br>        self.entity_fc_layer = FCLayer(<br>            config.hidden_size, config.hidden_size, <span class="hljs-number">0.1</span>)<br>        self.label_classifier = FCLayer(<br>            config.hidden_size * <span class="hljs-number">3</span>,<br>            config.num_labels,<br>            <span class="hljs-number">0.1</span>,<br>            use_activation=<span class="hljs-literal">False</span>,<br>        )<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">entity_average</span>(<span class="hljs-params">hidden_output, e_mask</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Average the entity hidden state vectors (H_i ~ H_j)</span><br><span class="hljs-string">        :param hidden_output: [batch_size, j-i+1, dim]</span><br><span class="hljs-string">        :param e_mask: [batch_size, max_seq_len]</span><br><span class="hljs-string">                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]</span><br><span class="hljs-string">        :return: [batch_size, dim]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        e_mask_unsqueeze = e_mask.unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [b, 1, j-i+1]</span><br>        length_tensor = (e_mask != <span class="hljs-number">0</span>).<span class="hljs-built_in">sum</span>(<br>            dim=<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size, 1]</span><br><br>        <span class="hljs-comment"># [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -&gt; [b, dim]</span><br>        sum_vector = torch.bmm(e_mask_unsqueeze.<span class="hljs-built_in">float</span>(),<br>                               hidden_output).squeeze(<span class="hljs-number">1</span>)<br>        avg_vector = sum_vector.<span class="hljs-built_in">float</span>() / length_tensor.<span class="hljs-built_in">float</span>()  <span class="hljs-comment"># broadcasting</span><br>        <span class="hljs-keyword">return</span> avg_vector<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids=<span class="hljs-literal">None</span>, attention_mask=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, labels=<span class="hljs-literal">None</span>, e1_mask=<span class="hljs-literal">None</span>, e2_mask=<span class="hljs-literal">None</span></span>):</span><br>        outputs = self.roberta(<br>            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)<br>        sequence_output = outputs[<span class="hljs-number">0</span>]<br>        pooled_output = outputs[<span class="hljs-number">1</span>]<br><br>        <span class="hljs-comment"># Average</span><br>        e1_h = self.entity_average(sequence_output, e1_mask)<br>        e2_h = self.entity_average(sequence_output, e2_mask)<br><br>        <span class="hljs-comment"># Dropout -&gt; tanh -&gt; fc_layer (Share FC layer for e1 and e2)</span><br>        pooled_output = self.cls_fc_layer(pooled_output)<br>        e1_h = self.entity_fc_layer(e1_h)<br>        e2_h = self.entity_fc_layer(e2_h)<br><br>        <span class="hljs-comment"># Concat -&gt; fc_layer</span><br>        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-<span class="hljs-number">1</span>)<br>        logits = self.label_classifier(concat_h)<br><br>        <span class="hljs-comment"># add hidden states and attention if they are here</span><br>        outputs = (logits,) + outputs[<span class="hljs-number">2</span>:]<br><br>        <span class="hljs-comment"># Softmax</span><br>        <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.num_labels == <span class="hljs-number">1</span>:<br>                loss_fct = nn.MSELoss()<br>                loss = loss_fct(logits.view(-<span class="hljs-number">1</span>), labels.view(-<span class="hljs-number">1</span>))<br>            <span class="hljs-keyword">else</span>:<br>                loss_type = <span class="hljs-string">&quot;focal&quot;</span><br>                beta = <span class="hljs-number">0.9999</span><br>                gamma = <span class="hljs-number">2.0</span><br><br>                loss_fct = CB_loss(beta=beta, gamma=gamma)<br>                loss = loss_fct(logits.view(-<span class="hljs-number">1</span>, self.num_labels),<br>                                labels.view(-<span class="hljs-number">1</span>), loss_type)<br><br>            outputs = (loss,) + outputs<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure></details></li><li><p>Model split &amp; combine<br>  <code>no_relation</code>만 구분하도록 학습시킨 모델, <code>relation</code>만 구분하도록 학습시킨 모델, <code>전체 데이터로 학습시킨 모델</code> 세 가지 <code>klue/roberta-large</code> 모델의 가중치를 freezing 하고 classifier만 학습시킨 것. LB 기준 73.251의 micro f1 score 달성.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CombineModels</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 이요한_T2166</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(CombineModels, self).__init__()<br><br>        c1 = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>, num_labels=<span class="hljs-number">2</span>)<br>        c2 = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>, num_labels=<span class="hljs-number">29</span>)<br>        c3 = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>, num_labels=<span class="hljs-number">30</span>)<br><br>        self.roberta1 = AutoModelForSequenceClassification.from_pretrained(<br>            <span class="hljs-string">&quot;split_model_no_rel_large&quot;</span>, config=c1)<br>        self.roberta2 = AutoModelForSequenceClassification.from_pretrained(<br>            <span class="hljs-string">&quot;split_model_rel_large&quot;</span>, config=c2)<br>        self.roberta3 = AutoModelForSequenceClassification.from_pretrained(<br>            <span class="hljs-string">&quot;sota_kfold&quot;</span>, config=c3)<br><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.roberta1.parameters():<br>            p.requires_grad = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.roberta2.parameters():<br>            p.requires_grad = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.roberta3.parameters():<br>            p.requires_grad = <span class="hljs-literal">False</span><br><br>        self.fc1 = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">768</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">29</span>, <span class="hljs-number">768</span>)<br>        self.fc3 = nn.Linear(<span class="hljs-number">30</span>, <span class="hljs-number">768</span>)<br><br>        self.classifier = nn.Sequential(<br>            nn.Dropout(p=<span class="hljs-number">0.1</span>),<br>            nn.Linear(<span class="hljs-number">768</span> * <span class="hljs-number">15</span>, <span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>),<br>            nn.Tanh(),<br>            nn.Dropout(p=<span class="hljs-number">0.1</span>),<br>            nn.Linear(<span class="hljs-number">768</span>, <span class="hljs-number">30</span>, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):</span><br>        logits_1 = self.roberta1(<br>            input_ids.clone(), attention_mask=attention_mask).get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br>        logits_2 = self.roberta2(<br>            input_ids.clone(), attention_mask=attention_mask).get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br>        logits_3 = self.roberta3(<br>            input_ids.clone(), attention_mask=attention_mask).get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br><br>        logits_1 = self.fc1(logits_1)<br>        logits_2 = self.fc2(logits_2)<br>        logits_3 = self.fc1(logits_3)<br><br>        concatenated_vectors = torch.cat((<br>            logits_1, logits_2, logits_3), dim=-<span class="hljs-number">1</span>)<br><br>        output = self.classifier(concatenated_vectors)<br>        outputs = SequenceClassifierOutput(logits=output)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>  </details></li><li><p>FC Layer -&gt; LSTM<br>  너무 늦게 도입하여 제출 실패.</p><details>  <summary>  코드 보기  </summary>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RobertaAddLSTM</span>(<span class="hljs-params">RobertaPreTrainedModel</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 정희영_T2210</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, config, *args, **kwargs</span>):</span><br>            <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>            self.bert = RobertaModel.from_pretrained(<span class="hljs-string">&quot;klue/roberta-large&quot;</span>)<br><br>            self.lstm = nn.LSTM(<span class="hljs-number">1024</span>, <span class="hljs-number">256</span>, batch_first=<span class="hljs-literal">True</span>, bidirectional=<span class="hljs-literal">True</span>)<br>            self.linear = nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">2</span>, <span class="hljs-number">30</span>)<br>            self.dropout = nn.Dropout(<span class="hljs-number">0.5</span>)<br>            self.tanh = nn.Tanh()<br>            self.linear2 = nn.Linear(<span class="hljs-number">30</span>, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):</span><br>            output = self.bert(input_ids, attention_mask=attention_mask)<br><br>            lstm_output, (h,c) = self.lstm(output[<span class="hljs-number">0</span>]) <span class="hljs-comment">## extract the 1st token&#x27;s embeddings</span><br>            hidden = torch.cat((lstm_output[:,-<span class="hljs-number">1</span>, :<span class="hljs-number">256</span>],lstm_output[:,<span class="hljs-number">0</span>, <span class="hljs-number">256</span>:]),dim=-<span class="hljs-number">1</span>)<br>            linear_output = self.linear(hidden.view(-<span class="hljs-number">1</span>,<span class="hljs-number">256</span>*<span class="hljs-number">2</span>))<br>            x = self.tanh(linear_output)<br>            x = self.dropout(x)<br>            outputs = SequenceClassifierOutput(logits=x)<br><br>            <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure></details></li><li><p>TAPT - <a href="https://arxiv.org/pdf/2004.10964.pdf">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a><br>  주어진 학습데이터로 사전학습된 모델에 TAPT 를 적용해보았을 때 약 0.5 정도의 validation f1 score 향상이 있었으나, 시간문제로 논문에서 제안된 epochs만큼 학습을 진행하지 못했음. koelectra와 roberta-base로 리더보드에 제출해본 결과 큰 성능향상이 없었기 때문에 large 모델에 적용해볼 수 없었음.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">edit by 정진원_T2206</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, RobertaForMaskedLM, ElectraForMaskedLM, BertForMaskedLM, AutoConfig, DataCollatorWithPadding, DataCollatorForLanguageModeling<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LineByLineTextDataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer, TrainingArguments<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EarlyStoppingCallback<br><br><span class="hljs-comment"># fetch pretrained model for MaskedLM training </span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>)<br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br>model = BertForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>)<br>model.to(device)<br><br><span class="hljs-comment"># Read txt file which is consisted of sentences from train.csv</span><br>dataset = LineByLineTextDataset(<br>    tokenizer=tokenizer,<br>    file_path=<span class="hljs-string">&#x27;data/train.txt&#x27;</span>,<br>    block_size=<span class="hljs-number">514</span> <span class="hljs-comment"># block size needs to be modified to max_position_embeddings</span><br>)<br><br>data_collator = DataCollatorForLanguageModeling( <br>    tokenizer=tokenizer, mlm=<span class="hljs-literal">True</span>, mlm_probability=<span class="hljs-number">0.2</span> <br>)<br><br><span class="hljs-comment"># need to change arguments </span><br>training_args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&quot;./klue-roberta-retrained&quot;</span>,<br>    overwrite_output_dir=<span class="hljs-literal">True</span>,<br>    learning_rate=<span class="hljs-number">5e-05</span>,<br>    num_train_epochs=<span class="hljs-number">200</span>, <br>    per_device_train_batch_size=<span class="hljs-number">16</span>,<br>    save_steps=<span class="hljs-number">100</span>,<br>    save_total_limit=<span class="hljs-number">2</span>,<br>    seed=<span class="hljs-number">30</span>,<br>    save_strategy=<span class="hljs-string">&#x27;epoch&#x27;</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">8</span>,<br>    logging_steps=<span class="hljs-number">100</span>,<br>    evaluation_strategy=<span class="hljs-string">&#x27;epoch&#x27;</span>,<br>    resume_from_checkpoint=<span class="hljs-literal">True</span>,<br>    fp16=<span class="hljs-literal">True</span>,<br>    fp16_opt_level=<span class="hljs-string">&#x27;O1&#x27;</span>,<br>    load_best_model_at_end=<span class="hljs-literal">True</span><br>) <br><br>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    data_collator=data_collator,<br>    train_dataset=dataset,<br>    eval_dataset=dataset,<br>    callbacks = [EarlyStoppingCallback(early_stopping_patience=<span class="hljs-number">3</span>)]<br>)<br><br>trainer.train()<br>trainer.save_model(<span class="hljs-string">&quot;./klue-roberta-retrained&quot;</span>)<br></code></pre></td></tr></table></figure>  </details></li></ul><p><em><strong>loss function</strong></em></p><ul><li><p>CB Loss<br>  R-BERT를 제외하고는 큰 성능향상을 못봤음.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyTrainer</span>(<span class="hljs-params">Trainer</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 문하겸_T2076</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, disable_wandb=<span class="hljs-literal">True</span>, *args, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        self.disable_wandb = disable_wandb<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss</span>(<span class="hljs-params">self, model, inputs, return_outputs=<span class="hljs-literal">False</span></span>):</span><br>        labels = inputs.get(<span class="hljs-string">&quot;labels&quot;</span>)<br>        outputs = model(**inputs)<br>        logits = outputs.get(<span class="hljs-string">&quot;logits&quot;</span>)<br>        beta = <span class="hljs-number">0.9999</span><br>        gamma = <span class="hljs-number">2.0</span><br><br>        criterion = CB_loss(beta, gamma)<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            criterion.cuda()<br>        loss_fct = criterion(logits, labels)<br><br>        <span class="hljs-keyword">return</span> (loss_fct, outputs) <span class="hljs-keyword">if</span> return_outputs <span class="hljs-keyword">else</span> loss_fct<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluation_loop</span>(<span class="hljs-params">self, *args, **kwargs</span>):</span><br>        eval_loop_output = <span class="hljs-built_in">super</span>().evaluation_loop(*args, **kwargs)<br><br>        pred = eval_loop_output.predictions<br>        label_ids = eval_loop_output.label_ids<br><br>        self.draw_confusion_matrix(pred, label_ids)<br>        <span class="hljs-keyword">return</span> eval_loop_output<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CB_loss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, beta, gamma, epsilon=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>(CB_loss, self).__init__()<br>        self.beta = beta<br>        self.gamma = gamma<br>        self.epsilon = epsilon<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, logits, labels</span>):</span><br>        <span class="hljs-comment"># self.epsilon = 0.1 #labelsmooth</span><br>        beta = self.beta<br>        gamma = self.gamma<br><br>        no_of_classes = logits.shape[<span class="hljs-number">1</span>]<br>        samples_per_cls = torch.Tensor(<br>            [<span class="hljs-built_in">sum</span>(labels == i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(logits.shape[<span class="hljs-number">1</span>])])<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            samples_per_cls = samples_per_cls.cuda()<br><br>        effective_num = <span class="hljs-number">1.0</span> - torch.<span class="hljs-built_in">pow</span>(beta, samples_per_cls)<br>        weights = (<span class="hljs-number">1.0</span> - beta) / ((effective_num) + <span class="hljs-number">1e-8</span>)<br><br>        weights = weights / torch.<span class="hljs-built_in">sum</span>(weights) * no_of_classes<br>        labels = labels.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>        weights = torch.tensor(weights.clone().detach()).<span class="hljs-built_in">float</span>()<br><br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            weights = weights.cuda()<br>            labels_one_hot = torch.zeros(<br>                <span class="hljs-built_in">len</span>(labels), no_of_classes).cuda().scatter_(<span class="hljs-number">1</span>, labels, <span class="hljs-number">1</span>).cuda()<br><br>        labels_one_hot = (<span class="hljs-number">1</span> - self.epsilon) * labels_one_hot + \<br>            self.epsilon / no_of_classes<br>        weights = weights.unsqueeze(<span class="hljs-number">0</span>)<br>        weights = weights.repeat(labels_one_hot.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>) * labels_one_hot<br>        weights = weights.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br>        weights = weights.unsqueeze(<span class="hljs-number">1</span>)<br>        weights = weights.repeat(<span class="hljs-number">1</span>, no_of_classes)<br><br>        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)<br>        <span class="hljs-keyword">return</span> cb_loss<br></code></pre></td></tr></table></figure>  </details></li><li><p><a href="https://arxiv.org/pdf/1906.07413.pdf">LDAM</a></p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LDAMLossTrainer</span>(<span class="hljs-params">Trainer</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 김민수_T2025</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        self.n_per_labels = self.train_dataset.get_n_per_labels()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss</span>(<span class="hljs-params">self, model, inputs, return_outputs=<span class="hljs-literal">False</span></span>):</span><br>        labels = inputs.get(<span class="hljs-string">&#x27;labels&#x27;</span>)<br>        outputs = model(**inputs)<br>        logits = outputs.get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br><br>        betas = [<span class="hljs-number">0</span>, <span class="hljs-number">0.99</span>]<br>        beta_idx = self.state.epoch &gt;= <span class="hljs-number">2</span><br>        n_per_labels = self.n_per_labels<br><br>        effective_num = <span class="hljs-number">1.0</span> - np.power(betas[beta_idx], n_per_labels)<br>        cls_weights = (<span class="hljs-number">1.0</span> - betas[beta_idx]) / np.array(effective_num)<br>        cls_weights = cls_weights / np.<span class="hljs-built_in">sum</span>(cls_weights) * <span class="hljs-built_in">len</span>(n_per_labels)<br>        cls_weights = torch.FloatTensor(cls_weights)<br><br>        criterion = LDAMLoss(cls_num_list=n_per_labels, max_m=<span class="hljs-number">0.5</span>, s=<span class="hljs-number">30</span>, weight=cls_weights)<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            criterion.cuda()<br><br>        loss_fct = criterion(logits, labels)<br>        <span class="hljs-keyword">return</span> (loss_fct, outputs) <span class="hljs-keyword">if</span> return_outputs <span class="hljs-keyword">else</span> loss_fct<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LDAMLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, cls_num_list, max_m=<span class="hljs-number">0.5</span>, weight=<span class="hljs-literal">None</span>, s=<span class="hljs-number">30</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        m_list = <span class="hljs-number">1.0</span> / np.sqrt(np.sqrt(cls_num_list))<br>        m_list = m_list * (max_m / np.<span class="hljs-built_in">max</span>(m_list))<br>        m_list = torch.cuda.FloatTensor(m_list)<br>        self.m_list = m_list<br>        <span class="hljs-keyword">assert</span> s &gt; <span class="hljs-number">0</span><br>        self.s = s<br>        self.weight = weight<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, target</span>):</span><br>        index = torch.zeros_like(x, dtype=torch.<span class="hljs-built_in">bool</span>)<br>        index.scatter_(<span class="hljs-number">1</span>, target.data.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), <span class="hljs-number">1</span>)<br><br>        index_float = index.<span class="hljs-built_in">type</span>(torch.cuda.FloatTensor)<br>        batch_m = torch.matmul(self.m_list[<span class="hljs-literal">None</span>, :], index_float.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>        batch_m = batch_m.view((-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        x_m = x - batch_m<br><br>        output = torch.where(index, x_m, x)<br>        <span class="hljs-keyword">return</span> F.cross_entropy(self.s * output.to(<span class="hljs-string">&#x27;cuda&#x27;</span>), target.to(<span class="hljs-string">&#x27;cuda&#x27;</span>), weight=self.weight.to(<span class="hljs-string">&#x27;cuda&#x27;</span>))<br></code></pre></td></tr></table></figure>  </details></li></ul><p>결과적으론 backbone 모델의 kfold score를 넘어서지 못했습니다. 그 이유로는 public leaderboard와 간극이 적은 적절한 validation dataset을 구성하지 못했기 때문입니다. </p><p>차선책으로 미리 stratified하게 0.1 비율로 split한 train, validation dataset을 고정시켜놓고 사용하였지만, 해당 데이터셋의 validation score는 public leaderboard와 <em><strong>평균적으로 15점 이상, 극단적인 경우 30점까지 차이가 존재</strong></em> 했습니다. 이러한 validation dataset으로만 실험을 했기 때문에 validation score를 신뢰하기 어려웠고, model이 좋은지 나쁜지 직접 제출해보기 전에는 알 수 없었습니다.</p><p>따라서 제출횟수가 한정적이므로 대부분 위의 validation score로만 검증을 했고, 성능향상 가능성이 없다고 판단해 추가적인 실험을 진행하지 못했습니다. 하지만 막상 1, 2등의 solution을 보니 저희가 진행했던 실험들로 점수를 올렸기에 속이 쓰렸습니다… :(</p><p>다음 대회에서는 public leaderboard와의 간극이 적은 적절한 validation dataset을 구성할 필요가 있으며, 또한 교차검증을 위한 validation dataset 역시 고정시켜놓을 필요가 있다고 느꼈습니다.</p><h2 id="🍜-Ensemble"><a href="#🍜-Ensemble" class="headerlink" title="🍜 Ensemble"></a>🍜 Ensemble</h2><ul><li><p>K-fold</p><p>  <code>klue/roberta-large</code> 모델을 사용하여 kfold(k=5)를 사용하여 성능을 개선하였습니다. single fold 기준 public leaderboard에서 약 71의 micro f1 score를 기록하였고, 5 fold ensemble을 통해 public leaderboard에서 73.5의 micro f1 score를 기록할 수 있었습니다.</p></li></ul><h3 id="👨‍🎨-앙상블-깎는-노인과-기도메타"><a href="#👨‍🎨-앙상블-깎는-노인과-기도메타" class="headerlink" title="👨‍🎨 앙상블 깎는 노인과 기도메타"></a>👨‍🎨 앙상블 깎는 노인과 기도메타</h3><p><img src="/images/soft_voting.jpg" alt="soft voting"></p><p>최종적으로 public leaderboard 기준 73 정도의 micro f1 score를 기록하였지만 예측 분포가 다른 결과들을 <code>soft voting</code>하여 public leaderboard 기준 74.306의 micro f1 score를 기록할 수 있었습니다.</p><p align="center">    <img src="/images/기도1.png" style="display: inline" height="100px">    <img src="/images/기도2.png" style="display: inline" height="100px">    <img src="/images/기도3.png" style="display: inline" height="100px">    <img src="/images/기도4.png" style="display: inline" height="100px"></p><p align="center">    <img src="/images/기도5.png" height="100px"></p><p>    최종적으로 제출된 결과는 Ensemble된 것들 중 public leaderboard 기준 AUPRC가 가장 높은 결과이며, private leaderboard 에서 다른 팀 대비 점수 하락폭이 적어서     <span class="linear_highlight">    순위가 9등 -> 5등으로 상승했습니다.    </span></p><p><img src="/images/final.png"></p><h2 id="💯-Good-Practice"><a href="#💯-Good-Practice" class="headerlink" title="💯 Good Practice"></a>💯 Good Practice</h2><p>저희 팀만의 Good Practice는 체계적으로 Notion에 실험관리를 한 것도 있지만, <code>huggingface</code>의 다양한 기능들을 사용해봤다는 것이고 그 중에서 좋은 효과를 낸 것으로는 <code>fp16</code>과 <code>hyperparameter_search</code>가 있습니다.</p><ul><li><p>fp16</p><p>  <img src="/images/fp16.png"><br>  fp16은 <code>Mixed-Precision Training</code>으로 32-bit Floating Point가 아닌 16-bit Floating Point를 사용하는 방식입니다. 이 방식을 통해 모델을 학습시킬 때 성능은 비슷하지만 약 60% 가량의 향상된 속도로 학습을 진행할 수 있었습니다.<br>  <code>TrainingArguments</code>에 <code>fp16=True</code>, <code>fp16_opt_level=&#39;O1&#39;</code>만 추가하면 바로 사용할 수 있어서 간단하게 다양한 실험을 진행할 수 있었습니다.</p></li></ul><ul><li><p>hyperparameter_search</p><p>  <code>hyperparameter_search</code>는 <code>Trainer</code>에 존재하는 method로 <code>raytune</code>, <code>optuna</code>, <code>SigOpt</code> 세 가지 중 자신의 환경에 설치되어 있는 라이브러리를 이용하여 적절한 hyperparameter를 탐색해주는 유용한 함수입니다. 저희는 <code>hyperparameter_search</code>로 public leaderboard 기준 4점 이상의 f1 score 향상을 기록할 수 있었습니다.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, Trainer, TrainingArguments<br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> load your tokenizer &amp; dataset</span><br><span class="hljs-comment"># tokenizer = ...</span><br><span class="hljs-comment"># dataset = ...</span><br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> change your pretrained model path</span><br>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;YOUR_MODEL_PATH&quot;</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model_init</span>():</span><br>    <span class="hljs-keyword">return</span> AutoModelForSequenceClassification.from_pretrained(<br>        model_path, config=config)<br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> fill it your training arguments</span><br>training_args = TrainingArguments(...)<br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> fill it your trainer arguments</span><br>trainer = Trainer(<br>    model_init=model_init, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> 반드시 model_init 함수로 모델을 불러와야합니다.</span><br>    args=training_args,<br>    ...<br>)<br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> optuna</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">optuna_hp_space</span>(<span class="hljs-params">trial</span>):</span><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;learning_rate&quot;</span>: trial.suggest_float(<span class="hljs-string">&quot;learning_rate&quot;</span>, <span class="hljs-number">5e-6</span>, <span class="hljs-number">5e-4</span>, log=<span class="hljs-literal">True</span>),<br>        <span class="hljs-string">&quot;num_train_epochs&quot;</span>: trial.suggest_int(<span class="hljs-string">&quot;num_train_epochs&quot;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>),<br>        <span class="hljs-string">&quot;seed&quot;</span>: trial.suggest_int(<span class="hljs-string">&quot;seed&quot;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">42</span>),<br>    &#125;<br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> ray tune</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ray_hp_space</span>():</span><br>    <span class="hljs-keyword">from</span> ray <span class="hljs-keyword">import</span> tune<br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;learning_rate&quot;</span>: tune.loguniform(<span class="hljs-number">5e-6</span>, <span class="hljs-number">5e-4</span>),<br>        <span class="hljs-string">&quot;num_train_epochs&quot;</span>: tune.choice(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)),<br>        <span class="hljs-string">&quot;seed&quot;</span>: tune.choice(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">42</span>)),<br>    &#125;<br><br>trainer.hyperparameter_search(<br>    direction=<span class="hljs-string">&quot;maximize&quot;</span>, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> or direction=&quot;minimize&quot;</span><br>    hp_space=ray_hp_space, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> if you wanna use optuna, change it to optuna_hp_space</span><br>    backend=<span class="hljs-string">&quot;ray&quot;</span>, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> if you wanna use optuna, remove this argument</span><br>)<br></code></pre></td></tr></table></figure>  </details><p>  <img src="/images/%EC%9A%A9%EB%9F%89%EA%BD%89%EC%B0%B8.png" alt="단점 - 용량꽉참"></p></li></ul><p>또한 <code>W&amp;B</code>를 팀으로 만들어서 팀원들이 실험하는 결과들을 전부 공유할 수 있게 만들었습니다. 덕분에 실패한 실험이나 성공한 실험들에 대해서 chart를 통해 더욱 쉽고 직관적으로 모델을 검증할 수 있었고, 팀원간 더 빠른 결과 공유가 가능했습니다. </p><p align="center">    <img src="/images/w0.png" height="300px"></p><p align="center">    <img src="/images/w1.png" style="display: inline" height="120px">    <img src="/images/w2.png" style="display: inline" height="120px"></p><p align="center">    <img src="/images/w3.png" style="display: inline" height="120px">    <img src="/images/w4.png" style="display: inline" height="120px"></p><h2 id="💌-Thanks-to"><a href="#💌-Thanks-to" class="headerlink" title="💌 Thanks to"></a>💌 Thanks to</h2><p><em><strong>청계산셰르파의 비밀병기, 이유경 멘토님.</strong></em></p><p align="center">    <img src="/images/notion_mentoring.png" style="display: inline" height="360px">    <img src="/images/notion_qna.png" style="display: inline" height="360px"></p><p>정말 바쁘신 와중에도 많은 것을 알려주시려고 열심히 찾아보시고, 따로 공부도 해가시면서 저희에게 많은 도움을 주셨습니다. 저희의 등반일지에 가장 큰 기여를 하신 이유경 멘토님께 다시한번 감사의 말씀 전해드리고 싶습니다.</p><br><p><em><strong>청계산셰르파 팀명에 항상 불만을 가지시는 성예닮 멘토님</strong></em><br><img src="/images/1.png"></p><p>유경멘토님의 사생팬답게 저희팀에도 많은 관심가져주시고 지켜봐주셔서 정말 든든합니다. 항상 말씀못드리는게 죄송할정도로, 많은 도움 주시고 알려주셔서 정말 감사합니다.</p><h2 id="마지막으로"><a href="#마지막으로" class="headerlink" title="마지막으로"></a>마지막으로</h2><p>저희가 처음에 계획했던 <code>기록</code>과 <code>공유</code>라는 가치에 있어서만큼은 전반적으로 팀원 모두가 만족할 수 있었던 프로젝트였습니다.</p><p>처음 합을 맞춤에도 불구하고 팀원 모두가 다음 프로젝트에서는 어떠한 역할을 수행하고, 어떤 식으로 협업을 하는 것이 효과적일지 스스로 깨우칠 수 있었다는 점에서 굉장히 고무적이며, 많은 깨달음을 얻을 수 있었던 경험이었습니다.</p><p>너무 좋은 팀원분들과 함께할 수 있어서 정말 좋았고 다음 대회가 너무나도 기다려지네요.</p><p>저희의 이야기가 조금이라도 도움이 되었길 바라면서 마치겠습니다.</p><p>긴 글 읽어주셔서 감사합니다.</p><h2 id="🤜-부록-팀원들-한마디"><a href="#🤜-부록-팀원들-한마디" class="headerlink" title="🤜 부록: 팀원들 한마디"></a>🤜 부록: 팀원들 한마디</h2><h3 id="좋았던-점"><a href="#좋았던-점" class="headerlink" title="좋았던 점"></a>좋았던 점</h3><ul><li>요한: 철저하게 기록을 하고, 실험결과를 공유했던 점이 가장 잘 한 것 같습니다. 특히 다양한 협업툴을 두고 사용한 것보다 그 기능들을 노션에다가 전부 통합하여 사용한게 혼란이 적어 잘 된 것 같습니다.</li><li>하겸: 실험 자체를 다양하게 시도하고, 실험 공유가 잘 됬던 것 같습니다. 이전 실험의 결론에서 다음 실험은 어떻게 할지를 정한 것도 좋았습니다.</li><li>준영: 팀원 간에 공유가 잘되서 좋았습니다. 기록이 잘되다보니 제가 하지 않은 실험에서도 아이디어를 가져올 수 있었습니다.</li><li>진원: 팀원들끼리 결과 공유와 기록이 잘 이루어졌습니다. 열심히 한 만큼 최종적으로도 괜찮은 결과를 얻어서 만족합니다.</li><li>민수: 실험을 많이 했던 것. 제출 횟수를 꽉 채워서 쓴게 좋았습니다.</li><li>희영: 의견 공유가 잘 되어서 내가 하지 않은 실험에서도 지식을 얻을 수 있어서 좋았습니다. 팀원들과 멘토님이 지식이 많아서 진짜 빠르게 배웠습니다.</li><li>진성 : 각자 실험에 있어 통제변인 설정을 철저히 하고 기록을 세세하게 해, 실험의 효과를 공유하기 좋았습니다.</li></ul><h3 id="아쉬운-점"><a href="#아쉬운-점" class="headerlink" title="아쉬운 점"></a>아쉬운 점</h3><ul><li>요한: 여러 실험 아이디어들이 있었지만, 막판으로 갈 수록 성능에 집착하여 큰 모델로만 실험을 하느라 모든 아이디어들에 대해서 실험을 하지 못했던 것이 아쉽습니다.</li><li>하겸 : 간극이 적은 validation set을 결국은 못찾았다. → 어떻게 찾을 수 있을지는 아직도 모르겠습니다. 성능면에서도 큰 도움이 되지 않아서 아쉬웠습니다.</li><li>준영: 모델 작업을 하지 못했습니다. 데이터 관련해서 많은 인사이트를 찾아보고 싶었지만 결과를 제대로 내지 못했습니다.</li><li>진원: 모델을 수정하는 작업을 많이 하지 못해 아쉬웠습니다. 다양한 실험을 진행하였지만, 모델의 성능을 올리는데 크게 기여하지 못했습니다.</li><li>민수: 모델을 태스크에 맞게 보다 적극적으로 변형하려는 시도를 하지 않았습니다.</li><li>희영: 대회 초기 생활 스케줄이 꼬여서 시간 낭비를 많이 했습니다. 생활 습관을 잘 잡고 시작하는 게 중요할 것 같습니다.</li><li>진성 : 수동적으로 할 일을 받아서 하거나, 다른 사람의 branch에 덧붙여서 작은 실험들만 했습니다. 다음 대회에서는 나도 적극적으로 논문 등에서 아이디어를 얻어와 큼직하게(빠르게) 정확하게 구현하는 연습을 해보자.</li></ul><h3 id="개선할-점"><a href="#개선할-점" class="headerlink" title="개선할 점"></a>개선할 점</h3><ul><li>요한: 성능은 어차피 오를 것이기 때문에, 비슷한 실험을 여러번 하는 것보다는 떠올린 아이디어들에 대해 전부 실험할 수 있도록 해봐야겠습니다.</li><li>하겸: 일단 강의부터 들어서 맨땅에 헤딩하지 않기</li><li>준영: 강의 빠르게 듣기. 데이터 빠르게 훑어보기. 모델 뜯어보기.</li><li>진원: 모델을 수정해보는 실험을 진행하고, 확실하게 성능 향상을 이루기 위해 실험을 진행할 때 조금 더 명확한 근거나 방법을 미리 조사해보고 진행하면 좋을 것 같습니다 (e.g. 논문 읽어서 train 횟수 정하기).</li><li>민수: 어떤식의 변형이 해당 태스크에 적합한지 단순 점수와 느낌 이외에 정량적으로 검증할 수 있는 방법론을 생각해보면 좋을 것 같습니다.</li><li>희영: 첫날 진짜 빠르게 강의 다 듣기. 10시~12시 사이 시간 잘 이용하기. 이동 후 바로 시작하기, 12시 넘으면 집에서 공부할 생각하지 말기. EDA를 너무 오래 하고 raw 데이터를 너무 많이 보면 시간 낭비일수도.</li><li>진성 : 후반부에 기록을 하는 데에 신경을 많이 못썼습니다. 결국 남는건 기록이니 다음 대회 긴 기간동안에도 꾸준히 기록하자.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
