<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MRC - Open Domain Question Answering</title>
    <link href="/MRC-Open-Domain-Question-Answering/"/>
    <url>/MRC-Open-Domain-Question-Answering/</url>
    
    <content type="html"><![CDATA[<h1 id="청계산셰르파의-MRC-ODQA-등반일지"><a href="#청계산셰르파의-MRC-ODQA-등반일지" class="headerlink" title="청계산셰르파의 MRC-ODQA 등반일지"></a>청계산셰르파의 MRC-ODQA 등반일지</h1><p><img src="/images/mrc/image-20211106210830331.png" alt="Final Result"></p><p>약 4주가량 참여했던 P Stage 의 <code>MRC - Open Domain Question Answering</code> 대회를 Wrap up 하였습니다. 청계산 셰르파의 일원들과 두 번째 합을 맞추었고, 어설픈 점도 많았지만 어떤 마음가짐과 전략으로 대회에 임했으며 서로가 서로의 <code>셰르파</code> 로써 등반을 완료할 수 있었는지 저희의 경험을 나누고자 합니다.</p><p>미흡한 점도 있었고, 여러 시행착오를 겪었지만 <code>KLUE - RE</code> 대회보다 2계단 상승한 3위를 달성할 수 있었습니다. 다양한 문제를 인식하고 해결하기 위한 아이디어를 수집한 뒤 결과적으로 구현하며 좋은 성과를 이루어낼 수 있었다는 점에서 팀원 모두가 많은 성장을 했다고 느낄 수 있었습니다. 언제나 그렇듯이, 저희의 경험이 어떠한 형태로든 도움이 되고 좋은 레퍼런스가 되기를 바라며 시작해보도록 하겠습니다.</p><h1 id="팀-소개"><a href="#팀-소개" class="headerlink" title="팀 소개"></a>팀 소개</h1><h2 id="청계산셰르파"><a href="#청계산셰르파" class="headerlink" title="청계산셰르파"></a>청계산셰르파</h2><p align="center">    <img src="/images/profile.png" style="display: inline" height="120px">    <img src="/images/2.png" style="display: inline" height="120px"></p><p>저희는 캠프 기간동안 모든 것을 생생하게 기억하고 나누는 <code>기록</code>과 <code>공유</code>라는 가치에 공감한 7명이 모여 팀을 구성했고, 서로가 서로의 가이드로서 좋은 영향을 주고받을 수 있는 셰르파가 되기를 원했습니다.</p><p>또한 주니어 엔지니어들의 로망은 판교역 근처 회사들에서 일을 하는 것입니다. 저희는 판교역의 뒷산인 청계산을 부스트캠프 과정에 빗대어 완벽하게 등반해보겠다는 의미로 청계산과 셰르파를 더해 <code>청계산셰르파</code>라는 이름을 사용하게 되었습니다.</p><h2 id="청계산셰르파들"><a href="#청계산셰르파들" class="headerlink" title="청계산셰르파들"></a>청계산셰르파들</h2><ul><li><a href="https://github.com/l-yohai"><img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/49181231?s=96&amp;v=4" width="20px"/></a> &nbsp; 이요한</li><li><a href="https://github.com/ddobokki">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/44228269?s=96&v=4" width="20px"></a> &nbsp; 문하겸</li><li><a href="https://github.com/20180707jun">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/50571795?s=96&v=4" width="20px"></a> &nbsp; 전준영</li><li><a href="https://github.com/godjw">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/47168115?s=96&v=4" width="20px"></a> &nbsp; 정진원</li><li><a href="https://github.com/lexiconium">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/84180121?s=96&v=4" width="20px"></a> &nbsp; 김민수</li><li><a href="https://github.com/hyeong01">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/38185429?s=96&v=4" width="20px"></a> &nbsp; 정희영</li><li><a href="https://github.com/jskwak98">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/47588410?s=96&v=4" width="20px"></a> &nbsp; 곽진성</li></ul><h1 id="대회-소개"><a href="#대회-소개" class="headerlink" title="대회 소개"></a>대회 소개</h1><style>  .linear_highlight {      background: linear-gradient(to top, #778899 10%, transparent 10%);  }</style><p>Question Answering (QA) 은 다양한 종류의 질문에 대해 대답하는 인공지능을 만드는 연구 분야입니다. 다양한 QA 시스템 중, Open-Domain Question Answering (ODQA) 은 주어지는 지문이 따로 존재하지 않고 사전에 구축되어 있는 Knowledge resource 에서 질문에 대답할 수 있는 문서를 찾아 질문에 대한 답을 하는 과제로 일반적인 QA 보다 Challenging한 과제입니다. </p><p><img src="/images/mrc/image-20211106222959110.png" alt="What is Goethe&#39;s masterpiece?"></p><p><img src="/images/mrc/image-20211106223121104.png" alt="ODQA Workflow"></p><p>대표적인 ODQA 시스템의 예시로는 Google 검색엔진이 있습니다. Google에 질문을 입력하게 되면 Wikipedia 등에서 Knowledge Resource에서 질문에 답을 할 수 있는 Retrieval이 동작하여 관련 문서를 찾고, Reader가 그 문서 속에서 정답을 찾아 답을 알려주는 시스템입니다.</p><p>이번 대회는 이 ODQA 시스템 중 질문과 관련있는 문서를 찾아주는 Retrieval과, 관련된 문서 중에서 적절한 답변을 찾는 Reader의 두 가지 stage를 구현하고 통합하여 정확한 답변을 많이 내주는 모델을 만드는 대회였습니다.</p><span class="linear_highlight">저희 팀은 4주동안 175회라는 다른 팀 대비 압도적인 제출횟수를 기록하며 굉장히 많은 실험을 이어갔고, 최종적으로 Private Leaderboard에서 3위를 기록하였습니다.</span><h1 id="Strategy-amp-Planning"><a href="#Strategy-amp-Planning" class="headerlink" title="Strategy &amp; Planning"></a>Strategy &amp; Planning</h1><p>이번 대회는 4주라는 긴 시간동안 진행되었기 때문에, 그 시간들을 헛되이 쓰지 않게 하기 위해서는 적절한 대회전략을 세울 필요가 있었고, 그에 걸맞는 계획을 수립해야 했습니다.</p><p>이번 대회에는 아래와 같은 제약사항이 있었습니다.</p><div class="note note-primary">            <ol><li>외부 dataset 사용 금지. <br></li><li>Pretrained Model weights의 경우 상업적 이용이 가능한 License를 가지고 있으며 누구에게나 공개된 모델이어야 함. <br></li><li>Reader Model을 학습시키는 것이 아닌, 제공된 학습 Data를 Generation Model 등을 이용하여 Augmentation 시키는 것을 목적으로 한다면 상업적 이용이 불가능하더라도 Pretrained Model Weights 사용 가능.</li></ol>          </div><p>지금까지의 경험과 위의 제약사항들을 토대로 보다 strict 하게 계획을 세우기로 하였고, 아래의 Timeline 대로 작업을 진행하기로 했습니다.</p><p><img src="/images/mrc/image-20211109124548155.png" alt="Timeline"></p><p>첫 1주차에는 주어진 baseline 코드가 어렵게 작성되어 있었기 때문에 MRC라는 Task에 대한 공부를 바탕으로 baseline 코드 분석 및 refactoring을 진행했습니다. 2주차부터 본격적으로 EDA와 성능평가를 기반으로 baseline 의 어떤 부분을 보완하면 성능을 향상시킬 수 있을지를 집중적으로 탐색해보았습니다. 그리고 그렇게 탐색한 결과를 바탕으로 3주차에는 data augmentation 과 retrieval, reader 의 성능을 향상시키기 위한 아이디어들을 실제로 구현하고 실험하며 성능을 최대한 끌어올렸고, 그렇게 얻은 모델들을 마지막 4주차 때 앙상블하여 최종 결과를 도출했습니다.</p><h1 id="Baseline-분석-및-Task-이해-Refactoring"><a href="#Baseline-분석-및-Task-이해-Refactoring" class="headerlink" title="Baseline 분석 및 Task 이해, Refactoring"></a>Baseline 분석 및 Task 이해, Refactoring</h1><p>베이스라인 코드를 훑어보고 분석하는 과정에서 상당한 난항을 겪었습니다. “적어도 실행은 된다!” 라는 측면에서는 완전한 형태로 제공되었지만, 다른 두 번의 대회와 다르게 내부 Process 가 복잡하고 대체로 경험이 있었던 분류 과제보다는 난이도가 어려웠기 때문입니다.</p><p>하지만 자유로운 실험과 Customizing 을 위해서는 개념적으로만 대략적으로 이해하고 있었던 ODQA Task 가 실제 코드로 어떻게 구현되었는지 보다 자세히 파악하는 동시에 저희가 편하게 사용할 수 있도록 Refactoring 을 진행하면서 필요한 모듈을 적용시키는 과정이 필요했습니다. 이 과정에서 셰르파의 일원인 김민수 캠퍼님이 굉장한 수고와 기여를 해주셨습니다.</p><h2 id="기존-베이스라인-코드"><a href="#기존-베이스라인-코드" class="headerlink" title="기존 베이스라인 코드"></a>기존 베이스라인 코드</h2><p>코드를 살피고 프로세스를 익히면서 처음에 아래와 같은 생각을 하게 되었습니다.</p><div class="note note-danger">            <p><strong>1. 코드의 가독성이 정말 좋지 않다는 것</strong><br><strong>2. 처음부터 저희의 입맛대로 작성하는 것이 차라리 더 쉽지 않을까</strong></p>          </div><p>베이스라인의 <code>train.py</code> 스크립트를 살펴보았을 때, 해당 코드의 간략한 구조는 아래와 같습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train.py</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    tokenizer, model, dataset 등의 initialization<br>    ...<br>    run_mrc(...)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_mrc</span>(<span class="hljs-params">...</span>):</span><br>    ...<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_train_features</span>(<span class="hljs-params">...</span>):</span><br>        trainset의 preprocessing<br>    prepare_train_features을 통한 trainset의 mapping<br>    ...<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_validation_features</span>(<span class="hljs-params">...</span>):</span><br>        evalset의 preprocessing<br>    prepare_validation_features을 통한 evalset의 mapping<br>    ...<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">post_processing_function</span>(<span class="hljs-params">...</span>):</span><br>        특정 metric으로 점수를 산출하기 위한 postprocessing<br>    ...<br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_metrics</span>(<span class="hljs-params">...</span>):</span><br>        특정 metric으로 점수를 산출<br><br>    trainer의 initialization<br>    trainer.train()<br>    ...<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><p>베이스라인 <code>run_mrc</code> 함수는 Preprocess 와 Postprocess, Compute Metrics 등의 함수를 정의하고 실행하며, 학습까지 시작시키는 여러 책임을 가지고 있는 함수였습니다. 이렇게 많은 작업들이 수행되는데, 이 작업들이 <span class="linear_highlight">nested function</span> 형태로 함수화되어 있었습니다. 이로인해 <code>run_mrc</code>의 구조가 한 눈에 보이지 않았고 각 nested function의 내용을 파악하는 것이 굉장히 힘들었습니다.</p><p>또한 위의 이유와 더불어 nested function은 그 특성상 run_mrc 상에서 정의된 변수, 객체 등에 접근할 수 있는데, <span class="linear_highlight">이로 인해 어떤 변수나 객체 등이 어느 nested function 안에서 변하거나 변하지 않았는지에 대해 파악하는 것에 어려움</span>이 있었습니다. 그리고 이러한 nested function 이 Huggingface Trainer 내부에서 동작해야하기 때문에 <code>train.py</code> 와 <code>inference.py</code> 등의 스크립트에서도 <span class="linear_highlight">중복으로 존재</span>했으며 Debugging 을 위해서는 Huggingface 의 코드들을 일일이 덧붙여가며 확인해야 했습니다.</p><h2 id="재구성한-베이스라인-코드"><a href="#재구성한-베이스라인-코드" class="headerlink" title="재구성한 베이스라인 코드"></a>재구성한 베이스라인 코드</h2><p>이러한 문제점들을 인식한 뒤, huggingface 와의 호환성을 유지하며 학습 과정에 쓰이는 입력과 출력 형식을 Customizing 하기 위해 huggingface의 <code>Trainer</code> 클래스를 상속받아 본인만의 <code>Custom Trainer</code> 클래스를 작성하고, 이를 통해 학습을 진행하도록 코드를 재구성하였습니다. 하지만 문제는 <code>Custom Trainer</code> 클래스를 작성하며 이미 존재하는 몇몇 method 를 task에 맞게 새로 작성하여 덮어씌웠는데,<br>이 과정에서 Huggingface 와 의존성이 존재하는 코드들을 대부분 제외시켰다는 것에 있었습니다. 이렇게 작성된 코드는 향후 다양한 실험을 진행할 때 방해요소가 되겠다는 생각을 하였으며, Task specific 한 방식으로 구성하는 것보다는 미리 <span class="linear_highlight"> 라이브러리와의 호환성 문제를 최소화하는 방향으로 코드를 재작성하는 것이 좋겠다</span>는 판단을 하게 되었습니다.</p><p>최종적으로 사용된 구조는 아래와 같습니다.</p><table><thead><tr><th align="center">스크립트</th><th align="center">베이스라인</th><th align="center">재구성 후</th></tr></thead><tbody><tr><td align="center"><code>train.py</code></td><td align="center">중복작성된 processing 함수들</td><td align="center"><code>processes.py</code>로부터 import</td></tr><tr><td align="center"><code>inference.py</code></td><td align="center">^^</td><td align="center">^^</td></tr><tr><td align="center"><code>utils_qa.py</code></td><td align="center">postprocessing 함수 및 유틸리티 함수들</td><td align="center">X</td></tr><tr><td align="center"><code>trainer_qa.py</code></td><td align="center">Custom huggingface Trainer 클래스</td><td align="center">X</td></tr><tr><td align="center"><code>processes.py</code></td><td align="center">X</td><td align="center">processing 함수들</td></tr><tr><td align="center"><code>metric.py</code></td><td align="center">X</td><td align="center">점수 산출을 위한 metric 함수</td></tr><tr><td align="center"><code>utils.py</code></td><td align="center">X</td><td align="center">유틸리티 함수</td></tr></tbody></table><p>위에서 언급했던 문제를 해결하기 위해 postprocessing 과정을 변경하였으며, huggingface의 기본 <code>Trainer</code>를 사용해 학습할 수 있도록 하였습니다. 이렇게 구성된 <code>train.py</code> 는 아래와 같습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>():</span><br>    tokenizer, model, dataset 등의 initialization<br>    trainset, evalset의 mapping<br><br>    trainer의 initialization<br>    trainer.train()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    train()<br></code></pre></td></tr></table></figure><p>하지만 베이스라인 코드의 의도를 몰라봤을 수도 있고, 이상하게 해석했을 여지가 남아있습니다. 그럼에도 나름대로 제공된 베이스라인 코드를 분석해보며 새롭게 reproduction 한 것에 의의를 두게 되었습니다. </p><div class="note note-success">            <p>이 과정을 통해 ODQA Task 가 어떻게 이루어지는지 Scratch 부터 파악할 수 있었으며, 모델의 input 부터 추론과 Metric 은 물론 output 까지 명확히 파악할 수 있었습니다.</p>          </div><h1 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h1><p><img src="/images/mrc/image-20211109060404922.png" alt="Dataset"></p><p>주어진 학습 데이터의 샘플은 3,952개로 매우 적은 양이었습니다. 사전학습된 모델로 Fine-tuning을 진행했을 때 쉽게 Overfitting이 일어날 거란 판단을 하게 되었고, <span class="linear_highlight">Augmentation에 집중하여 좀 더 Robust한 모델이 될 수 있도록</span> 하고 다양한 Augmentation들을 진행한 후 모델에 쉽게 공급하기 위해서 해당 파이프라인을 구축하는 것에 집중할 수 있도록 계획을 세웠습니다.</p><p>이후에는 Retrieval과 Reader 두 가지 성능이 모두 중요했기 때문에 성능을 올리기 위한 방법을 찾고 구현하는 쪽에 집중하며, 최종적으로는 <span class="linear_highlight">다양한 Ensemble</span>을 통해 모델의 성능을 극대화시키고자 했습니다.</p><h1 id="Retrieval"><a href="#Retrieval" class="headerlink" title="Retrieval"></a>Retrieval</h1><div class="note note-info">            <p><code>Retrieval</code>은 질문에 맞는 문서를 찾는 것을 뜻합니다. Retrieval 을 수행하기 위해서는 질문에 대한 문서들의 Rank 를 계산해야하는데, 이를 위해서 질문과 문서를 Embedding 해야 합니다. 이 때 Embedding 된 벡터의 성질에 따라서 Sparse Embeddding 과 Dense Embedding 으로 분류할 수 있습니다. </p>          </div><div class="note note-warning">            <p>주의해야할 사항으로는, 둘 중 하나가 다른 것보다 무조건 ‘좋다’라기 보다는 각각이 강점을 가지는 분야가 다르다는 것이며 그로인해 상황에 맞추어 더 적합한 방법을 선택해야 할 필요가 있습니다.</p>          </div><p>이번 대회의 Inference 는 Test dataset의 question에 대해서 Retrieval을 통하여 유사한 Top-k 개의 passage를 가져오도록 한 뒤 해당 passage들을 question 뒤에 concat 하여 reader 모델로 정답을 예측하는 과정으로 이루어졌습니다. <span class="linear_highlight"> <code>k</code>를 늘릴 수록 Retrieval의 성능은 향상되었으나, reader 모델의 혼란이 가중된다</span>는 문제가 있었기에 Retrieval의 성능을 개선할 필요가 있었습니다.</p><h2 id="Sparse-Embedding"><a href="#Sparse-Embedding" class="headerlink" title="Sparse Embedding"></a>Sparse Embedding</h2><p>다양한 Sparse Embedding 방법론 중 저희는 TF-IDF 와 BM25 를 집중적으로 사용하였습니다.</p><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>TF-IDF는 여러 문서로 이루어진 문서군이 있으면, 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치입니다. 이를 이용해 질문에 대해 정답이 있을 만한 문서를 찾아낼 수 있습니다. TF와 IDF의 곱으로 문서의 랭킹을 매기며 TF와 IDF가 의미하는 건 다음과 같습니다.</p><div class="note note-info">            <p><strong>TF</strong></p><ul><li>Term Frequency의 약자로 단어의 등장 빈도를 말합니다.</li><li>특정 단어가 <strong>문서</strong> 내에 얼마나 자주 등장하는지를 나타내고, 이 값이 높을수록 문서에서 중요하다고 가정 할 수 있습니다.</li></ul><p><strong>IDF</strong></p><ul><li>Inverse Document Frequency의 약자로 단어가 제공하는 정보의 양을 나타냅니다.</li><li>DF는 단어가 <strong>문서군</strong>내에 등장하는 빈도를 의미하며 IDF는 이것의 역수를 취한 후 로그를 취하여 얻습니다.</li><li>the, a 같은 관사는 TF는 높아도, 거의 모든 문서에 등장하게 되므로 IDF의 값이 0에 가깝기 때문에 단어가 나타내는 정보 자체는 낮다고 할 수 있습니다.</li><li>자주 등장하지 않는 고유 명사 같은 경우 IDF가 커지기 때문에 단어 자체가 가지는 정보량은 많다고 불 수 있습니다.</li></ul>          </div><p>TF 값과 IDF의 값을 곱해 주는 것으로 문서에 대한 Sparse Embedding을 진행 할 수 있습니다.</p><p>TF-IDF를 이용한 Retrieval은 다음과 같은 순서로 진행됩니다.</p><div class="note note-secondary">            <ol><li>사용자가 입력한 query를 토큰화</li><li>기존에 단어 사전에 없는 토큰들은 제외</li><li>query를 생각하고 이에 대한 TF-IDF 계산</li><li>query의 TF-IDF 값과 문서군들의 TF-IDF 값을 곱하여 유사도 계산</li><li>가장 높은 점수의 문서를 선택</li></ol>          </div><h3 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h3><p>BM25 는 TF-IDF의 개념의 바탕으로 문서의 길이까지 고려하여 점수를 산정합니다.</p><p><img src="/images/mrc/zNfTAJg1U-EGGBaiosZoq7JkersOT6JrKY7fJn_LHEK-SgcNd2D6O2n6kT-8w8iMjwjnTQxwjZfle2LJD1FpaLvNLFVQYwafhguw340hXHoSkrP1H6vuoM9h5wLYycUP-WE-Od5D.png"></p><p>BM25는 실제 검색엔진, 추천 시스템 등에서 아직까지도 많이 사용되는 알고리즘이며 TF-IDF 보다 Retrieval 성능이 뛰어났습니다. 하지만 Vectorize 를 하는 과정, 주어진 Passage 에 fitting 을 진행하는 과정, 그리고 Top-k passage 를 가져오도록 할 때 많은 시간이 소요된다는 문제가 있었습니다.</p><h2 id="Elastic-Search"><a href="#Elastic-Search" class="headerlink" title="Elastic Search"></a>Elastic Search</h2><div class="note note-info">            <p><a href="https://ko.wikipedia.org/wiki/%EC%9D%BC%EB%9E%98%EC%8A%A4%ED%8B%B1%EC%84%9C%EC%B9%98">Elastic Search</a>는 확장성이 뛰어난 오픈소스 풀텍스트 검색 및 분석 엔진입니다. 방대한 양의 데이터를 신속하게, 거의 실시간으로 저장, 검색, 분석할 수 있도록 지원합니다. 일반적으로 복잡한 검색 기능 및 요구 사항이 있는 애플리케이션을 위한 기본 엔진/기술로 사용됩니다.</p>          </div><p>Elastic Search 는 검색엔진으로 뛰어난 속도로 방대한 양의 문서를 검색할 수 있습니다. 저희는 BM25 알고리즘과 Nori Tokenizer 기반의 Elastic Search 를 도입함으로써 <span class="linear_highlight">단순 BM25 를 사용하는 것보다 약 2배 가량 빠른 시간</span>에 Retrieval 을 진행할 수 있었습니다. 또한 동일한 BM25 를 사용하더라도 Tokenizer 에 따른 성능차이가 존재했습니다. 많은 Tokenizer 를 실험해보진 못했으나, 그 중에서 가장 성능이 좋았던 Nori Tokenizer 를 사용하게 되었습니다.</p><p>Validation Dataset 에 대해서 벤치마크를 측정했을 때, Top 20 개의 Retrieval 기준 Elastic Search 만으로도 93.75% 의 매우 높은 성능을 자랑했습니다. 하지만 유사한 문서를 많이 가져올수록 Reader 의 혼란은 가중될 것이므로 Retrieval 의 성능을 극대화 시켜서 Retrieval 로 가져온 문서를 최대한 적게 하자는 생각을 하게 되었습니다.</p><h2 id="Dense-Embedding"><a href="#Dense-Embedding" class="headerlink" title="Dense Embedding"></a>Dense Embedding</h2><p>Sparse Embedding을 사용하면 검색하고자 하는 모든 문서를 이루고 있는 word에 대해 해당 문서가 가지고 있는 word를 표시해야 하므로 차원의 수가 매우 커지고 대부분의 특성이 0이 되는 문제가 있었습니다. 이러한 문제와 위의 생각을 바탕으로 Retrieval 의 성능을 향상시키기 위한 방법을 고려하던 중 Dense Embedding 을 도입해보기로 하였습니다. </p><div class="note note-info">            <p>Dense Embedding 은 문서들을 Embedding 할 때 Sparse Embedding보다는 낮은 차원, 고밀도 벡터로 Embedding 하는 것을 뜻합니다</p>          </div><p>Dense Embedding 에서의 각 차원은 특정 term에 대응되지 않고 대부분의 값이 0이 아닌 값들이기 때문에 각 Token 들의 Semantic 한 정보를 효율적으로 담아낼 수 있다고 생각하였습니다. </p><p>아이디어를 얻게 된 논문은 <a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a> 으로, 단순 Sparse Embedding 보다는 성능이 안좋았었지만 두 개의 BERT Encoder 를 사용하면서 성능이 비약적으로 발전하게 되었다고 합니다. </p><p><img src="/images/mrc/8.png" alt="DPR paper benchmark"></p><p>논문에서는 ODQA Task 에서 대부분 BM25 보다 DPR 의 성능이 좋다는 실험결과가 있었기 때문에 충분히 도입을 고려해볼만하다고 생각하게 되었습니다.</p><h3 id="Dense-Embedding의-생성-학습"><a href="#Dense-Embedding의-생성-학습" class="headerlink" title="Dense Embedding의 생성, 학습"></a>Dense Embedding의 생성, 학습</h3><p><img src="/images/mrc/4.png"></p><div class="note note-info">            <p>DPR(Dense Passage Retrieval) 에서는 두 개의 BERT Encoder 를 사용하여 Question 과 Passage 에 대한 Dense Embedding 을 생성한 후 내적을 통해 Similarity Score 를 구하는 방식으로 학습이 이루어집니다. 이때 각 배치에서 매칭되는 embedding vector들의 내적값은 정답으로 간주해 점수를 최대화 하고, 매칭되지 않는 vector들의 값들은 오답으로 간주해 점수를 최소화함으로 질문과 문서의 유사도 점수를 구하는 모델을 만들 수 있습니다.</p>          </div><p><img src="/images/mrc/image-20211111152754625.png" alt="In-batch negative"></p><blockquote><p>Finally, we explore in-batch negative training with additional ‘hard’ negative passages that have high BM25 scores given the question, but do not contain the answer string (the bottom block). These additional passages are used as negative passages for all questions in the same batch. We find that adding a single BM25 negative passage improves the result substantially while adding two does not help further.</p></blockquote><p>참고한 논문에서는 이렇게 점수를 최대화 할때 오답들을 배치의 매칭되지 않는 값으로 하는 것에 추가로 question에 대한 정답이 passage에는 없지만 BM25 점수가 높은 문장들을 추가적으로 오답으로 간주하게 하여 (Hard negative passage) 학습시키는 방법론을 제시합니다.</p><p><img src="/images/mrc/image-20211111152828239.png" alt="In-batch hard negative"></p><p>저희는 이러한 방법론을 도입하여 BM25 로 Negative Sampling 을 진행한 뒤 유사도 행렬에서 주대각선의 값들이 최대가 되는 방향으로 학습을 진행하였습니다.</p><p>하지만 막상 실험을 해보니 DPR의 성능은 만족스럽지 못했고, 오히려 TF-IDF보다도 점수가 낮았습니다. 이러한 원인으로 아래와 같은 이유를 예상해볼 수 있었습니다.</p><div class="note note-warning">            <ol><li>학습 데이터에는 질문에 주요 Term 들이 많이 포함되어 있었기 때문에 오히려 질문에 포함된 단어들의 의미를 <code>추론</code>하여 정답을 뽑는 것이 아니라 질문만 보고도 어느정도 답을 예측할 수 있었기 때문일 것이다.</li><li>데이터셋을 제작한 Annotator 가 질문에 대한 답을 알고 있는 상태로 질문을 생성했기 때문에 어느정도의 bias 가 생겼을 것이다.</li></ol>          </div><blockquote><p>We conjecture that the lower performance on SQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As a result, there is a high lexical overlap between passage and questions, which gives BM25 a clear advantage. Second, the data was collected from only 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as argued previously by Lee et al. (2019).</p></blockquote><ul><li>저희가 참고한 논문에서는 SQuAD 데이터셋이 이러한 문제점을 안고 있어서 DPR 보다 BM25 의 성능이 뛰어나다고 언급하고 있기 때문에 비슷한 이유를 예상해볼 수 있었습니다.</li></ul><h2 id="Bi-Encoder-Embedding-amp-Cross-Encoder-Rerank"><a href="#Bi-Encoder-Embedding-amp-Cross-Encoder-Rerank" class="headerlink" title="Bi-Encoder Embedding &amp; Cross-Encoder Rerank"></a>Bi-Encoder Embedding &amp; Cross-Encoder Rerank</h2><p>다음으로 시도해본 것은 Bi-Encoder Embedding 을 사용하여 관련 문서를 가져온 뒤 Cross-Encoder 로 Retrieval 을 Rerank 하는 방식입니다. 해당 문서 <a href="https://sbert.net/examples/applications/retrieve_rerank/README.html">Retrieve &amp; Re-Rank</a> 에서 아이디어를 얻을 수 있었습니다.</p><div class="note note-info">            <p>Sentence BERT 는 Bi-Encoder 를 사용하여 Large-scale 의 <code>Similarity Comparison, Clustering, Information Retrieval</code> 로 활용할 수 있는 Sentence Embedding 방법론입니다.</p>          </div><p>기존의 BERT 로 유사한 두 문장을 찾기 위해서는 <code>(n * (n - 1)) / 2</code> 번의 연산을 수행해야 했기 때문에 굉장히 비효율적이며 연산량이 많았습니다. Sentence BERT 는 이러한 문제를 해결하고 Siamese 네트워크로 문장을 벡터화할 수 있는 방법을 제안했습니다. 따라서 STS(Semantic Textual Similarity) 데이터셋으로 사전학습된 Sentence BERT 를 사용하면 보다 빠르게 더욱 유사한 Embedding 을 추출할 수 있을 것으로 예상했습니다.</p><p>Bi-Encoder Embedding &amp; Cross-Encoder Rerank 방식으로 Retrieval 을 수행하는 코드는 아래와 같습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">search</span>(<span class="hljs-params">query, k</span>):</span><br>    bi_encoder_retrieval, cross_encoder_retrieval = [], []<br><br>    <span class="hljs-comment">##### Sematic Search #####</span><br>    <span class="hljs-comment"># Encode the query using the bi-encoder and find potentially relevant passages</span><br>    question_embedding = bi_encoder.encode(query, convert_to_tensor=<span class="hljs-literal">True</span>)<br>    question_embedding = question_embedding.cuda()<br>    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=<span class="hljs-number">100</span>)<br>    hits = hits[<span class="hljs-number">0</span>]  <span class="hljs-comment"># Get the hits for the first query</span><br><br>    <span class="hljs-comment">##### Re-Ranking #####</span><br>    <span class="hljs-comment"># Now, score all retrieved passages with the cross_encoder</span><br>    cross_inp = [[query, corpus[hit[<span class="hljs-string">&#x27;corpus_id&#x27;</span>]]] <span class="hljs-keyword">for</span> hit <span class="hljs-keyword">in</span> hits]<br>    cross_scores = cross_encoder.predict(cross_inp)<br><br>    <span class="hljs-comment"># Sort results by the cross-encoder scores</span><br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(cross_scores)):<br>        hits[idx][<span class="hljs-string">&#x27;cross-score&#x27;</span>] = cross_scores[idx]<br><br>    <span class="hljs-comment"># Output of top-k hits from bi-encoder</span><br>    hits = <span class="hljs-built_in">sorted</span>(hits, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&#x27;score&#x27;</span>], reverse=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">for</span> hit <span class="hljs-keyword">in</span> hits[<span class="hljs-number">0</span>:k]:<br>        bi_encoder_retrieval.append(hit[<span class="hljs-string">&#x27;corpus_id&#x27;</span>])<br><br>    hits = <span class="hljs-built_in">sorted</span>(hits, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">&#x27;cross-score&#x27;</span>], reverse=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">for</span> hit <span class="hljs-keyword">in</span> hits[<span class="hljs-number">0</span>:k]:<br>        cross_encoder_retrieval.append(hit[<span class="hljs-string">&#x27;corpus_id&#x27;</span>])<br><br>    <span class="hljs-keyword">return</span> bi_encoder_retrieval, cross_encoder_retrieval<br></code></pre></td></tr></table></figure><p>Top 1 의 Retrieval 을 수행했을 때는 DPR 보다 높은 정확도를 기록하였지만, k 의 개수를 높일수록 DPR 보다 성능이 낮게 나왔는데, 그 이유로는 저희가 사용하던 학습데이터에 추가적인 fitting 을 진행하지 않았기 때문으로 예상됩니다. Bi-Encoder 를 학습시키기 위해서는 유사도를 기반으로 labeling 된 데이터가 필요했으며, <span class="linear_highlight"><code>sentence_transformer</code> 라이브러리에 대한 추가적인 학습 등 fitting 을 진행하기엔 많은 비용이 필요</span>했습니다. </p><p>만약 이 방법을 보다 빠른 시점에 시도하고 실험을 진행할 수 있었으면 Retrieval 의 성능을 보다 비약적으로 발전시킬 수 있었을 것이란 생각을 하게 되었습니다.</p><h2 id="Hybrid-BM25-DPR"><a href="#Hybrid-BM25-DPR" class="headerlink" title="Hybrid (BM25 + DPR)"></a>Hybrid (BM25 + DPR)</h2><p>하지만 위와 같은 이유들에도 불구하고 저희는 DPR 의 배경과 장점, 가능성에 공감하여 어떻게든 DPR 을 활용해보려고 했으며, BM25 와 같은 Sparse Embedding 의 단점인 <code>Semantic</code> 한 단어들을 포착하는데 어려움이 있다는 것을 보완하기 위하여 <span class="linear_highlight">DPR 과 Elastic Search 로 나온 Retrieval 결과를 혼합하여 사용하는 방식을 도입</span>해보기로 하였습니다. 논문에서는 <code>Multi Training (BM25 + DPR)</code> 라는 키워드로 언급하고 있으나, 저희는 <code>Hybrid Retrieval</code> 라는 이름으로 명명하였습니다.</p><div class="note note-info">            <p>Hybrid Retrieval 은 BM25 Score 에 DPR 의 내적값인 <code>sim(q, p)</code> 를 더하여 문서를 Re-ranking 하는 방식입니다.</p>          </div><p><code>유령은 어느 행성에서 지구로 왔는가?</code> 라는 질문에 대한 답을 포함하는 문서의 Title 은 <code>더크 젠틀리의 성스러운 탐정사무소</code> 입니다. 해당 질문에 대한 Elastic Search, DPR, Hybrid Retrieval 의 예시는 아래와 같습니다.</p><figure class="highlight python"><figcaption><span>Elastic Search</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">----------------------------------------------------------------------------------------<br>top_1 passage score : <span class="hljs-number">20.487246</span><br>목성의 대기에서 보이는 줄무늬는 적도와 평행하면서 행성을 둘러싸는 대(zone)와 띠(belt)라고 불리는 물질의 반대 순환류에 의한 것이다. 대는 밝은 줄무늬로, 대기에서 상대적으로 고도가 높은 곳에 있다. 이들은 내부의 상승 기류를 가지고 있는 고기압 영역이다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br>top_2 passage score : <span class="hljs-number">20.10372</span><br>갈리프레이 (Gallifrey)는 영국의 SF 텔레비전 드라마 《닥터 후》에서 등장하는 행성이다. 드라마의 주인공인 닥터와 마스터를 비롯한 지금까지 등장한 모든 타임 로드의 고향이다. 카스터보로스 성단 내에서 <span class="hljs-string">&quot;은하 중심에서 은하좌표로 10-0-11-0-0 하고도 0-2 지점&quot;</span>에 위치해 있으며 쌍성계를 이루고 있다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br>top_3 passage score : <span class="hljs-number">19.851677</span><br>행성의 역행 운동은 지구가 보다 느린 외행성을 지날 때나, 보다 빠른 행성이 지구를 지날 때, 하늘을 통해서 행성이 거꾸로 이동하는 것으로 보이는 것이다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br></code></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>DPR</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">----------------------------------------------------------------------------------------<br>top_1 passage score : <span class="hljs-number">390.04132080078125</span><br><span class="hljs-number">1</span>) <span class="hljs-number">4</span>:<span class="hljs-number">774</span>는 Korean Embassy Cases란 파일명으로 <span class="hljs-number">1953</span>년 <span class="hljs-number">1</span>월부터 <span class="hljs-number">1956</span> <span class="hljs-number">5</span>월 사이에 있었던 일들의 기록이 <span class="hljs-number">106</span> frames에 걸쳐 보관되었다. 여기서 다루고 있는 중요 사안은 Korean official seals, sword, <span class="hljs-keyword">and</span> crown 등으로 미군 병사에 의해 한국에서 약탈된 문화재의 기록을 언급하고 있다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br>top_2 passage score : <span class="hljs-number">388.6401672363281</span><br>허블 우주 망원경은 TrES-1b에서 물이 있을 가능성이 있음을 시사하는 자료를 확보했으며, 행성의 크기를 더욱 정확하게 알 수 있었고, 위성이 존재할 가능성도 있음을 시사해 주었다. 그러나 관측된 궤도요소로 미루어 보아 TrES-1b 주위에서는 위성이 발견되지 않았다고 연구팀은 결론지었다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br>top_3 passage score : <span class="hljs-number">388.4365234375</span><br>카스텔랑은 <span class="hljs-number">1969</span> 년부터 <span class="hljs-number">1973</span> 년까지 건설되었으며 그해 <span class="hljs-number">11</span>월 <span class="hljs-number">11</span>일에 완공되었다, <span class="hljs-number">2000</span>년 <span class="hljs-number">5</span>월 세아라 주 정부는 경기장을 개혁하기 시작했습니다. 개혁은 <span class="hljs-number">3</span> 단계로 나뉘어 <span class="hljs-number">2001</span>년 <span class="hljs-number">5</span>월 <span class="hljs-number">16</span>일에 시작되었다. 첫 번째 단계는 도랑과 관람석 교차점의 복구와 낮은 벽의 복구로 구성되었다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br></code></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>Hybrid</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">----------------------------------------------------------------------------------------<br>top_1 passage score : <span class="hljs-number">403.3033144296875</span><br>더크 젠틀리의 성스러운 탐정사무소의 줄거리는 이야기의 중추적인 부위에 자리잡은 시간 여행이란 주제 때문에 줄거리가 이어져 있지 않다. <span class="hljs-number">40</span>억년전 지구에서, **사락사라**의 사람들이 자신들만의 낙원을 만들기 위해 지구에 착륙하였다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br>top_2 passage score : <span class="hljs-number">402.4703392617188</span><br>목성의 대기에서 보이는 줄무늬는 적도와 평행하면서 행성을 둘러싸는 대(zone)와 띠(belt)라고 불리는 물질의 반대 순환류에 의한 것이다. 대는 밝은 줄무늬로, 대기에서 상대적으로 고도가 높은 곳에 있다. 이들은 내부의 상승 기류를 가지고 있는 고기압 영역이다. &lt;생략&gt;<br>----------------------------------------------------------------------------------------<br>top_3 passage score : <span class="hljs-number">401.86636352148435</span><br>은하수를 여행하는 히치하이커를 위한 안내서는 방대한 지식의 은하대백과사전보다 더 유명한 책인데, 그 이유는 은하대백과사전보다 더 싸다는 것, 그리고 앞에다가 큼지막고 친근하게 당황하지 마시오(Don<span class="hljs-string">&#x27;t Panic)라고 적어놓은 것이었다. &lt;생략&gt;</span><br><span class="hljs-string">----------------------------------------------------------------------------------------</span><br></code></pre></td></tr></table></figure><div class="note note-success">            <p>Elastic Search 와 DPR 모두 질문에 대한 정답이 있는 문서를 찾지 못했으나, 이 둘의 점수를 선형결합한 Hybrid retrieval 은 정답이 포함된 문서를 1순위로 가져오는 것을 확인할 수 있었습니다.</p>          </div><h2 id="Retrieval-성능평가"><a href="#Retrieval-성능평가" class="headerlink" title="Retrieval 성능평가"></a>Retrieval 성능평가</h2><p>저희는 위에서 실험한 모든 Retrieval 의 성능평가를 진행했고, 그 결과는 아래와 같습니다. 모든 구간에서 Hybrid Retrieval 의 성능이 가장 높았으며, 저희의 예상대로 DPR 이 BM25 의 단점을 잘 보완해준다는 것을 확인할 수 있었습니다.</p><p><img src="/images/mrc/Top-k_Retrieval_Acc.png" alt="Top-k Retrieval Acc."></p><p><img src="/images/mrc/image-20211107042614732.png" alt="Retrieval Benchmark (Acc.)"></p><p><span class="linear_highlight">최종적으로 가장 성능이 좋았던 Hybrid Retrieval 을 선택하게 되었고, Top-k 로 가져오는 Passage 의 개수는 Reader 모델을 실험하고 Leaderboard 에 제출하면서 최적의 개수를 찾기로 결정하였습니다.</span></p><h1 id="Reader"><a href="#Reader" class="headerlink" title="Reader"></a>Reader</h1><div class="note note-info">            <p><code>Reader</code>는 주어진 질문에 대해 Retrieval이 가져온 문서를 읽고 정답을 찾아내는 모델입니다. </p>          </div><p>Reader의 경우 <code>Extractive</code> 와 <code>Generative</code> 방식이 존재하는데, Extractive Reader는 문서 내에서 모델이 정답일 것으로 예측하는 <code>토큰의 시작과 마지막 인덱스</code>의 확률(logit)을 구하고 이들의 합이 최대가 되는 문자열을 정답으로 예측합니다. 반면 Generative Reader는 문서를 읽고 정답일 것으로 예측되는 <code>토큰을 생성</code>하여 만들어낸 문자열을 정답으로 예측합니다.</p><p>학습데이터의 양이 매우 적었기 때문에 <code>사전학습된 한국어 모델을 활용</code>하고자 했으며, Extractive 방식에 집중하고자 했습니다. 영어의 경우 일반적으로 Generative Reader가 성능이 좋다고 알려져 있습니다. 왜냐하면 <span class="linear_highlight">Extractive Reader의 경우 정답이 반드시 문서 내에 존재해야 한다는 단점이 있는 반면, Generative Reader는 문서 내에 정답이 존재하지 않아도 충분히 정답이 될 토큰을 생성해낼 수 있기 때문입니다.</span> 하지만 Huggingface Hub에 공개되어 있는 사전학습된 한국어 생성 모델중에 중에 저작권 문제 없는 모델들은</p><div class="note note-secondary">            <ul><li><code>hyunwoongko/kobart</code> (seq2seq)</li><li><code>google/mt-5</code> 계열 (seq2seq)</li><li><code>kykim/gpt3-kor-small_based_on_gpt2</code> (gpt2)</li></ul>          </div><p>위의 세 가지 정도였고, Extractive Reader로 사용할 수 있는 Language Model들과는 다르게 모델의 크기가 적고 실제 Baseline 성능평가 당시 사전학습된 Extractive Reader로 Fine-tuning 하는 것보다 성능이 낮았기 때문에 Generative 방식은 사용하지 않았습니다.</p><p><span class="linear_highlight">저희는 Reader 의 성능 개선을 위한 모든 아이디어들에 대하여 실험을 진행하였고, 실험 결과들에 의해서만 좋은 방법들을 취사선택하였습니다.</span></p><h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p><img src="/images/mrc/image-20211107045109925.png" alt="Backbone 성능평가"></p><p>Extractive 방식으로 사용할 수 있는 여러 가지 사전학습 모델로 성능평가를 진행했고, 테스트 결과 validation EM 기준 가장 점수가 높았던 <code>klue/roberta-large</code> 모델을 backbone 으로 삼게 되었습니다.</p><h2 id="Performance-evaluation-of-backbone-according-to-retrieval"><a href="#Performance-evaluation-of-backbone-according-to-retrieval" class="headerlink" title="Performance evaluation of backbone according to retrieval"></a>Performance evaluation of backbone according to retrieval</h2><p>사전학습된 Backbone 모델의 정확한 성능평가를 위하여 baseline 에 주어진 inference 코드로 evaluation 을 진행했습니다. 처음에 주어졌던 retrieval 방식은 TF-IDF 를 이용하여 질문에 대한 Top-k 개의 문서를 가져온 뒤 아래와 같이 불러온 문서를 모두 붙여서 정답의 시작과 마지막 인덱스를 예측하도록 이루어져 있었습니다. 질문에 Top-k 개의 문서를 붙인 뒤 Inference를 하기 위한 모델의 input 형식은 아래와 같습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">question + [SEP] + passage_1 + <span class="hljs-string">&#x27; &#x27;</span> + passage_2 + <span class="hljs-string">&#x27; &#x27;</span> + ... + passage_k<br></code></pre></td></tr></table></figure><p><img src="/images/mrc/image-20211109062628253.png" alt="TF-IDF top-k passage inference"></p><div class="note note-success">            <p>K의 개수를 늘릴 수록 public leaderboard 와 validation score 의 간극이 줄어드는 것을 확인하였고, 이러한 retrieval 방식에 착안하여 다음 실험을 계획할 수 있었습니다.</p>          </div><h2 id="Concat-Top-k-Negative-Samples"><a href="#Concat-Top-k-Negative-Samples" class="headerlink" title="Concat Top-k Negative Samples"></a>Concat Top-k Negative Samples</h2><p>해당 방법의 아이디어를 얻은 논문은 Retrieval 과정에서 많이 참고한 <a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a> 입니다. 해당 논문에서는 Retrieval을 위한 Dense Embedding을 만들 때 </p><blockquote><p> Our best model uses gold passages from the same mini-batch and one BM25 negative passage.</p></blockquote><p>BM25를 통해 가져온 negative sample을 추가하여 같이 학습시켰을 때가 가장 높은 퍼포먼스를 내는 모델이었다고 언급합니다. <span class="linear_highlight">이 실험에 착안하여 Reader 모델 학습 때 역시 negative sample을 추가하여 학습시키면 성능이 향상될 것이란 기대를 하게 되었습니다.</span></p><p>해당 실험을 통해 성능이 향상될 것이란 기대를 하게된 근거는 아래와 같습니다.</p><div class="note note-danger">            <ol><li>현재 Inference 는 Retrieval 로부터 질문과 유사한 Top-k 개의 passage를 불러온 뒤 concat 하고 그 안에서 Extractive Reader 가 정답을 찾게 하는 과정으로 이루어진다. Inference 과정처럼 Reader 모델 학습 때도 여러 passage를 concat 해놓고 정답을 찾게 하는식으로 최대한 비슷한 환경을 조성해 놓으면 예측 때 모델의 혼란이 줄어들 것이다.</li><li>Retrieval 이 upgrade 되더라도 실험의 전제가 변하지 않을 것이고, 동일하게 학습시켜놓았던 모델로 더 좋은 결과를 얻을 수 있을 것이다.</li><li>answer 키워드가 반복될 수 있는 positive sample 을 사용하면 오히려 모델의 혼란이 가중될 것이다.</li></ol>          </div><p><img src="/images/mrc/image-20211109095217407.png" alt="W&amp;B line chart concat negative samples"></p><p><span class="linear_highlight">실험결과 예상과 같이 여러 passage를 concat 하였을 때 기존 validation score 보다 높은 점수를 기록하였고, possitive sample 보다는 negative sample 을 concat 하여 Reader 를 학습시켰을 때 훨씬 높은 점수를 기록할 수 있었습니다.</span></p><p>Negative sample 을 선택하는 방법에 따라서도 Reader 의 성능이 천차만별이었습니다. 특히 Retrieval과 Tokenizer 의 영향을 가장 많이 받을 것으로 예상했고, 두 가지에 집중하여 가장 좋은 성능을 내는 negative sample 을 뽑을 수 있도록 하였습니다.</p><p>사용할 수 있는 Retrieval 은 BM25, Elastic search, Hybrid retrieval 이 있었으며, Tokenizer 는 Elastic search 에 사용되는 Nori Tokenizer 와 BERT Tokenizer 가 있었습니다. 저희는 이 조합들을 사용하여 validation 을 측정해보았고 실험결과는 아래와 같습니다.</p><p><img src="/images/mrc/image-20211109101344173.png" alt="W&amp;B line chart according to negative sampling"></p><p>결국, 최종적으로 사용한 Negative Sampling 기준은 아래와 같습니다.</p><div class="note note-primary">            <p>Elastic Search 와 Nori Tokenizer 를 사용하며, 정답을 포함하지 않는 상위 4개의 전처리된 passage</p>          </div><p>처음에는 Retrieval 성능이 올라갈수록 모델이 더 혼동하기 쉬운 Negative Sample 을 많이 포함시켜 Reader 의 성능이 오를 것으로 예상하였으나, 막상 실험을 해보니 Hybrid Retrieval 보다는 BM25 계열의 Sparse Embedding 을 활용하여 negative sampling 을 진행했을 때 결과가 더 좋았습니다.</p><p>이러한 원인을 <code>Hybrid Retrieval 로 Negative Sampling 을 진행한 Dataset 에 중복되는 문서들이 더 많다</code> 는 것으로 예상할 수 있었습니다.</p><p>기존 Train dataset 에는 약 500 건 정도의 ground truth passage 가 중복이었고, 저희가 사용하는 wiki 에는 중복되는 Passage 가 약 3,000 개, 같은 title 의 문서이지만 문단이 다른 경우가 약 29,000 개로 unique 한 wiki 문서는 총 31,755 개였습니다. </p><p><img src="/images/mrc/Chart_Title_(1).png" alt="Concat passage distribution"></p><p>이 때 저희의 방식으로 Train dataset 에 대하여 Top-k (k=4) 개의 문서를 합쳤을 때 총 19,760 개의 wiki passage 를 보게 되는데, 위의 차트와 같이 Hybrid Retrieval 로 Negative Sampling 을 진행하였을 때 중복된 문서가 약 <code>1,400 개</code> 정도 더 포함되어 있었습니다. <span class="linear_highlight">또한 Semantic 한 정보까지 포함하여 고려하는 Hybrid Retrieval 을 사용하게 되면 중복된 Ground Truth 에 포함되는 Negative Sample 들이 대체로 일치했습니다. 따라서 이미 너무나도 쉽게 Over-fitting 이 발생하는 상황에서 중복되는 Passage 를 많이 포함시키며 오히려 Negative Sample 로서 기대했던 역할을 제대로 수행하지 못하게 되었다고 생각했습니다.</span></p><h2 id="hyperparameter-tuning"><a href="#hyperparameter-tuning" class="headerlink" title="hyperparameter tuning"></a>hyperparameter tuning</h2><p>위의 실험들을 바탕으로 Elastic Search 로부터 Top-k 개의 negative sample 을 포함하여 데이터셋을 재구축하였고, 그 중 가장 성능이 높았던 k=4 를 기준으로 hyperparameter tuning 을 진행했습니다. <code>Optuna</code> 라이브러리와 Huggingface Trainer 에 존재하는 <code>hyperparameter_search</code> 함수를 사용하였고, batch size, learning rate, seed, gradient accumulation step, weight decay 다섯 개에 대하여 validation EM score 를 maximize 할 수 있도록 search 를 진행했습니다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_hp_space</span>(<span class="hljs-params">trial</span>):</span><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;learning_rate&quot;</span>: trial.suggest_float(<span class="hljs-string">&quot;learning_rate&quot;</span>, <span class="hljs-number">1e-6</span>, <span class="hljs-number">1e-4</span>, log=<span class="hljs-literal">True</span>),<br>        <span class="hljs-string">&quot;seed&quot;</span>: trial.suggest_int(<span class="hljs-string">&quot;seed&quot;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">123</span>),<br>        <span class="hljs-string">&quot;per_device_train_batch_size&quot;</span>: trial.suggest_categorical(<br>            <span class="hljs-string">&quot;per_device_train_batch_size&quot;</span>, [<span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">16</span>]<br>        ),<br>        <span class="hljs-string">&quot;weight_decay&quot;</span>: trial.suggest_float(<span class="hljs-string">&quot;weight_decay&quot;</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.3</span>),<br>        <span class="hljs-string">&quot;gradient_accumulation_steps&quot;</span>: trial.suggest_categorical(<br>            <span class="hljs-string">&quot;gradient_accumulation_steps&quot;</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>]<br>        ),<br>    &#125;<br></code></pre></td></tr></table></figure><p><img src="/images/mrc/image-20211107051016138.png" alt="f1 &amp; batch size &amp; learning rate"></p><p><img src="/images/mrc/image-20211107051439508.png" alt="hyperparameter importance"></p><p>Hyperparameter Search 를 진행하면서 Validation Score 가 굉장히 Sensitive 하게 변동되는 경향이 있었고, 그 때문에 결과가 좋지 않은 Trial 에 대하여 Pruning 이 올바로 동작하지 않는 이슈가 있었습니다. 따라서 모델의 성능이 어떠한 Parameter 에 많은 영향을 받고 어떤 조합일 때 점수가 높게 분포되는지를 확인해보았습니다. 결과적으로 learning rate 의 영향을 상당히 많이 받으며, batch size 가 클 때 학습이 가장 잘 이루어진다는 것을 알 수 있었습니다.</p><p><img src="/images/mrc/image-20211107050716799.png" alt="sweep"></p><p>그렇게 약 60번의 trial 을 시도했고, Learning rate 와 batch size 에 주목하며 Search 를 진행하는 Hyperparameter 범위를 좁혀가며 실험을 진행했습니다. Search 가 끝난 이후에는 W&amp;B의 sweep 기능을 이용하여 가장 좋은 조합을 찾을 수 있었습니다.</p><p><img src="/images/mrc/image-20211107050457499.png"></p><div class="note note-success">            <p>그렇게 public leaderboard 기준 5 이상의 score 향상을 얻을 수 있었습니다.</p>          </div><h2 id="Custom-Heads"><a href="#Custom-Heads" class="headerlink" title="Custom Heads"></a>Custom Heads</h2><p>실험을 이어가면서 Test dataset 에 대해 Inference 를 하고 나온 결과를 살펴보던 중 아래의 예시들을 통해 의문을 가지게 되었습니다.</p><div class="note note-warning">            <ol><li>예측한 정답에 형용사, 조사 등이 포함되어 있는 경우 - <code>대통령인 빌헬름 미클라스</code>, <code>&lt;자유&gt;지에</code></li><li>예측한 정답의 substring 이 반복되는 경우 - <code>주노해변 주노해변</code></li><li>예측한 정답이 문장인 경우 - <code>남자아이 빈을 데려다 양자로 삼는다는 것에서..</code></li></ol>          </div><p>Extractive 방식에 따라 문서 내에 존재하는 토큰의 위치를 찾게 되는데, 위의 예시처럼 <code>형용사</code> 를 포함해야 할 것인지, <code>반복</code> 되는 키워드의 경우 어떻게 잘라내야 할 것인지, <code>문장</code> 이 정답이라면 문장 전체를 정답으로 할 지, 아니면 <code>명사구</code> 가 될 수 있도록 end position 을 이동시켜야 할 지 등에 대한 고민을 하게 되었습니다. 그리고 이러한 문제가 발생한 원인은 모델이 <span class="linear_highlight">‘일관적이지 않게’</span> 정답을 추출해내고 있기 때문일 것으로 예상하고 사용하고 있던 huggingface 의 QuestionAnswering 소스 코드를 살펴보았습니다.</p><p><img src="/images/mrc/image-20211110040706772.png" alt="RobertaForQuestionAnswering"></p><p>Huggingface 의 Extractive Reader 방식으로 사용되는 <a href="https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaForQuestionAnswering">QuestionAnswering</a> 모델의 head 는 backbone 의 sequence output shape 인 hidden_size 를 2(start_logit, end_logit) 로 축소한 Linear layer 가 사용됩니다. 저희가 backbone 으로 사용한 <code>klue/roberta-large</code> 의 경우 hidden_size 가 1,024 로 매우 컸는데, 실험을 이어가다 보니 <span class="linear_highlight">이렇게 큰 사이즈의 벡터를 바로 두 개의 logit 으로 축소한다는 것</span>에서 위의 예시와 같은 문제점들이 생겨날 수 있을 것으로 판단하게 되었습니다. 또한 <span class="linear_highlight">다양한 head 를 사용하고 튜닝함으로써 모델이 보다 일관적으로 정답을 예측</span>하고, 이러한 문제를 해결할 수 있을 것으로 기대하게 되었습니다. 마지막으로 최종 제출 전략인 Ensemble 을 고려하였을 때 <span class="linear_highlight">head 에 다양성을 준 모델의 퍼포먼스가 낮아도 Ensemble 의 좋은 재료가 될 수 있을 것</span>이란 기대로, 최대한 다양한 head 를 실험해보기로 하였습니다.</p><p>Head 에 custom layer 를 추가하기 위해 아래 논문과 블로그에서 아이디어를 얻게 되었습니다.</p><div class="note note-info">            <ul><li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15718571.pdf">Research of LSTM Additions on Top of SQuAD BERT Hidden Transform Layers</a></li><li><a href="https://arxiv.org/pdf/2001.09694v4.pdf">Retrospective Reader for Machine Reading Comprehension</a></li><li><a href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/">Implementing a CNN for Text Classification in TensorFlow</a></li><li><a href="https://ratsgo.github.io/natural%20language%20processing/2017/03/19/CNN/">CNN으로 문장 분류하기</a></li></ul>          </div><p>저희가 실험에 사용했던 Custom Layer 와 전체적인 Architecture 는 아래와 같습니다.</p><p><img src="/images/mrc/image-20211110070044172.png" alt="Custom Heads"></p><div class="note note-warning">            <p>RNN Family 를 사용한 head 의 모델에서는 정답이 포함된 Sequence 에 조금 더 집중할 수 있기를 기대했고, CNN 은 정답과 조금 더 유사한 feature 들, Transformer Encoder 에서는 사전학습된 Backbone 의 representation 보다 주어진 학습 데이터에 집중하여 보다 성능을 개선할 수 있기를 기대했습니다.</p>          </div><p><img src="/images/mrc/image-20211110071600955.png" alt="W&amp;B line chart custom heads"></p><p>막상 실험을 진행하니, 현재 backbone 도 과적합이 쉽게 일어나는 sensitive 한 상황이었기 때문에 head 에서 복잡도를 키우게 되니 validation score 는 backbone 보다 하락하였습니다. 특히 Conv1d layer 에서는 out_feature 가 작을수록, RNN Family 에서는 hidden_size 가 클수록 좋은 성능을 보였으며 전체적으로 custom layer 의 개수를 늘릴 수록 학습이 제대로 진행되지 않았습니다.</p><p>Custom head 를 부착한 모델들로 inference 를 진행한 후 나온 nbest_predictions 를 정성적으로 살펴보았을 때 각각의 head 들의 특징을 대략적으로나마 알 수 있었는데</p><p><img src="/images/mrc/image-20211110091444688.png" alt="Qualitative evaluation"></p><p>위의 예시에서 Conv1d 는 정답인 <code>사락사라</code> 와 비슷한 의미를 가진 ‘행성’, ‘지구’, ‘베텔게우스’ 등의 단어에 집중하는 모습을 보였고, Bi-LSTM과 Bi-GRU 는 정답인 <code>사락사라</code> 가 포함되어 있는 문장인 <code>40억년전 지구에서, 사락사라의 사람들이 자신들만의 낙원을 만들기 위해 지구에 착륙하였다.</code> 라는 시퀀스에 집중하였으며, Transformer Encoder 의 경우 Conv1d 와 RNN Family 의 특성을 모두 가지고 있는 듯한 경향을 보였습니다.</p><div class="note note-success">            <p>다만 각각의 모델들은 backbone 보다 public leaderboard score 가 비슷하거나 조금 더 낮은 점수를 기록하였습니다. 하지만 점수가 비슷함에도 예측한 정답들이 매우 달랐기 때문에 Ensemble 의 좋은 재료로 활용할 수 있을 것으로 예상할 수 있었습니다.</p>          </div><h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><p>다음으로 시도한 것은 Data Augmentation 입니다. <span class="linear_highlight">학습 데이터가 너무 적었기 때문에 더욱 robust 한 모델을 만들기 위해서는 여러 방면으로 데이터를 증강한 뒤 다양한 데이터셋을 만들 필요가 있다고 생각했습니다.</span> 크게 AEDA, Shuffling, Question Generation 세 가지 방법으로 Augmentation 을 진행하였으며, 해당 방법들로 약 10배 이상의 데이터를 만들 수 있었습니다.</p><h3 id="AEDA"><a href="#AEDA" class="headerlink" title="AEDA"></a>AEDA</h3><p><img src="/images/mrc/image-20211110162348848.png" alt="AEDA punctuation"></p><div class="note note-info">            <p><a href="https://arxiv.org/pdf/2108.13230.pdf">AEDA(An Easier Data Augmentation Technique for Text Classification)</a> 는 Original Text 의 random 한 위치에 punctuation 을 삽입하는 Augmentation 방법입니다. </p>          </div><p>해당 논문에서는 <code>Text Classification</code> Task 에서 큰 성능 향상을 보았는데, 각각의 Token 사이에 노이즈를 추가함으로써 모델의 과적합을 방지하고 서로 붙어있는 Token 간의 의존성을 낮추어 일반화 성능이 좋은 모델을 만들어 낼 수 있지 않았을까 라는 생각을 하게 되었습니다. </p><p>이 생각을 바탕으로 학습데이터에 존재하는 질문들에다가 AEDA Augmentation 을 사용하면 짧은 Sequence 라고 하더라도 질문의 의도나 Representation 을 잘 파악할 수 있을 것이란 예상을 하게 되었습니다. 저희는 하나의 Question 에 대하여 1개, 2개, 4개, 8개의 augmentation sentence 를 생성하였습니다. 질문의 마지막에는 항상 <code>?</code> 가 포함되어 있었기 때문에 마지막 인덱스를 제외한 text 에 <code>[&quot;.&quot;, &quot;,&quot;, &quot;!&quot;, &quot;;&quot;, &quot;:&quot;]</code> 의 punctuation 들을 삽입했습니다. 빠른 작업속도를 위해 Mecab Tokenizer 를 사용하였고, 삽입하는 확률을 0.2 로 설정함으로써 punctuation 이 너무 자주 삽입되는 것을 방지했습니다.</p><p>하지만 <code>Question + [SEP] + Ground Truth Passage + &#39; &#39; + Negative Samples</code> 로 이루어지는 input 형태에서 Question 에만 변형을 주는 것은 좋은 성능으로 이어지지 않았습니다. 그 이유를 예상해보았을 때 Question 에 대한 학습에는 도움이 되었을 지 몰라도, 아래 두 가지를 떠올려볼 수 있었습니다.</p><div class="note note-warning">            <ol><li>AEDA 를 수행한 sentence 가 많을수록 SEP Token 을 기준으로 합치는 Passage 들이 매우 많았기 때문에 실제 정답을 찾아내는 수행 능력만큼은 큰 차이가 없었을 것이다.</li><li>오히려 겹치는 Passage 에 대해서 보다 쉽게 overfitting 이 발생했기 때문에 inference 에서 기존보다 예측 결과가 나빴을 것이다.</li></ol>          </div><p>따라서 AEDA 를 제외한 다른 Augmentation 방법에 집중하게 되었습니다.</p><h3 id="Sentence-Shuffle"><a href="#Sentence-Shuffle" class="headerlink" title="Sentence Shuffle"></a>Sentence Shuffle</h3><p>다음으로 시도한 Augmentation 은 Sentence 를 뒤섞는 방법입니다.</p><p><img src="/images/mrc/image-20211110200555562-6542356.png" alt="Origin Data"></p><p>위의 예시를 다시 보면, <code>유령은 어느 행성에서 지구로 왔는가?</code> 에 대한 정답은 <code>사락사라</code> 입니다. 이 때 정답이 포함된 문장은 Passage 의 가장 첫 부분에 나타나고 있습니다. 또한 이 질문의 경우 <span class="linear_highlight">앞, 뒷문장 또는 문서의 맥락과 상관 없이 정답이 포함된 문장만 보더라도 정답을 예측할 수 있습니다.</span> 이렇게 여러 문장과의 관계를 살피지 않고 한 문장만 보고 정답을 맞출 수 있는 쉬운 질문의 경우 다른 문장은 학습에 오히려 <code>방해요소로 작용할 것</code>이란 생각을 하게 되었습니다. 그 생각을 바탕으로 문장들을 뒤섞는 아이디어를 떠올리게 되었고 <a href="https://www.google.co.kr/amp/s/neptune.ai/blog/data-augmentation-nlp/amp">Data-Augmentation-NLP</a> 해당 블로그를 참조하여 성능 향상 가능성을 보게 되었습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_dataset))):<br>    split_sentences = kss.split_sentences(train_dataset[i][<span class="hljs-string">&#x27;context&#x27;</span>])<br>    split_sentences_table = &#123;s: i <span class="hljs-keyword">for</span> i, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(split_sentences)&#125;<br>    answer_start = train_dataset[i][<span class="hljs-string">&#x27;answers&#x27;</span>][<span class="hljs-string">&#x27;answer_start&#x27;</span>][<span class="hljs-number">0</span>]<br><br>    ...<br><br>    random.shuffle(split_sentences)<br>    shuffle_index = [split_sentences_table[sentence] <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> split_sentences]<br>    shuffle_answer_sentence_index = shuffle_index.index(answer_sentence_index)<br></code></pre></td></tr></table></figure><p>kss 라이브러리의 <code>split_sentences</code> 함수를 사용하여 passage 를 문장단위로 분리한 이후 random 으로 shuffle 을 진행하였습니다. 이 때 정답의 시작 인덱스가 변경되기 때문에, 분리한 문장 배열에서 원래 정답이 속해있던 문장의 index 를 hash_table 에 저장한 뒤 변경된 문장의 위치에서 올바른 정답을 찾아갈 수 있는 처리를 해주었습니다.</p><p><img src="/images/mrc/image-20211110155315446.png" alt="Sentence Shuffle"></p><p><img src="/images/mrc/image-20211110201240878.png" alt="Sentence Shuffled Data"></p><p>결과는 위의 사진과 같습니다. 문장의 위치를 섞어줌으로써 Backbone 에게 어려운 Sample 을 더 많이 보여주어 성능 향상을 기대해볼 수 있었습니다.</p><h3 id="Passage-Shuffle"><a href="#Passage-Shuffle" class="headerlink" title="Passage Shuffle"></a>Passage Shuffle</h3><p>기존에는 Ground Truth 에다가 Negative Sample 4개를 이어붙여서 학습을 진행했었는데 sentence shuffle 을 진행하면서 이어붙이는 passage 들도 뒤섞는 것에 대한 아이디어를 떠올리게 되었습니다. <span class="linear_highlight">왜냐하면 Ground Truth 가 항상 맨 앞에 등장하기 때문에 Reader 모델이 Passage 들의 앞부분에만 집중할 수 있다고 생각했기 때문입니다. 또한 inference phase 에서 Retrieval 의 score 대로 passage 를 이어붙이게 되는데, 이 때 정답을 찾을 수 있는 passage 가 맨 앞에 오지 않는다면, Reader 의 성능이 급격히 떨어질 수도 있을 것이란 생각을 하게 되었습니다.</span></p><p><img src="/images/mrc/image-20211110155322213.png" alt="Passage Shuffle"></p><p>이러한 아이디어에 착안하여 위와 같이 Ground Truth 와 Negative Sample 의 순서를 random 하게 섞어줌으로써 Reader 의 성능 향상을 기대해볼 수 있었습니다.</p><h3 id="Sentence-Shuffle-Passage-Shuffle"><a href="#Sentence-Shuffle-Passage-Shuffle" class="headerlink" title="Sentence Shuffle + Passage Shuffle"></a>Sentence Shuffle + Passage Shuffle</h3><p><img src="/images/mrc/image-20211110155406916.png" alt="Sentence Shuffle + Passage Shuffle"></p><p>또한 앞의 두 아이디어와 비슷한 기대를 하며 Passage 들을 섞고 난 뒤 각각의 Passage 의 문장들까지 섞어놓은 학습데이터까지 구축하게 되었습니다. 이렇게 위의 세 가지 방식을 사용하여 처음보다 3배의 데이터를 추가로 확보할 수 있었습니다.</p><h3 id="Question-Generation"><a href="#Question-Generation" class="headerlink" title="Question Generation"></a>Question Generation</h3><p>다음으로 시도한 Augmentation 은 Question Generation 입니다. 외부데이터셋 사용이 제한된 상태에서 사용할 수 있는 Augmentation 중 가장 기대를 많이했던 방법입니다. <a href="https://arxiv.org/abs/1812.06705">Conditional BERT Contextual Augmentation</a> 논문을 보고 생성모델을 사용한 augmentation 기법에 아이디어를 얻게 되었고, 아래의 문서와 레포지토리를 참고하여 간단하게 Question Generation을 수행할 수 있었습니다.</p><ul><li><a href="https://kakaobrain.github.io/pororo/seq2seq/qg.html">Pororo Question Generation</a></li><li><a href="https://kakaobrain.github.io/pororo/tagging/ner.html">Pororo Named Entity Recognition</a></li><li><a href="https://github.com/codertimo/KorQuAD-Question-Generation/tree/main/korquad_qg">KorQuAD-Question-Generation</a></li></ul><p>질문을 생성하기 위해서는 Target 이 되는 Answer 가 필요했기 때문에 NER 을 활용하여 Entity 인 것들을 Target 으로하는 질문 생성과, Wiki 문서의 제목이 문서 본문에 포함되면, 제목을 Target 으로 하는 질문을 생성하는 두 가지 접근법을 생각해 냈습니다. 그리고 사전학습된 생성모델인 SKT-AI 의 KoGPT-2 와 Kakaobrain 의 KoBART 를 사용하여 새로운 질문을 생성해냈고, 학습 데이터와 유사한 형태의 데이터를 만들어낼 수 있었습니다.</p><ol><li><p>NER + Question Generation</p><p><img src="/images/mrc/image-20211110155258759.png" alt="NER &amp; Question Generation"></p><p>Pororo 의 NER 은 Entity 가 없으면 <code>&#39;O&#39;</code> 를 반환합니다. 따라서 ‘O’ 가 부착되지 않은 Entity 를 정답으로 하는 질문을 생성해냈고, 약 14만 건의 데이터를 확보할 수 있었습니다. <span class="linear_highlight">다만 질문의 퀄리티가 상당히 좋지 않았기 때문에 filtering 을 거칠 필요가 있었습니다.</span> 정성적으로 살펴본 결과 entity 의 길이가 짧은 것들 대부분에 의존명사가 포함되어 있었고, 질문이 이러한 entity 를 제대로 표현할 수 없겠다는 생각이 들었습니다.<br>학습 데이터셋의 정답 길이의 평균이 6 이상이었기 때문에 길이가 6보다 적은 entity 를 제거하였고, 약 2만 3천 개 정도의 질문이 남게 되었습니다. 또한 다른 target 에 대하여 동일한 질문이 곳곳에 포함되어 있었고, target 과 전혀 상관없는 질문이 생성되기도 하였습니다. 그래서 저희는 <a href="https://kakaobrain.github.io/pororo/text_cls/sts.html">Pororo Semantic Textual Similarity</a> 를 사용하여 질문과 답변의 유사도가 0.5 이상인 것들만 추려서 최종적으로 약 2천 개의 데이터를 확보하게 되었습니다.</p></li><li><p>Wiki Title question generation</p></li></ol><p>  <img src="/images/mrc/image-20211110214338773.png" alt="Wiki Title Question Generation"></p><p>  Named Entity 를 Target 으로 하여 생성해낸 질문들을 필터링하니 데이터가 별로 남지 않아서 다른 방법을 고안하게 되었습니다. Wiki의 title 을 Target 으로 질문을 생성해내면 <span class="linear_highlight">문서를 대표하는 정답으로 질좋은 질문들을 생성해낼 수 있다</span>고 판단했기 때문입니다. 이 때 주어진 Wiki 에서 중복을 제거하고, Title 이 겹치는 문서들도 제거한 31,726 개의 Wiki 문서에서 Title 이 문서 내에 포함되어 있는 문서의 개수는 약 17,000 개였습니다. 저희는 KoGPT-2 를 사용하여 17,000 여개의 정답을 Target 으로 하는 질문들을 생성했습니다.</p><p>그렇게 총 약 3만 개의 질의응답 데이터셋을 구축할 수 있었고, 이렇게 구축한 데이터셋들을 서로 다른 조합으로 하여 학습을 이어갔습니다. 하지만, Augmentation 한 데이터셋으로 학습하여도 기존의 리더보드 점수를 따라잡을 수 없었습니다.</p><div class="note note-danger">            <p>Pororo 는 기본적으로 batchify 를 지원하지 않기 때문에 빠른 속도로 augmentation 을 진행할 수 없었습니다. 단순히 컴퓨터를 켜놓고 대기할 수 밖에 없었는데, 다음에는 batchify 방식을 구현하여 보다 빠른 작업을 진행해야 겠다는 생각을 하게 되었습니다.</p>          </div><h2 id="Curriculum-learning"><a href="#Curriculum-learning" class="headerlink" title="Curriculum learning"></a>Curriculum learning</h2><p>Augmentation 을 진행했는데 성능 개선을 못 본 것에 대하여 아래와 같은 이유들을 생각해볼 수 있었습니다.</p><div class="note note-warning">            <ol><li>생성해낸 질문의 퀄리티가 생각보다 좋지 않아서 오히려 모델의 학습을 방해했다.</li><li>증강한 질의응답의 난이도가 너무 어렵다.</li></ol>          </div><p>정성적인 평가를 진행했을 때 생성해낸 질문들 중에 이상하거나 중복된 질문들을 찾지 못했습니다. 또한 Sentence 와 Passage 를 모두 shuffle 한 학습데이터의 경우 Validation EM Score 가 40점을 넘기지 못했습니다. 때문에 두 번째 이유에 집중하여 모델의 학습 방식을 개선해보기로 하였습니다. 그 중에서도 <a href="https://hwiyong.tistory.com/327">Curriculum Learning - 대학원생이 쉽게 설명해보기</a> 블로그를 참고하게 되었고 쉬운 샘플부터 점점 난이도를 높여가며 학습시키는 것이 일반화와 더 빠른 수렴에 도움이 된다는 것을 알게 되었습니다.</p><p>따라서 Level 1, 2, 3 세 단계를 두고 Augmented Dataset, Passage Shuffled Dataset, Sentence and Passage Shuffled Dataset 난이도별로 학습을 시켜보게 되었습니다.</p><p><img src="/images/mrc/image-20211110155032798.png" alt="Curriculum Learning Workflow"></p><p><img src="/images/mrc/image-20211110155211405.png" alt="W&amp;B line chart Curriculum Learning"></p><p><img src="/images/mrc/image-20211110224114767.png" alt="Curriculum Learning Benchmark"></p><div class="note note-success">            <p>기존 가장 높은 점수를 기록했던 모델보다 약 3점의 스코어 향상이 있었고, Curriculum Learning 을 통해 얻은 결과들을 Ensemble 했을 때는 약 5점의 스코어 향상을 기록할 수 있었습니다.</p>          </div><h2 id="Combine-Models"><a href="#Combine-Models" class="headerlink" title="Combine Models"></a>Combine Models</h2><p>이 외에도 여러 모델을 결합한 형태로 실험을 이어갔습니다. <a href="https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4">Combining numerical and text features in deep neural networks</a> 해당 글에서 아이디어를 얻었으며, Single model 보다는 여러 모델의 hidden state 를 concat 하여 end-to-end 로 학습시킬 때 더 좋은 퍼포먼스를 기대할 수 있지 않을까라는 생각 때문이었습니다.</p><p>BERT 계열의 비슷한 모델을 Combine 하는 것은 단순 Layer 를 두 배로 늘리는 것 정도의 효과밖에는 기대하지 않았기 때문에, 최대한 Model 의 Architecture 가 다른 모델들을 가지고 실험을 해보고자 했습니다.</p><ul><li>Roberta-Electra</li><li>Roberta-Bigbird</li></ul><p>최종적으로는 위의 두 가지 조합을 사용하게 되었습니다. 위의 조합을 선택한 이유는 아래와 같습니다.</p><div class="note note-warning">            <ol><li>Electra 의 경우 Generator 는 BERT 의 MLM 구조와 동일하지만, Discriminator 에 의해 RoBERTa 와는 다른 Representation 을 기대할 수 있을 것이다.</li><li>Bigbird 의 경우 Block sparse attention 을 이용하여 Roberta 보다 긴 sequence 에 강력할 것이다.</li></ol>          </div><p>하지만, <span class="linear_highlight">단순히 hidden state 를 concat 한 뒤 정답의 position 을 추출하는 방식만으로는 backbone 의 성능을 따라갈 수 없었습니다.</span> 서로의 Representation 들이 다르기 때문에 이미 어느정도의 Features 를 학습한 사전학습 모델을 결합하는 것으로는 좋은 효과를 도출해낼 수 없었던 것으로 예상됩니다. Scratch 부터 학습을 진행했으면 더 좋은 성능을 기대해볼 수 있었을 것으로 기대합니다.</p><h2 id="Pytorch-Versionup"><a href="#Pytorch-Versionup" class="headerlink" title="Pytorch Versionup"></a>Pytorch Versionup</h2><p>다른 팀원이 가장 좋은 성능의 모델을 재현하던 중 다른 pytorch 버전으로 학습시켰을 때 결과가 다르다는 소식을 전해들었고, Seed 를 고정했음에도 다른 결과가 나온 것이 이상하여 원인을 찾던 중 아래와 같은 이슈를 발견할 수 있었습니다.</p><div class="note note-warning">            <p>RNG(Random Number Generator) 혹은 PyTorch 버전에 따른 Random Number 에 따라서 결과 재현이 안될 수도 있고, 모델의 수렴속도가 달라질 수 있다. </p>          </div><p>또한 PyTorch 혹은 Huggingface transformers 와 같은 라이브러리의 버전에 따라서도 float dividing 이나 연산의 처리가 달라질 수 있기 때문에 버저닝에 따른 결과가 다를 수 있다는 것을 알게 되었습니다. 저희는 Pytorch 를 가장 최신버전인 1.10.0 으로 업그레이드하여 조금의 성능 향상을 이끌 수 있었습니다. </p><p><img src="/images/mrc/image-20211111124458612.png" alt="Pytorch Version Benchmark"></p><div class="note note-success">            <p>결과적으로 위의 모든 시도들을 종합했을 때 Public Leaderboard 에서 단일모델 69.580 이라는 점수를 달성할 수 있었습니다.</p>          </div><h2 id="KFold-Cross-Validation"><a href="#KFold-Cross-Validation" class="headerlink" title="KFold Cross Validation"></a>KFold Cross Validation</h2><p>저희는 Private Leaderboard 가 공개되었을 때 shake up 이 발생하는 것에 경각심을 가지고 있었습니다. 따라서 모델의 일반화 성능을 향상시키기 위해 KFold 교차검증 방식을 도입하기로 했습니다.</p><p><code>sklearn</code> 의 <code>KFold</code> 를 이용하여 dataset 을 분리하였고, 정답이 되는 토큰의 위치를 찾아내는 Extractive Reader 방식이었기 때문에 start 와 end logit 들의 평균을 구하고 그 평균값으로 후처리를 진행하도록 하였습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">kfold = KFold(n_splits=<span class="hljs-number">5</span>)<br><br>split_datasets = []<br><span class="hljs-keyword">for</span> i, (train_idx, valid_idx) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(kfold.split(dataset)):<br>    split_dataset = DatasetDict()<br>    split_dataset[<span class="hljs-string">&quot;train&quot;</span>] = Dataset.from_dict(concat_dataset[train_idx])<br>    split_dataset[<span class="hljs-string">&quot;validation&quot;</span>] = Dataset.from_dict(concat_dataset[valid_idx])<br>    split_datasets.append(split_dataset)<br></code></pre></td></tr></table></figure><p><img src="/images/mrc/image-20211111134617529.png" alt="KFold Cross Validation"></p><p>하지만 Public Leaderboard 기준으로 KFold 를 수행한 결과가 단일모델 대비 2점 가량 낮은 67.500 를 기록하였습니다. 실제 Private Leaderboard 결과에서도 단일모델보다 성능이 떨어졌습니다. 그 이유로는</p><div class="note note-warning">            <ol><li>Random 하게 fold 를 나눈 것에서 모델의 학습 성능이 저하되었을 수 있다.</li><li>Fold 별 모델의 Hyperparameter 를 다르게 설정하였으면 더 좋은 결과가 나올 수 있었을 것이다.</li></ol>          </div><p>위의 두 가지를 예상해볼 수 있었고, <span class="linear_highlight">각 fold 별로 다시 hyperparameter 를 tuning 하고 재학습시키는 것에는 너무 오랜 시간이 걸리기 때문에 KFold 의 성능을 더 개선하기 보다는, 이미 예측한 결과들을 Ensemble 에 사용하기로 하였습니다.</span></p><h1 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h1><p>저희는 처음부터 Ensemble 을 고려하였기 때문에 위와 같은 다양한 실험들을 하면서 inference 의 결과들을 꾸준히 모아두기로 하였었고, 결과적으로 약 150개의 inference 결과를 모을 수 있었습니다. inference 를 통해 나오는 파일은 아래와 같습니다.</p><ol><li>질문에 대해 예측한 정답들 중 확률값이 높은 20개의 정답, start/ end logit, 그리고 확률값을 저장하는 n_best_predictions.json 파일 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br>    <span class="hljs-attr">&quot;mrc-1-000653&quot;</span>: [<br>        &#123;<br>            <span class="hljs-attr">&quot;start_logit&quot;</span>: <span class="hljs-number">4.3632707595825195</span>,<br>            <span class="hljs-attr">&quot;end_logit&quot;</span>: <span class="hljs-number">3.9985079765319824</span>,<br>            <span class="hljs-attr">&quot;text&quot;</span>: <span class="hljs-string">&quot;사락사라&quot;</span>,<br>            <span class="hljs-attr">&quot;probability&quot;</span>: <span class="hljs-number">0.5359510779380798</span><br>        &#125;,<br>        &#123;<br>            <span class="hljs-attr">&quot;start_logit&quot;</span>: <span class="hljs-number">-5.266266345977783</span>,<br>            <span class="hljs-attr">&quot;end_logit&quot;</span>: <span class="hljs-number">-3.9985079765319824</span>,<br>            <span class="hljs-attr">&quot;text&quot;</span>: <span class="hljs-string">&quot;지구에서, 사락사라&quot;</span>,<br>            <span class="hljs-attr">&quot;probability&quot;</span>: <span class="hljs-number">0.21724958717823029</span><br>        &#125;,<br>                        .<br>                        .<br>                        .<br></code></pre></td></tr></table></figure></li><li>각 질문 별로 가장 높은 확률 값을 가지는 정답만 저장하는 predictions.json 파일 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs json">&#123;<br>    <span class="hljs-attr">&quot;mrc-1-000653&quot;</span>: <span class="hljs-string">&quot;40억년전 지구&quot;</span>,<br>    <span class="hljs-attr">&quot;mrc-1-001113&quot;</span>: <span class="hljs-string">&quot;냉전 종식&quot;</span>,<br>    <span class="hljs-attr">&quot;mrc-0-002191&quot;</span>: <span class="hljs-string">&quot;빌헬름 미클라스&quot;</span>,<br>                        .<br>                        .<br>                        .<br>&#125;<br></code></pre></td></tr></table></figure></li></ol><p>저희는 약 150개의 pair 중에서 public leaderboard 기준 EM score 가 60 이상인 96개의 prediction, nbest_prediction 를 활용하게 되었습니다.</p><h2 id="Soft-Voting"><a href="#Soft-Voting" class="headerlink" title="Soft Voting"></a>Soft Voting</h2><div class="note note-info">            <p>첫번째 방식은 모델들의 n_best_predictions에서 같은 정답들의 probability를 합하여 가장 probability가 가장 큰 정답을 선택하는 soft voting 방법이었습니다. </p>          </div><p>아래 그림과 같은 두 예측 결과를 사용한다고 할 때, 두 모델의 “사락사라” 라는 예측값의 확률을 더한 결과가 약 1.5987로 가장 높아 “사락사라”를 첫번째 질문에 대한 정답으로 선택하는 방식입니다.</p><p><img src="/images/mrc/softvote.png" alt="Soft Voting"></p><p>Soft Voting 방식으로 Public EM 62점 이상의 30개의 결과를 앙상블한 결과 약 3점 정도 상승한 <strong>72.080</strong> 의 EM Score 를 기록할 수 있었습니다.</p><p>성능을 더욱 높이기 위하여 모델의 결과를 분석해본 결과, <span class="linear_highlight">단일 모델 성능이 높아질수록 다른 모델들에 비해 정답의 probability 가 높은(확신을 하는) 경향이 있었습니다.</span> 이러한 모델들은, 정답을 결정하는데 오히려 확증편향이 생길 것으로 예상하였고 영향력을 감소시키기 위해 probability가 threshold 값 이상이면 clipping 하는 방법도 이용해보았지만 큰 효과를 볼 수 없었습니다.</p><p>Soft Voting 에 사용한 코드는 아래와 같습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">last_answer = &#123;&#125;<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(mrc_id)):<br>    answer_dict = &#123;&#125;  <br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> json_files:<br>            answer = file[mrc_id[i]][j][<span class="hljs-string">&#x27;text&#x27;</span>]<br>            prob = file[mrc_id[i]][j][<span class="hljs-string">&quot;probability&quot;</span>]<br><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> answer <span class="hljs-keyword">in</span> answer_dict:<br>                answer_dict[answer] = prob<br>            <span class="hljs-keyword">else</span>:<br>                answer_dict[answer] += prob<br>    temp = <span class="hljs-built_in">list</span>(answer_dict.items())<br>    temp.sort(key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>],reverse=<span class="hljs-literal">True</span>)<br>    last_answer[mrc_id[i]] = temp[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure><h2 id="Hard-Voting"><a href="#Hard-Voting" class="headerlink" title="Hard Voting"></a>Hard Voting</h2><div class="note note-info">            <p>두번째 방식은 모델들의 predictions.json 파일들을 합쳐, 가장 많이 등장한 정답을 최종 정답으로 선택하는 방법이었습니다. 하드 보팅의 경우 LB상 Public EM 성능이 soft voting에 비해 높지 않았습니다. </p>          </div><p>이를 개선하기 위하여, Voting 후 1위와 2위의 득표(점수)의 차이가 Threshold 이하 일 경우 각 후보 단어들의 비교를 진행하여, 다른 단어의 sub string인 경우 해당 단어와 substring 의 길이의 비율만큼 얻은 점수를 더해 주는 Rule base 알고리즘을 수행하였습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json">[(냉전 종식, <span class="hljs-number">20</span>점), (냉전, <span class="hljs-number">15</span>점)] <br></code></pre></td></tr></table></figure><p>Voting 이후 위와 같은 결과를 얻었다고 했을 때, “냉전”은 “냉전 종식”의 sub string이므로 “냉전 종식”의 20점의 일부분을 추가로 얻게 됩니다. 그 결과 “냉전”은 “냉전 종식” 보다 더 많은 점수를 가지게 되어 최종 정답은 “냉전”이 되도록 하였습니다.</p><p>단순 hard voting 을 적용하였을 때는 Public EM 기준 <code>70.000</code> 을 기록하였으나, 해당 방식을 사용하여 <code>71.250</code> 으로 향상시킬 수 있었습니다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python">result = <span class="hljs-built_in">dict</span>()<br>    <br><span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> mrc_id: <br>    polls = defaultdict(<span class="hljs-built_in">int</span>)<br>    <br>    <span class="hljs-keyword">for</span> json_file <span class="hljs-keyword">in</span> hard_vot_json_files:<br>        predict = json_file[key]<br>        polls[predict] += <span class="hljs-number">1</span><br><br>    sorted_polls = <span class="hljs-built_in">sorted</span>(polls.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> sorted_polls[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] - sorted_polls[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] &lt; <span class="hljs-number">5</span>:<br>        num_of_candidates = <span class="hljs-number">3</span><br>        regularize_probs_candidate_top_3 = sorted_polls[:num_of_candidates]<br>        <span class="hljs-keyword">for</span> n, candidate <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(regularize_probs_candidate_top_3):<br>            candidate_answer, candidate_score = candidate<br><br>            <span class="hljs-keyword">for</span> m, diff_candidate <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(regularize_probs_candidate_top_3):<br>                <span class="hljs-keyword">if</span> n != m:<br>                    diff_candidate_answer, diff_candidate_score = diff_candidate<br>                    <span class="hljs-keyword">if</span> candidate_answer <span class="hljs-keyword">in</span> diff_candidate_answer:<br>                        <span class="hljs-comment"># substring에 length ratio 만큼 score 몰아주기</span><br>                        candidate_score += (diff_candidate_score * (<span class="hljs-built_in">len</span>(candidate_answer) / (<span class="hljs-built_in">len</span>(diff_candidate_answer) - <span class="hljs-number">1</span>)))<br><br>            regularize_probs_candidate_top_3[n] = (candidate_answer, candidate_score)<br><br>        regularize_probs_candidate_top_3.sort(key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>],reverse=<span class="hljs-literal">True</span>)<br>        voted = regularize_probs_candidate_top_3[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">else</span>:<br>        voted = sorted_polls[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><br>    result[key] = voted<br></code></pre></td></tr></table></figure><h2 id="최종-제출"><a href="#최종-제출" class="headerlink" title="최종 제출"></a>최종 제출</h2><p>Leaderboard 의 score 차이가 크지 않은 soft voting 결과와 hard voting에 rule base 알고리즘을 수행한 결과에 후처리를 적용하여 최종 제출물로 선택하려고 했습니다. 후처리는 정답 중 ‘이’, ‘에’와 같은 조사로 끝나는 정답들에 대해 <span class="linear_highlight">konlpy의 POS tagger를 이용하여 정답에서 조사를 제거</span>해주었습니다.</p><p><img src="/images/mrc/final_result.png" alt="Private Leaderboard"></p><div class="note note-success">            <p>마지막에는 서버상 오류로 soft voting한 결과는 제출하지 못하였지만, hard voting 결과가 public leaderboard 점수보다 오히려 상승한 <strong>EM 71.670, F1 81.400</strong> 을 기록하며 최종 3위를 기록할 수 있었습니다.</p>          </div><p>나중에 확인해보니 soft voting 결과는 오히려 60점 대로 떨어졌습니다.</p><h1 id="시도했으나-잘-안된-것들"><a href="#시도했으나-잘-안된-것들" class="headerlink" title="시도했으나 잘 안된 것들"></a>시도했으나 잘 안된 것들</h1><h2 id="Generative-Reader"><a href="#Generative-Reader" class="headerlink" title="Generative Reader"></a>Generative Reader</h2><p>QA 과제를 해결하는 데엔 Query와 Passage를 바탕으로 적절한 <code>토큰을 생성</code>해 답을 만드는 <code>Generative Reader</code>를 활용할 수 있습니다. Baseline 성능 평가 당시 Generative Reader 를 사용한 Validation Score 가 Extractive 방식보다 낮았기 때문에 집중적으로 활용하지 못했습니다. 하지만 영어에서는 Extractive 보다는 Generative 방식이 성능이 좋다고 알려져있으며, 과제의 답안들이 Passage 내에 추출 가능하게 존재하고, Reader가 산출한 답안과 정답간의 EM을 통해 평가하는 특성상, <code>Extractive Reader</code>보다 좋은 성능을 기대하기 보단 앙상블 과정의 다양성 확보를 목표로 <code>Generative Reader</code>를 꾸준히 시도해보았습니다.</p><p>Model input 형식은 <code>[질문] : &#123;question&#125; [지문] : &#123;passage&#125;</code> 형태이며, max_sequence_length 를 초과하는 경우 정답을 포함한 passage 임을 알 수 있게 special token 을 활용하여 정답을 포함하는지, 포함하지 않는지 표기하였습니다. Prediction 단계에서는 Beam Search 를 사용하여 Output 을 생성해냈고, inference 를 위해 <code>Extractive Reader</code>가 답안이 있을 걸로 예상되는 위치들의 Logit 값을 통해 최종 Prediction을 선정했던 데에 착안해서, 모델이 답안을 생성하며 사용한 확률값을 기준으로 사용하여 후처리를 진행했습니다.</p><p>이 과정에서 input sequence length의 길이를 줄여 한번에 모델이 읽는 지문의 양을 줄이는 시도를 하게 되었고, 결과적으로 Validation Metric 기준으로 EM: 40, f1: 47 정도로 성능을 향상시킬 수 있었습니다. </p><div class="note note-danger">            <p>다만, 어느정도 긴 호흡으로 지문을 해석해야 정답을 도출할 수 있는 경우, sequence length가 길게 주어졌을 때 잘 맞추던 질문을 낮은 sequence length에서는 맞추지 못하는 사례들이 존재하였으며 <code>Extractive Reader</code> 에서 많은 성능을 이끈 Negative Sampling 을 도입하니 정답이 없는 input이 훨씬 많아지고 성능이 더욱 감소했습니다.</p>          </div><p>이러한 이유들로 <code>Generative Reader</code> 는 최종적으로 활용되지 못했습니다. 추가적으로 학습시킨 Generative Model 을 HuggingFace의 <code>EncoderDecoderModel</code> 구조의 Decoder 로 불러와 Extractive 와 합치는 모델링 작업을 시도했으나 구현상의 미숙함들과 실수로 인해 실험을 더욱 이어가지 못했습니다.</p><h2 id="Retro-Reader"><a href="#Retro-Reader" class="headerlink" title="Retro Reader"></a>Retro Reader</h2><p>Retro Reader 는 <a href="https://paperswithcode.com/sota/question-answering-on-squad20">SQuAD2.0 Benchmark</a> 에서 EM 기준 7위에 Rank 되어 있는 모델입니다. </p><div class="note note-info">            <p>Retro Reader 의 논문 <a href="https://arxiv.org/abs/2001.09694v4">Retrospective Reader for Machine Reading Comprehension</a> 을 살펴보면, Answerable 을 판별하고 정답을 예측하는 Sketch Reader 와 정답을 예측하는 Intensive Reader 로 구성되어 있으며, 이들의 예측값을 바탕으로 Decoder 에서 최종 정답을 예측하는 방식입니다.</p>          </div><p><img src="/images/mrc/retro_reader.png" alt="Retro Reader Architecture"></p><p>해당 방식의 Github <a href="https://github.com/cooelf/AwesomeMRC">AwesomeMRC</a> 을 참고하여 구현을 시도해보았지만 <span class="linear_highlight">Huggingface 의 transformers 버전이 매우 낮아서 Porting 하여 사용하기에는 레거시가 많이 존재</span>하였습니다. 또한 <span class="linear_highlight">No Answer 에 대한 데이터셋을 따로 구축했어야 했는데, 이 때 Dataset 의 Scale 을 어떻게 조정해야할지 정하지 못했습니다.</span> 단순하게 생각해보면 Train dataset 의 전체 질문들에 대해 Ground Truth 를 제외한 모든 Wiki 의 Passage 를 붙일 수 있었고, 양을 조절하여 Random 하게 No Answer Passage 를 뽑아올 수 있었으나 정확한 기준을 세우지 못하였으며 또한 Question Generation, AEDA 등으로 증강한 데이터셋을 활용하였을 때 학습 시간만 오래 들고 별다른 성능 향상을 보지 못했기 때문에 생각만 하고 시도하지 않았습니다.</p><p>다시 생각해보면 충분히 시도할만한 가치가 충분히 있었으나, 아쉬움이 남습니다.</p><h2 id="Focussing-on-surrounding-sentences-of-the-answer"><a href="#Focussing-on-surrounding-sentences-of-the-answer" class="headerlink" title="Focussing on surrounding sentences of the answer"></a>Focussing on surrounding sentences of the answer</h2><p>Data Augmentation 과 Custom Head 를 부착한 방식으로 이미 큰 성능 향상을 보았지만, 모델 구조를 더욱 개선할 필요를 느끼게 되었습니다. 현재 Inference 시 Retrieval 로 20 개의 질문과 유사한 문서를 찾아오도록 하는데, 이 문서들의 순서를 바꿀 때마다, 그리고 20개보다 더 많은 문서를 찾아오도록 할 때마다 모델이 다른 정답들을 예측했기 때문입니다. 따라서 Reader 가 더욱 일관된 답변을 예측할 수 있도록 하기 위해 고민하던 중 아래의 문서와 논문에서 아이디어를 얻을 수 있었습니다.</p><ul><li><a href="https://www.sbert.net/">SBERT</a></li><li><a href="https://arxiv.org/abs/1611.01603">Bidirectional Attention Flow for Machine Comprehension</a></li></ul><div class="note note-primary">            <p>Reader 가 정답을 예측할 때 정답인 것으로 예측되는 문장과, 주변 sentence 에 집중할 수 있도록 하면 Retrieval 이 아무리 많은 문서를 가져오더라도 오답률이 줄어들 것이다.</p>          </div><p>이 아이디어를 구현하기 위해 두 가지 방법을 떠올리게 되었습니다.</p><div class="note note-warning">            <ol><li>Sentence BERT 를 사용하여 Question 과 Passage 에 대한 Embedding 을 RoBERTa Embedding 에 더해주자.</li><li>Question 과 Passage 에 대한 Embedding 으로 Cosine Similarity Score 를 구하고, 해당 Score 를 weight 로 하여 RoBERTa 의 last hidden state 에 곱해주자.</li></ol>          </div><p><strong>Similarity Embedding</strong></p><p><img src="/images/mrc/image-20211110234505636.png"></p><p>Huggingface 의 <a href="https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaForQuestionAnswering">RobertaEmbeddings</a> 를 살펴보면 BERT 의 구조와 동일하게 <code>Token Embeddings, Segment Embeddings, Position Embeddings</code> 를 모두 더한 뒤 LayerNormalization 과 Dropout 을 취한 Embedding 을 반환합니다. <span class="linear_highlight">이 때 Sentence Transformer 로 계산한 Query 와 Passage 의 Embedding 을 더해주면, 단순히 유사도를 포함한 Embedding 을 고려할 수 있을 것이란 생각을 했습니다.</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RobertaEmbeddings</span>(<span class="hljs-params">nn.Module</span>):</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, input_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span></span>):</span><br>      <br>      ...<br><br>      embeddings = inputs_embeds + token_type_embeddings<br><br>      <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>          position_embeddings = self.position_embeddings(position_ids)<br>          embeddings += position_embeddings<br><br>      <span class="hljs-comment"># add similarity embedding</span><br>      embeddings += similarity_embeddings<br><br>      embeddings = self.LayerNorm(embeddings)<br>      embeddings = self.dropout(embeddings)<br>      <span class="hljs-keyword">return</span> embeddings<br><br></code></pre></td></tr></table></figure><p><span class="linear_highlight">하지만 embedding layer 에 다른 모델의 embedding 값을 더해주니 이미 사전학습된 backbone 의 embedding 값들이 망가지며 제대로 학습이 이루어지지 않았습니다. 최대한 similarity embedding 을 backbone 의 embedding 과 유사하게 맞춰주기 위하여 embedding dimension 에 따라 표준편차 등을 이용하여 scaling 도 진행해봤으나 역시나였습니다.</span> Scratch 부터 재학습 시킬 수 있는 시간과 자원이 없었기 때문에, 다른 방법을 몰색했습니다.</p><p><strong>Similarity Weight Layer</strong></p><p>Embedding Layer 를 수정하여 재학습시키는 것 대신에 Sentence BERT 를 사용하여 얻은 Question, Passage Embedding 으로 코사인 유사도를 구한 뒤 해당 유사도들을 backbone 의 last hidden state 에 weight 을 주는 방식을 시도해 보았습니다. 해당 아이디어는 <a href="https://arxiv.org/abs/2004.08795">Extractive Summarization as Text Matching</a> 를 통해 얻게 되었습니다. </p><div class="note note-info">            <p>Summarization Task 의 <code>Content Selector</code> 방식에서는 중요 문장들의 후보를 추린 이후에 해당 문장들만을 통해 요약문을 생성하게 됩니다. </p>          </div><p>처음에는 이처럼 정답을 찾을 수 있을 것 같은 문장들의 후보를 추린 이후에 전체 passage 가 아닌 그 문장들에 대해서만 정답을 찾게 하는 방식을 시도했었는데, Summarization 에 대한 이해와 기본기가 부족했으며 시간적 여유도 없었기에 포기하게 되었습니다. </p><p>따라서 <span class="linear_highlight">정답을 찾을 수 있는 문장들의 후보가 아닌, 유사도를 기반으로 질문과 유사한 문장들에 대해서 가중치를 주는 방식</span>을 생각해내게 되었습니다. 아이디어의 검증은 저희팀의 든든한 멘토님이신 이유경 멘토님께서 도와주셨으며 hidden state 에 대해서 가중치를 주는 방식만 보았을 때는 틀린 접근이 아니라는 말씀을 주셨기 때문에 걱정없이 시도해볼 수 있었습니다.</p><p><img src="/images/mrc/image-20211111133432522.png"></p><p>과정을 간략하게 풀어쓰면 아래와 같습니다.</p><p>학습 데이터의 평균적인 질문 길이는 60 정도였습니다. 전처리 여부나 스페셜 토큰을 고려하여 Backbone 으로부터 나온 Sequence Output 에 64 채널을 기준으로 Question, Passage hidden state 로 분리한 뒤 SEP 토큰이 있는지 조사합니다. SEP 토큰이 있었다면 질문이 포함되어 있는 것이므로, 이 때는 Decoding 을 진행한 후 Sentence BERT의 Bi-Encoder 를 사용하여 Question 과 Passage 의 임베딩을 생성합니다. 이 임베딩을 통해 코사인 유사도를 계산한 뒤 정규화와 scaling 을 진행한 후 이것을 weight 로 하여 dense layer 를 통과한 hidden state 에 곱해주고, 그것을 최종 output logit 으로 반환합니다.</p><div class="note note-danger">            <p>하지만 돌아가게끔 만드는 것 만으로도 어려움이 많았고, 구현 과정에서 실수가 있었는지 학습이 제대로 이루어지지 않았습니다. </p>          </div><p>어떻게든 돌아가는 코드를 작성하고 학습이 이루어지지 않는 것을 보았을 때 굉장히 아쉬움이 많이 남았지만, 대회 막바지였기 때문에 해당 방법은 포기하게 되었습니다.</p><h2 id="APE-Adaptive-Passage-Encoding"><a href="#APE-Adaptive-Passage-Encoding" class="headerlink" title="APE (Adaptive Passage Encoding)"></a>APE (Adaptive Passage Encoding)</h2><p>아이디어를 얻은 논문은 아래와 같습니다.</p><div class="note note-warning">            <ul><li><a href="https://arxiv.org/abs/2107.02102">Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints</a></li><li>Fusion-in-Decoder (<a href="https://arxiv.org/abs/2007.01282">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a>)</li></ul>          </div><p><img src="/images/mrc/ape.png"></p><div class="note note-info">            <p>APE 방식은 질문에 대해 Retrieval 이 수행한 n개의 Passage 에 대하여 Pruning 방식으로 적절한 Passage 로 여겨지는 Candidates 를 추리고, 해당 Candidates 에 Attention 을 수행하는 방법입니다. </p>          </div><p>위에서 시도한 <code>Focussing on surrounding sentences of the answer</code> 방법보다 훨씬 우아한 방식이라고 생각했기 때문에 해당 논문의 <a href="https://github.com/uclnlp/APE">APE Github</a> 을 참조하여 시도해보려고 했습니다. 하지만 <span class="linear_highlight">Facebook 의 오픈소스인 <a href="https://github.com/facebookresearch/FiD">FiD</a> 에 의존성이 걸려있고, Generative 모델인 T5를 사용하며 Huggingface 의 transformers 버전도 <code>3.0.2</code> 로 매우 낮았기 때문에 한국어로 사용가능한 MT5 나, Extractive 방식으로 porting 하기엔 상당히 어려움이 있었습니다.</span> 또한 이 논문을 알게 된 시기가 대회 종료 2일 전으로, Ensemble 을 수행하지 못한 상황에서 시도하기에는 시간적인 여유도 없었습니다. 추후에 영어로 ODQA 에 도전하게 된다면, APE 를 0순위로 시도해보고 싶다는 생각을 하게 되었습니다.</p><h1 id="마지막으로"><a href="#마지막으로" class="headerlink" title="마지막으로"></a>마지막으로</h1><p>전반적으로 KLUE-RE 대회때보다 팀원들 모두가 성장했음을 조금이라도 느낄 수 있었고, 특히 실험을 진행하면서 문제점을 파악하고 문제를 정의하며 해결하는 과정에서 많은 즐거움을 얻을 수 있었습니다.</p><p>저희의 이야기가 조금이라도 도움이 되었길 바라면서 마치겠습니다.</p><p>긴 글 읽어주셔서 감사합니다.</p><br>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>KLUE-RE NLP 왕초보 후기(문하겸)</title>
    <link href="/KLUE-RE-NLP-%EC%99%95%EC%B4%88%EB%B3%B4-%ED%9B%84%EA%B8%B0-%EB%AC%B8%ED%95%98%EA%B2%B8/"/>
    <url>/KLUE-RE-NLP-%EC%99%95%EC%B4%88%EB%B3%B4-%ED%9B%84%EA%B8%B0-%EB%AC%B8%ED%95%98%EA%B2%B8/</url>
    
    <content type="html"><![CDATA[<h1 id="P-stage-KLUE-RE-NLP-왕초보-후기-하겸"><a href="#P-stage-KLUE-RE-NLP-왕초보-후기-하겸" class="headerlink" title="P-stage (KLUE-RE) NLP 왕초보 후기(하겸)"></a>P-stage (KLUE-RE) NLP 왕초보 후기(하겸)</h1><h2 id="글을-쓰기에-앞서"><a href="#글을-쓰기에-앞서" class="headerlink" title="글을 쓰기에 앞서"></a>글을 쓰기에 앞서</h2><ul><li>저는 NLP에 어렴풋한 관심은 있었지만 그렇다고 NLP는 나의 인생을 걸만한 목표야! 까지는 않았던 사람입니다. 대충 어떤 모델, 방법론이 있다 정도만 알고 있는 수준이었죠.</li><li>하지만 공부를 할 때 그냥 눈으로만 개념을 익히면 완전히 자신의 것이 되지 않는 것은 모두 알고 있는 사실일 겁니다. 하다못해 게임만 하더라도 유튜브에서 고수들의 영상을 보는 것으로 자기가 그와 비슷한 고수가 되지는 않죠. 직접 연습, 플레이해야 자신의 것이 되는 것이겠죠.</li><li>저에게 이번 P-stage가 그랬습니다. 어렴풋이 알고 있는 것들을 직접 코딩하고, 모델링을 해보면서 조금은 뚜렷하게 받아들여진 것 같습니다.</li><li>이번 회고는 거의 아무것도 모르던 왕초보가 P-stage를 겪으면서 어떻게 초보가 되어 갔는지에 대한 기록입니다.</li></ul><h1 id="1-어떤-대회-인가요-KLUE는-뭐고-RE는-뭐죠-🤔"><a href="#1-어떤-대회-인가요-KLUE는-뭐고-RE는-뭐죠-🤔" class="headerlink" title="1. 어떤 대회 인가요? KLUE는 뭐고 RE는 뭐죠?🤔"></a>1. 어떤 대회 인가요? KLUE는 뭐고 RE는 뭐죠?🤔</h1><h3 id="1-1-KLUE"><a href="#1-1-KLUE" class="headerlink" title="1.1 KLUE"></a>1.1 KLUE</h3><ul><li>한국 사람이 NLP를 시작했다면 KLUE란 단어를 어디선가 들어 봤을 겁니다. 저도 그랬고요. 하지만 정확히 KLUE가 뭔지는 몰랐죠. 그냥 MNIST 같은 건가? 라고 어렴풋이 생각했을 뿐입니다.</li></ul><p><img src="/images/hakyeom/1.jpg" alt="1.jpg"></p><p><img src="/images/hakyeom/2.jpg" alt="처음 KLUE를 들었을 때의 내 심정.... 그래서 KLUE가 뭔데!!!!!"></p><p>처음 KLUE를 들었을 때의 내 심정…. 그래서 KLUE가 뭔데!!!!!</p><ul><li>KLUE가 뭔지 알려면 먼저 GLUE를 알아야합니다.<ul><li>GLUE : General Language Understanding Evaluation의 약자로 UW, NYU, DeepMind 등이 모여 협업한 벤치마크 데이터 세트입니다. 자연어 이해를 평가할 수 있는 11개의 과제들로 구성되어 있습니다. 다양한 언어 모델이 통일된 일련의 데이터 세트 안에서 평가될 수 있었고, 모델 간의 공정한 비교를 통해 언어 모델이 빠른 속도로 발전할 수 있는 원동력이 되었습니다.</li></ul></li><li>그렇다면 GLUE는 <strong>모든 언어</strong> 모델의 벤치마크를 할 수 있나요?<ul><li>GLUE가 공신력이 있고 논문에서도 종종 등장 하지만 영어로 이루어진 데이터 세트기 때문에 다른 언어에 대한 성능을 평가하는 것에는 어려움이 있었고 비 영어권 국가(프랑스, 중국, 러시아, 인도, 인도네시아 등)에서 벤치마크 데이터 세트가 공개 되었습니다.</li><li>한국어 자연어처리 분야에도 사전학습된 다양한 언어모델이 공개되고 활용되고 있었지만, 그 모델들을 공신력 있게 평가 할 수 있는 벤치 마크가 없었습니다. 기존에 제작된 한국어 데이터 세트들을 벤치마크로 활용하기 어려웠을 뿐 아니라 벤치마크 제작을 위해 다양한 기관의 협업을 이끌어 내기 어려웠기 때문입니다.</li></ul></li><li>그래서 KLUE가 등장했습니다.<ul><li>한국어 언어 모델이 구문록적, 의미론적 표상을 제대로 학습했는지 평가 할 수 있는 과제를 포함하고, 학계뿐 아니라 산업계에서도 수요가 있는 과제를 포함하는 한국어 벤치마크 데이터 세트… 그것이 KLUE 입니다.</li><li>KLUE는 다음과 같은 Task를 포함합니다.<ol><li>개체명 인식(Named Entity Recognition)</li><li>의존 구문 분석(Dependency Parsing)</li><li>문장 분류(Text classification)</li><li>자연어 추론(Natural Language Inference)</li><li>문장 유사도(Semantic Textual Similarity)</li><li>관계 추출(Relation Extraction)</li><li>질의 응답(Question &amp; Answering)</li><li>목적형 대화(Task-oriented Dialogue)</li><li>일상 대화 이해(Open-domain Dialogue Understanding)</li></ol></li></ul></li></ul><h2 id="1-2-RE"><a href="#1-2-RE" class="headerlink" title="1.2 RE"></a>1.2 RE</h2><ul><li>위에서 KLUE Task의 6번 관계 추출(Relation Extraction)의 약자가 RE입니다. 관계 추출이라… 어떤 것을 의미할까요?</li><li>관계 추출(Relation Extraction)은 문장의 단어(Entity)에 대한 속성과 관계를 예측하는 문제입니다. 관계 추출은 지식 그래프 구축을 위한 핵심 구성 요소로, 구조화된 검색, 감정 분석, 질문 답변하기, 요약과 같은 자연어처리 응용 프로그램에서 중요합니다. 비구조적인 자연어 문장에서 구조적인 triple을 추출해 정보를 요약하고, 중요한 성분을 핵심적으로 파악할 수 있습니다.</li><li>이렇게 봐서는 살짝 이해가 가지 않습니다. input data는 무엇이고 어떤 결과를 도출해 내면 되는지 확인해 보겠습니다.</li></ul><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">sentence:</span> 오라클(구 썬 마이크로시스템즈)에서 제공하는 자바 가상 머신 말고도 각 운영 체제 개발사가 제공하는 자바 가상 머신 및 오픈소스로 개발된 구형 버전의 온전한 자바 VM도 있으며, GNU의 GCJ나 아파치 소프트웨어 재단(ASF: Apache Software Foundation)의 하모니(Harmony)와 같은 아직은 완전하지 않지만 지속적인 오픈 소스 자바 가상 머신도 존재한다.<br><span class="hljs-symbol">subject_entity:</span> 썬 마이크로시스템즈<br><span class="hljs-symbol">object_entity:</span> 오라클<br><br><span class="hljs-symbol">relation:</span> 단체:별칭 (org:alternate_names)<br></code></pre></td></tr></table></figure><ul><li>즉 문장에 포함된 subject entity와 object entity의 관계를 도출해 내는 Task라고 이해 할 수 있습니다. 다만 한가지 기억해야 할 것은 같은 문장이더라도 subject entity와 object entity가 다르게 주어진다면 모델의 output 역시 달라져야 할 것입니다.</li></ul><h2 id="1-3-P-stage-개요"><a href="#1-3-P-stage-개요" class="headerlink" title="1.3 P-stage 개요"></a>1.3 P-stage 개요</h2><ul><li><p>단순하게 RE Task를 잘 수행하는 모델을 만들어 제출해라! 라고 말하면 아쉬우니 대회의 룰과 데이터의 통계를 살펴보겠습니다.</p></li><li><p>전체 데이터에 대한 통계는 다음과 같습니다.</p><ul><li>train.csv: 총 32470개</li><li>test_data.csv: 총 7765개 (정답 라벨 blind = 100으로 임의 표현)</li><li>학습을 위한 데이터는 총 32470개 이며, 7765개의 test 데이터를 통해 리더보드 순위를 갱신합니다. public과 private 2가지 종류의 리더보드가 운영됩니다.</li><li>Data 예시</li></ul><p><img src="/images/hakyeom/3.png" alt="1.png"></p><ul><li><p>column 1: 샘플 순서 id</p></li><li><p>column 2: sentence.</p></li><li><p>column 3: subject_entity</p></li><li><p>column 4: object_entity</p></li><li><p>column 5: label</p></li><li><p>column 6: 샘플 출처</p></li><li><p>Class 설명</p><p><img src="/images/hakyeom/4.png" alt="4.png"></p></li></ul></li><li><p><strong>대회 참여 규칙</strong></p><ul><li><p><strong>[외부 데이터셋 규정] 외부 데이터 사용은 금지합니다.</strong> 학습에 사용될 수 있는 데이터는 AI Stages 에서 제공되는 데이터셋 한 가지 입니다.</p></li><li><p><strong>[평가 데이터 활용] test_data.csv에 대한 Pseudo labeling을 금지합니다</strong>. <strong>test_data.csv을 이용한 TAPT(Task-Adaptive Pretraining)는 허용 합니다.</strong> 단 평가 데이터를 <strong>눈으로 직접 판별 후 라벨링 하는 행위는 금지합니다.</strong> 제공된 학습 데이터을 사용한 데이터 augumentation 기법이나, 생성모델을 활용한 데이터 생성 등, 학습 데이터를 활용한 행위는 허용 됩니다.</p><p>(학습 효율 측면에서 테스트셋의 라벨링을 추론하는 행위는 본 대회에서는 금지합니다)</p></li></ul></li></ul><h1 id="2-어떤-방식으로-문제-P-stage-를-해결-하려고-했나요-🧭"><a href="#2-어떤-방식으로-문제-P-stage-를-해결-하려고-했나요-🧭" class="headerlink" title="2. 어떤 방식으로 문제(P-stage)를 해결 하려고 했나요? 🧭"></a>2. 어떤 방식으로 문제(P-stage)를 해결 하려고 했나요? 🧭</h1><h2 id="2-1-기록의-기록에-의한-기록을-위한…"><a href="#2-1-기록의-기록에-의한-기록을-위한…" class="headerlink" title="2.1 기록의 기록에 의한 기록을 위한…"></a>2.1 기록의 기록에 의한 기록을 위한…</h2><ul><li><p>저희 팀 <strong>청계산 셰르파</strong>는 P-stage competition에서 1등을 하기보다는 부스트 캠프에서 공부, 시도, 도전 그 결과 여부에 상관없이 모든 것을 생생하게 기록으로 남기는 것을 목표로 모인 팀입니다. NLP를 잘하는 사람들이 모인 것이 아니라 기록과 그것을 공유하는 것을 가장 중요하게 여기는 캠퍼들이 모인 팀입니다. 굳이 따지면 <strong>NLP 실력 자체는 특출나게 뛰어나지는 않았습니다.</strong></p></li><li><p>닐 암스트롱의 발자국이 아직도 달에 남아 있습니다. 그 발자국처럼 저희도 오랫동안 지워지지 않을 기록을 위해 모인 것이죠. 이렇게 제출용 회고록 말고 따로 회고를 남기는 것도 그것을 위함입니다.</p></li><li><p>그렇기에 저희는 이번 P-stage에서 모든 것을 기록하기로 했습니다. 물론 모든 것을 다 할 수는 없지만 적어도 최대한 기록하려고 했습니다. 실험, 시도, 성공, 실패 그리고 그 모든 것을 함께 공유하기로 했습니다.</p><p><img src="/images/hakyeom/5.png" alt="이렇게 모델, 패러미터, 점수, 간단한 고찰만 기록하는 것이 아니라"></p><p>이렇게 모델, 패러미터, 점수, 간단한 고찰만 기록하는 것이 아니라</p></li></ul><p>  <img src="/images/hakyeom/5_1.png" alt="개인의 연구일지를 통해"></p><p>개인의 연구일지를 통해</p><p><img src="/images/hakyeom/5_5.png" alt="왜 그런 실험을 했고, 결과와 앞으로 어떤걸 할 것인지 간단하게라도 적었습니다."></p><p>왜 그런 실험을 했고, 결과와 앞으로 어떤 실험을 진행할 것인지 간단하게라도 적었습니다.</p><ul><li>물론 위에 언급한 기록 말고도 Kanban을 통한 실험 계획, 회의록, 멘토링, 유용한 자료, 논문, github 주소 등등 기록 할 수 있는 모든 것을 기록했습니다.</li><li>TMI로 이렇게 기록해두니 모델의 개선 같은 실용적인 부분뿐만 아니라 연구 일지에서 개개인의 스타일이 달라서 구경하는 재미가 쏠쏠했습니다. 👍</li></ul><h2 id="2-2-객체지향-기록지향-실험지향"><a href="#2-2-객체지향-기록지향-실험지향" class="headerlink" title="2.2 객체지향?! 기록지향!! 실험지향!!"></a>2.2 객체지향?! 기록지향!! 실험지향!!</h2><ul><li><p>위에서 언급한 것처럼 저희 팀은 기록과 공유가 1 옵션이었습니다. 그 결과 개인의 실험을 상세하게 기록할 수 있었고, 중복되는 실험이 없었으며, 효과가 있었던 방법 들을 그때그때 바로 적용 할 수 있었고, 별로였던 시도를 다른 인원이 다시 실험하지 않게 돼서 정말 다양한 실험을 할 수 있었던 것 같습니다.</p><p><img src="/images/hakyeom/6.png" alt="저 시도 하나하나가 중복이나 헛된 시도가 거의 없었습니다."></p><p>저 시도 하나하나가 중복이나 헛된 시도가 거의 없었습니다.</p></li><li><p>아래에서 언급 하겠지만 저희 조는 5등이라는 꽤 좋은 결과를 얻었습니다. 위에서 말한 것처럼 NLP 실력이 특출나지 않은, 어찌 보면 NLP 초보들이 모인 팀에서 이런 결과를 얻을 수 있었던 것은 기록을 철저하게 한 것, 공유를 활발히 한 덕분이라고 생각합니다.</p></li></ul><p><img src="/images/hakyeom/7.jpg" alt="「기록의 차이」가 느껴지십니까?"></p><p>「기록의 차이」가 느껴지십니까?</p><h2 id="2-3-기록과-공유가-1순위면-등수는-아예-신경을-쓰지-않았나요-🙄"><a href="#2-3-기록과-공유가-1순위면-등수는-아예-신경을-쓰지-않았나요-🙄" class="headerlink" title="2.3 기록과 공유가 1순위면 등수는 아예 신경을 쓰지 않았나요?!🙄"></a>2.3 기록과 공유가 1순위면 등수는 아예 신경을 쓰지 않았나요?!🙄</h2><p><img src="/images/hakyeom/8.png" alt="그게 아니죠~!"></p><p>그게 아니죠~!</p><ul><li>여러 부자들이 돈 자체가 인생의 목적이어서 부자가 된 게 아니고 좋아하는 일, 의미 있는 일을 하다 보니 돈은 알아서 따라 들어왔다고 말을 합니다. 이와 비슷한 맥락으로 저희는 기본을 탄탄히, 차근차근히 하면 등수는 알아서 따라오리라 생각했습니다. 실제로도 그랬습니다.</li><li>기본을 탄탄히 한다는 것이 어떤 뜻일까요? 실험을 하면 변인 통제를 확실히 하고, 그 결과를 <strong>기록</strong>및 <strong>공유</strong>하며 그 결과에 따라 다음 실험 및 시도를 논리적으로 선택하는 것이 아닐까요? 결국 기록과 공유입니다.</li><li>즉 등수를 아예 신경 쓰지 않았다기보다는 조금은 귀찮고 힘들 수 있어도 기록을 꼼꼼히, 그리고 그것을 확실히 공유하면 당연히 따라 들어오는 상 정도로 받아들이기로 했습니다. 만약 기록과 공유를 철저히 했음에도 불구하고 하위권 점수를 얻었으면 저희 조의 컨셉을 바꾸게 됐을 지도 모르지만(가능성은 작지만) 결국 올바른 선택이었음이 판명되었습니다.</li></ul><h1 id="3-등반일지-🏔"><a href="#3-등반일지-🏔" class="headerlink" title="3. 등반일지 🏔"></a>3. 등반일지 🏔</h1><p>등반일지는 약 2주일가량 펼쳐진 P-stage의 저희 팀의 실험을 정말 <strong>간략</strong>하게만 써보겠습니다. 모든 시도를 적기에는 너무 늘어지는 감이 있으니 대략적으로만 느끼면 좋을 것 같습니다.</p><h2 id="3-1-산-초입"><a href="#3-1-산-초입" class="headerlink" title="3.1 산 초입"></a>3.1 산 초입</h2><ul><li>대회 초반에는 데이터 EDA를 간단하게 하며 팀원 모두 Hugging face의 transforemrs 라이브러리 및 앞으로 사용하게 될 베이스라인의 리팩토링, 추가 기능 추가 등을 신경 썼습니다</li><li>다른 팀원들은 모르겠으나 저 같은 경우 저번 P-stage에 K-fold, Wandb 등을 활용하지 못한 것이 아쉬웠으나 이번 조에서는 그 부분을 일찍부터 활용 할 수 있어서 정말 만족했습니다.</li><li>또 기록을 남기기 위한 캠퍼들의 모임이니 하이퍼 패러미터, 실험 내용과 결과, 시도하고 있는 것들을 기록하는 페이지를 노션에 구축함으로 앞으로 있을 다양한 실험의 기초 토대를 마련했습니다.</li><li>padding 방식도 fixed에서 dynamic padding으로 바꾸면서 모델 학습에 걸리는 시간을 약 30%를 단축 할 수도 있었습니다.</li><li>그 와중에 리더보드 1등을 운 좋게 달성해서 기분 삼아 스크린샷으로도 남겼습니다.</li></ul><p><img src="/images/hakyeom/9.png" alt="그래도 1등 한번 해봤으니 정상은 아니지만 사진 한번 찍어봤습니다. &quot;김치~&quot; &quot;찰칵&quot;"></p><p>그래도 1등 한번 해봤으니 정상은 아니지만 사진 한번 찍어봤습니다. “김치~” “찰칵”</p><h2 id="3-2-산-중턱"><a href="#3-2-산-중턱" class="headerlink" title="3.2 산 중턱"></a>3.2 산 중턱</h2><ul><li>다른 팀들의 점수가 올라오는 와중에 벽으로 느껴졌던 F1 score 70을 넘는 팀들이 등장하기 시작했습니다.</li><li>이 와중에 저희 조 실험을 통해서 klue/roberta-large 모델을 통한 classification 모델을 사용하면 F1 score가 70을 넘을 수 있다는 사실을 깨달았습니다.</li><li>그것과 동시에 기존 입력 <code>[CLS] 이순신 [SEP] 조선 [SEP] 이순신은 조선 중기의 무신이다. [SEP]</code>의 input format에서 <code>[CLS] # * entity 1 map * 이순신#은 @ * entity 2 map * 조선 @ 중기의 무신이다. [SEP]</code>와 같은 input format으로 변경하면 점수가 더 오르는 실험 결과도 얻을 수 있었습니다.</li><li>hugging face의 hp_search를 이용해서 모델 학습의 최적의 하이퍼 패러미터를 찾았습니다. 이 최적화된 하이퍼 패러미터로 F1 score를 약 2점가량 얻을 수 있었습니다.</li><li>fp16 (16-bit Floating point)를 모델 학습에 사용함으로 성능은 비슷하되 학습 시간을 60%가량 향상된 속도로 모델 실험에 박차를 가 할 수 있었습니다. Roberta-large 기준 K-fold를 수행하면 대략 학습에 10시간 이상 걸리던 시간을 3~4시간 정도로 단축했습니다.</li></ul><p><img src="/images/hakyeom/10.png" alt="이거 정말 빠릅니다.... 스포츠카를 탄 느낌이었어요."></p><p>이거 정말 빠릅니다…. 스포츠카를 탄 느낌이었어요.</p><ul><li>또 학습이 완료된 모델을 분석함으로 모델이 어떤 Label을 헷갈려 하는지, 어떤 데이터를 Augmentation 해야 하는지를 정했고 EDA, AEDA, Back Translation, subject object changing을 수행하여 data imbalance를 극복하고자 했습니다.</li><li>모델 자체의 실험도 진행했습니다. hugging face에서 <code>AutoModelForSequenceClassification</code> 만을 사용하는 것이 아니라 label에 따라 model을 split 해보기도 하고 기존 모델들의 출력을 다시 이용하여 classification 하는 combine model, entity embedding을 이용하는 <a href="https://github.com/monologg/R-BERT">R-BERT</a>, RE task에서 sota를 기록한 <a href="https://github.com//Saintfe/RECENT">recent</a>도 시도해보았습니다.</li><li>다양한 Loss function도 도입해봤습니다. 단순 cross entropy 보다 imbalence data에 효과적이라는 <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf">CB loss</a>, <a href="https://arxiv.org/pdf/1906.07413.pdf">LDMA loss</a> 를 도입했습니다.</li><li>또 TAPT를 통하여 MLM 모델에 Test data도 포함한 NLP data의 문장에 대한 확률 분포도 재학습해 봤습니다.</li><li>위의 모든 실험이 성과를 본 것은 아니었지만 2주가 짧은 시간, 그리고 딥러닝의 학습에 오랜 시간이 걸린다는 것을 생각해보면 많은 시도였다고 생각합니다. 이 모든 것이 기록과 공유를 통해 낭비 없는 실험을 한 덕분이라고 생각합니다.</li></ul><h2 id="3-3-정상-직전"><a href="#3-3-정상-직전" class="headerlink" title="3.3 정상 직전"></a>3.3 정상 직전</h2><p><img src="/images/hakyeom/11.jpg" alt="img3.daumcdn.jpg"></p><ul><li>저번 P-stage에서의 경험, 그리고 여러 Competition의 솔루션, 후기 등등을 읽으면서 느낀 것이 하나 있습니다. 바로 <strong>앙상블은 언제나 옳다</strong>는 것입니다. (가볍게 하는 말이니 사실이 아니더라도 너그럽게 봐주세요.)</li><li>저희 팀의 앙상블 전략은 크게 두 가지 였습니다.<ul><li>좀 똘똘한 애들로 모아보자!<ul><li>함께 문제를 풀더라도 머리 좋은 애들끼리 문제를 풀면 조금 더 좋은 성과가 있지 않겠습니까? 그래서 리더보드 기준으로 F1 score가 높은 모델들을 앙상블을 진행했습니다. 그 결과  F1 score는 public 리더보드 기준 74.306으로 9위로 마무리 할 수 있었습니다.</li></ul></li><li>머리는 좋은데 공부를 안 한 애들을 모아보자!<ul><li>F1 score와는 상관 없이 AUPRC가 높은 모델들을 앙상블을 진행했습니다. 학습(공부)가 좀 덜되서 F1 score는 낮더라도 precision이나 recall이 적절하게 분배된 것이 아니라 한쪽으로 특화되었기 때문에 F1 score은 낮을지라도 AUPRC가 더 높지 않을까? 그 모델들을 앙상블 하면 안정된 precision이나 recall을 얻어 시너지를 낼 수 있지 않을까 싶어 시도한 전략입니다.</li></ul></li></ul></li></ul><h2 id="3-4-하산"><a href="#3-4-하산" class="headerlink" title="3.4 하산"></a>3.4 하산</h2><p><img src="/images/hakyeom/13.png" alt="퍼블릭.PNG"></p><ul><li>public 기준으론 9등을 했습니다. 앙상블로 3등 올랐습니다.</li></ul><p><img src="/images/hakyeom/14.png" alt="프라이빗.PNG"></p><ul><li>private 기준으로 5등입니다. AUPRC 기준 앙상블이어서 그런건진 몰라도 다른 팀보다 점수 하락 폭이 적었습니다. 무려 4등이나 올랐습니다.</li><li>하지만 점수로 나눈 등수보다 제출 횟수 99회(1등)가 더 뜻  깊게 다가옵니다. 이는 모두가 노력한 결과라고 생각합니다.</li><li>저희가 모일 때 한가지 다짐했던 게 있습니다. 앞만 보지 말고, 누군가의 등을 보지 않고, 서로서로 도와주며 함께 나아가자는 것입니다.</li><li>누군가 실력이 좋아서 그 사람만 의지했다면 제출 횟수가 저렇게 높지 않았을 것으로 생각합니다. 서로서로를 도와주며 함께 나아 갔기에 얻은 점수보다 소중한 제출 횟수입니다.</li></ul><h1 id="4-훌륭한-이야기입니다-그런데-님은-뭐-했어요"><a href="#4-훌륭한-이야기입니다-그런데-님은-뭐-했어요" class="headerlink" title="4. 훌륭한 이야기입니다. 그런데 님은 뭐 했어요?"></a>4. 훌륭한 이야기입니다. 그런데 님은 뭐 했어요?</h1><p>살짝 반전이긴 한데 최종 기출 기준으로는 없어요. AUPRC 기준의 앙상블을 제가 하긴 했는데 그건 누군가 한 명이 코드 돌리기만 하면 됐던 일이라 딱히 제가 팀에 기여한 건 아니군요.</p><p>저희 팀원 모두가 훌륭한 분들이고 충분히 더 상위의 등수를 얻을 수 있었는데 못한 건… <strong>네, 그렇습니다. 사실 제가 범인이었어요.</strong></p><p><img src="/images/hakyeom/15.jpg" alt="img1.daumcdn.jpg"></p><p>그래서 기여한 부분이 아니라 시도했던 부분을 설명해 보고자 합니다. 후술하겠지만 결론은 운이 없었고 그보다 실력이 더 없었어요.</p><h2 id="4-1-이게-되네-⁉"><a href="#4-1-이게-되네-⁉" class="headerlink" title="4.1 이게 되네 ⁉"></a>4.1 이게 되네 ⁉</h2><p><img src="/images/hakyeom/16.png" alt="저희가 많이 사용하고 앙상블의 대부분을 담당했던 모델입니다."></p><p>저희가 많이 사용하고 앙상블의 대부분을 담당했던 모델입니다.</p><ul><li>위에도 설명했듯이 저희 조의 input format은 subject entity와 object entity에 대한 정보라고는 문장에 껴 넣은 punctuation marker와 entity map뿐입니다. 물론 사람이 보기에는 문장에 뜬금없이 저런 게 들어가 있고 몇 번 보다 보면 저렇게 표시된 단어들끼리의 상관관계를 알 수 있을지도 모릅니다.</li><li>딥러닝이 사람의 뇌가 학습하는 방법을 따라 했다는 것도 이제는 옛말이고 지금 와서는 거대한 비선형 근사식이 아니겠습니까? 과연 모델이 그냥 많은 문장들을 보면서 maker와 반복되어 나타나는 entity map을 보면서….</li></ul><p><img src="/images/hakyeom/17.png" alt="6d58e24f4920a4368a4f8cad8db0230e.png"></p><p>***”음 저건 subject entity군, 이건 object entity고 저기 <em>로 표시된 건 entity의 type을 변환한 entity map이라 각 entity의 정보를 얻어 내는 데 도움이 되겠군. 그러니까 이러쿵저러쿵 해서 #으로 둘러 쌓인 것과 @로 둘러싸인 것의 관계는 이거네”</em>**</p><ul><li><p>라고 한다고요?!</p></li><li><p>물론 실험결과는 성능도 잘 나오긴 했는데 납득이 안가잖아요.</p></li><li><p>위대한 아인슈타인을 감히 제가 이해했다고 말하긴 죄송스럽지만, 양자역학을 거부한 심정이 이해가 갔습니다. 분명 양자역학은 양자들의 원리를 <strong>성공적</strong>으로 설명해냈지만 납득이 안가잖아요?! 저도 그랬습니다. 저 모델의 성능이 아무리 <strong>좋게</strong> 나와도 납득이 가지 않잖아요?</p></li></ul><p><img src="/images/hakyeom/18.jpg" alt="막상 배울땐 몰랐지만 비슷한 경험을 하고 나니까 조금은 이해할 수 있었습니다."></p><p>막상 배울땐 몰랐지만 비슷한 경험을 하고 나니까 조금은 이해할 수 있었습니다.</p><h2 id="4-2-하라는-공부는-안하고-😡"><a href="#4-2-하라는-공부는-안하고-😡" class="headerlink" title="4.2 하라는 공부는 안하고! 😡"></a>4.2 하라는 공부는 안하고! 😡</h2><p><img src="/images/hakyeom/19.png" alt="R-BERT"></p><p>R-BERT</p><ul><li><p>그래서 제가 찾은 모델 아키텍처는 <a href="https://arxiv.org/abs/1905.08284">R-BERT</a> 였습니다.</p></li><li><p>저기 그림에 표현된 토큰들과 특수문자가 보이시나요? 딱 저희의 input 형식이 저런 형식이었잖아요? 게다가 추가로 entity mask를 사용해서 BERT의 output vector에 추가적인 entity는 무엇이라는 정보를 classification에 사용 할 수 있었습니다.</p></li><li><p>BERT 부분만 우리가 사용하는 Roberta로 바꾸면 완벽할 것만 같았습니다. entity 정보를 특수문자로만 학습하던 애가 특수문자 + entity mask까지 사용하면 당연히 성능이 올라야 하지 않겠습니까? 학원, 독서실 안 보내도 공부 잘하던 애를 학원, 독서실 보내면 성적이 더 올라서 전교 1등을 할 수 있다는 기대를 해도 되지 않겠습니까?</p></li><li><p>그리고 결과는…</p><p><img src="/images/hakyeom/20.png" alt="123123123.png"></p></li><li><p>당시 저희 조에서 성능을 내던 방식은 F1 score 72점 후반에서 73점 중반이었습니다. 그러니까 오히려 성능이 하락한 것이죠.</p></li><li><p>그러니까 공부하라고 학원, 독서실 보내놨는데 PC방에 가버린 꼴입니다.</p><p><img src="/images/hakyeom/21.jpg" alt="사실 독서실간다 하고 PC방가서 노는게 몇배는 더 재밌습니다. 저도 알아요."></p><p>사실 독서실 간다 하고 PC방가서 노는 게 몇 배는 더 재밌습니다. 저도 알아요.</p></li><li><p>그 후 R-BERT에서 더 이상 성능을 낼 수 없을 거라 예상하고 실험은 종료했습니다.</p></li></ul><h2 id="4-3-나는-나는-나는-나는-R-BERT를-했다-😇"><a href="#4-3-나는-나는-나는-나는-R-BERT를-했다-😇" class="headerlink" title="4.3 나는! 나는! 나는! 나는! R-BERT를 했다! 😇"></a>4.3 나는! 나는! 나는! 나는! R-BERT를 했다! 😇</h2><p><a href="https://www.youtube.com/watch?v=x7w7CeiyLEE&t=1130s">https://www.youtube.com/watch?v=x7w7CeiyLEE&amp;t=1130s</a></p><ul><li>대회가 끝나고 1, 2위 팀의 솔루션과 추가 발표로 3위 팀의 솔루션을 들을 수 있었습니다.</li><li>여기서는 적지 않았으나 1, 2위 팀의 솔루션은 저희 팀에서 시도 했지만 실력, 시간등 복합적인 문제로 저희 조에 적용을 못시켰던 방법들 이었습니다. 살짝 아쉬웠습니다.</li><li>문제는 3등 팀의 솔루션이었습니다. 저희 조는 5등이었지만 3위 팀과는 0.13점 차이로 3, 4, 5 등 팀의 차이는 정말 한 끗차이었습니다. 헌데 3등 팀에서 사용했던 모델중에 R-BERT가 있다는 순간의 심정은 글로는 설명 못 할 것 같습니다.</li><li>앙상블은 보통 다양한 모델을 조합하면 성능이 좋아진다고 알려져 있었습니다. R-BERT의 실험이 성공적이어서 저희 조 앙상블 모델에 R-BERT가 추가되었더라면 0.13 차이는 물론이고 더 위도 어쩌면 가능하지 않았을까? 라는 생각이 드는건 어쩔 수가 없었습니다.</li><li>4챕터를 쓰면서 운이 살짝 없었다고 적긴 했지만 다시 생각해보면 운은 상관 없고 그냥 저의 실력 부족, 그러니까 실험을 제대로 하지 않은 탓이라고 생각이 드네요. 저 자신에게 크게 실망했습니다.</li><li>나름 실험을 꼼꼼하게 한다고 했는데 충분하지 않았습니다. 대회 마지막 가서는 성능을 내야 한다는 조급한 마음에 이도 저도 안됐던 것 같습니다. 다른 팀원 분들에게 미안할 따름입니다.</li></ul><h1 id="5-다음-목적지-🏔"><a href="#5-다음-목적지-🏔" class="headerlink" title="5. 다음 목적지 🏔"></a>5. 다음 목적지 🏔</h1><ul><li>바로 MRC 대회가 열리기 때문에 자책하는 것은 그만두고 빠르게 재정비해서 MRC 대회에 임해야겠다고 생각했습니다.</li><li>아쉬운 것은 아쉬운 거고 앞으로 더 잘하면 되지 않겠어요?</li><li>그래도 두 번의 P-stage를 경험하면서 계속 성장하고 있다는 게 느껴져서 다음에는 더 잘 할 수 있을 거라고 생각하고 있습니다.</li><li>그런 의미로 1등 조원 한분께 <strong>다음 대회는</strong> <strong>각오하세요. 1등 하겠습니다.</strong> 라고 말은 했습니다 99% 농담이지만요. 헌데 그분이 웃으시면서 <strong>“같이 성장해 나가요.”</strong> 라는 답변을 들었는데 눈부셔서 바라볼 수도 없었습니다.</li></ul><p><img src="/images/hakyeom/22.jpg" alt="사실 이렇게 보였을지도 모릅니다. ㅋㅋ"></p><p>사실 이렇게 보였을지도 모릅니다. ㅋㅋ</p><ul><li>최종 결론은 다음 P-stage도 성장하고 싶습니다. 혼자 아니라 팀원, 캠퍼 모두하고 함께 성장했으면 좋겠습니다.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>KLUE - Relation Extraction</title>
    <link href="/KLUE_RE/"/>
    <url>/KLUE_RE/</url>
    
    <content type="html"><![CDATA[<h1 id="청계산셰르파의-KLUE-RE-등반일지"><a href="#청계산셰르파의-KLUE-RE-등반일지" class="headerlink" title="청계산셰르파의 KLUE-RE 등반일지"></a>청계산셰르파의 KLUE-RE 등반일지</h1><p>약 12일동안 참여했던 P Stage의 <code>KLUE - Relation Extraction</code> 대회의 Wrap up Report이자 회고록입니다. 철저한 <code>기록</code>과 <code>공유</code>에 공감한 7명의 팀원이 처음 합을 맞추었기 때문에 어설픈 점도 있었지만, 그간의 생생한 기록들을 바탕으로 어떻게 서로가 서로의 <code>셰르파</code>로써 등반을 완료할 수 있었는지 저희의 경험을 나누고자 합니다.</p><p>물론 미흡한 점도 있었고, 여러 시행착오를 겪었지만 결과적으로는 팀원 모두가 만족한 협업이었다는 점에서 지난 대회보다 많은 성장을 할 수 있었다고 느낄 수 있었습니다. 저희의 경험이 어떠한 형태로든 도움이 되고, 좋은 레퍼런스가 되기를 바라며 시작해보도록 하겠습니다.</p><br><p><strong>목차</strong></p><ul><li><a href="#%E2%9A%94%EF%B8%8F-%ED%8C%80-%EC%86%8C%EA%B0%9C">팀소개</a><ul><li><a href="#%F0%9F%A4%9C-%EB%B6%80%EB%A1%9D-%ED%8C%80%EC%9B%90%EB%93%A4-%ED%95%9C%EB%A7%88%EB%94%94">청계산셰르파</a></li><li><a href="#%F0%9F%91%A8%E2%80%8D%F0%9F%91%A8%E2%80%8D%F0%9F%91%A6%E2%80%8D%F0%9F%91%A6-%EC%B2%AD%EA%B3%84%EC%82%B0%EC%85%B0%EB%A5%B4%ED%8C%8C%EB%93%A4">청계산셰르파들</a></li></ul></li><li><a href="#%F0%9F%94%8E-%EB%8C%80%ED%9A%8C-%EA%B0%9C%EC%9A%94">대회개요</a></li><li><a href="#%F0%9F%A4%9D-%ED%98%91%EC%97%85">협업</a><ul><li><a href="#%F0%9F%A5%BE-%EC%82%AC%EC%A0%84%EB%85%BC%EC%9D%98">사전논의</a></li><li><a href="#%E2%9A%99%EF%B8%8F-%ED%98%91%EC%97%85%ED%88%B4">협업툴</a></li><li><a href="#%F0%9F%92%BB-%EC%BD%94%EB%93%9C-%EA%B4%80%EB%A6%AC">코드관리</a></li><li><a href="#%F0%9F%A7%91%E2%80%8D%F0%9F%94%AC-%EC%B2%B4%EA%B3%84%EC%A0%81%EC%9D%B8-%EC%8B%A4%ED%97%98">체계적인 실험</a></li></ul></li><li><a href="#%F0%9F%9B%8B-Data-Experiments">Data Experiments</a><ul><li><a href="#%F0%9F%91%81-Data-EDA">Data EDA</a></li><li><a href="#%F0%9F%92%AA-Data-Augmentation">Data Augmentation</a></li><li><a href="#%F0%9F%94%A7-Data-Preprocessing-amp-Tokenizer">Data Preprocessing</a></li></ul></li><li><a href="#%F0%9F%A7%98-Modeling">Modeling</a></li><li><a href="#%F0%9F%8D%9C-Ensemble">Ensemble</a><ul><li><a href="#%F0%9F%91%A8%E2%80%8D%F0%9F%8E%A8-%EC%95%99%EC%83%81%EB%B8%94-%EA%B9%8E%EB%8A%94-%EB%85%B8%EC%9D%B8%EA%B3%BC-%EA%B8%B0%EB%8F%84%EB%A9%94%ED%83%80">앙상블 깎는 노인과 기도메타</a></li></ul></li><li><a href="#%F0%9F%92%AF-Good-Practice">Good Practice</a></li><li><a href="#%F0%9F%92%8C-Thanks-to">Thanks to</a></li><li><a href="#%EB%A7%88%EC%A7%80%EB%A7%89%EC%9C%BC%EB%A1%9C">마지막으로</a></li><li><a href="#%F0%9F%A4%9C-%EB%B6%80%EB%A1%9D-%ED%8C%80%EC%9B%90%EB%93%A4-%ED%95%9C%EB%A7%88%EB%94%94">부록: 팀원들 한마디</a></li></ul><h2 id="⚔️-팀-소개"><a href="#⚔️-팀-소개" class="headerlink" title="⚔️ 팀 소개"></a>⚔️ 팀 소개</h2><h3 id="⛰-청계산셰르파"><a href="#⛰-청계산셰르파" class="headerlink" title="⛰ 청계산셰르파"></a>⛰ 청계산셰르파</h3><p align="center">    <img src="/images/profile.png" style="display: inline" height="120px">    <img src="/images/2.png" style="display: inline" height="120px"></p><p>저희는 캠프 기간동안 모든 것을 생생하게 기억하고 나누는 <code>기록</code>과 <code>공유</code>라는 가치에 공감한 7명이 모여 팀을 구성했고, 서로가 서로의 가이드로서 좋은 영향을 주고받을 수 있는 셰르파가 되기를 원했습니다.</p><p>또한 주니어 엔지니어들의 로망은 판교역 근처 회사들에서 일을 하는 것입니다. 저희는 판교역의 뒷산인 청계산을 부스트캠프 과정에 빗대어 완벽하게 등반해보겠다는 의미로 청계산과 셰르파를 더해 <code>청계산셰르파</code>라는 이름을 사용하게 되었습니다.</p><h3 id="👨‍👨‍👦‍👦-청계산셰르파들"><a href="#👨‍👨‍👦‍👦-청계산셰르파들" class="headerlink" title="👨‍👨‍👦‍👦 청계산셰르파들"></a>👨‍👨‍👦‍👦 청계산셰르파들</h3><ul><li><a href="https://github.com/l-yohai"><img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/49181231?s=96&amp;v=4" width="20px"/></a> &nbsp; 이요한</li><li><a href="https://github.com/ddobokki">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/44228269?s=96&v=4" width="20px"></a> &nbsp; 문하겸</li><li><a href="https://github.com/20180707jun">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/50571795?s=96&v=4" width="20px"></a> &nbsp; 전준영</li><li><a href="https://github.com/godjw">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/47168115?s=96&v=4" width="20px"></a> &nbsp; 정진원</li><li><a href="https://github.com/lexiconium">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/84180121?s=96&v=4" width="20px"></a> &nbsp; 김민수</li><li><a href="https://github.com/hyeong01">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/38185429?s=96&v=4" width="20px"></a> &nbsp; 정희영</li><li><a href="https://github.com/jskwak98">  <img style="border-radius: 50%; display: inline" src="https://avatars.githubusercontent.com/u/47588410?s=96&v=4" width="20px"></a> &nbsp; 곽진성</li></ul><h2 id="🔎-대회-개요"><a href="#🔎-대회-개요" class="headerlink" title="🔎 대회 개요"></a>🔎 대회 개요</h2><p>이번에 참여한 대회의 과제는 문장 내 개체간 관계 추출 과제로, 문장의 단어(Entity)에 대한 속성과 관계를 예측하는 자연어 처리 과제입니다. 관계 추출은 비구조적인 자연어 문장에서 구조화된 정보를 추출하는 데에 목적을 두고 있습니다.</p><p>저희가 사용한 데이터셋은 <a href="https://arxiv.org/pdf/2105.09680.pdf">「KLUE: Korean Language Understanding Evaluation」(Park et al., 2021)</a>을 통해 공개된 KLUE-RE 데이터셋으로, KLUE 데이터셋을 사전학습한 RoBERTa를 fine-tuning하여 베이스모델로 사용하였으며, private leaderboard에서 f1 score 기준 19팀 중 5등, auprc 기준 19팀 중 2등을 기록하였습니다.</p><p>🥈 Final Score<br><img src="/images/final.png"></p><ul><li>micro F1 score: 73.732 (19팀 중 <strong>5등</strong>)</li><li>AUPRC score: 82.964 (19팀 중 <strong>2등</strong>)</li></ul><p><img src="/images/kakaotalk.png" alt="묘비아님"></p><h2 id="🤝-협업"><a href="#🤝-협업" class="headerlink" title="🤝 협업"></a>🤝 협업</h2><h3 id="🥾-사전논의"><a href="#🥾-사전논의" class="headerlink" title="🥾 사전논의"></a>🥾 사전논의</h3><p>RE 대회가 시작하기 1주일 전부터, 저희는 미리 대회를 위한 전략들을 구상했습니다. 서로의 스타일을 모르기에 당장은 결정이 안나더라도, 세부적인 사항들에 대한 논의를 단 한번이라도 거쳤던 것은 추후에 의사결정속도와 팀원들의 만족에 있어서 큰 영향을 끼쳤습니다.</p><details>    <summary>    대회 전 논의사항들    </summary><br><ul><li><p>프로젝트관리 툴/채널 정하기</p><ul><li>ex) Github project의 kanban board, notion, slack, zoom, google meet, git page, github action 등등</li><li>카카오톡 (슬랙보다 많이 접함.) 슬랙처럼 스레드 형식의 대화를 할 수 없음.<ul><li>중요한 이슈가 생기면 슬랙에도 같이 이야기하자.</li></ul></li></ul></li><li><p>코드명세서 or 컨벤션</p><ul><li>naming<ul><li>클래스, Static Vars = CamelCase</li><li>변수명, 함수명 = snake_case</li></ul></li><li>formatting (&amp; auto formatter)<ul><li>autopep8</li><li>black, yap…</li></ul></li><li>annotation<ul><li>‘’’ docstring ‘’’</li><li>VSCODE CODE_ANCHORE<ul><li>TODO, NOTE,</li></ul></li><li>docstring을 알아서 해주는 게 있는지 서치해보기!</li><li>필요한 내용만 작성할 수 있도록 룰을 추가로 정할 것.<br>주석은 영문으로!!</li></ul></li><li><del>indentation (tab 1 or space 4)</del></li><li>그 외 vscode extensions<ul><li>git graph, git lens</li><li>Live share</li></ul></li></ul></li><li><p>가상환경이나 환경관리 전략 (zsh, dotenv, … conda, pip …)</p><ul><li><strong>conda 사용!</strong></li></ul></li><li><p><strong>미정</strong></p><ul><li>브랜치를 어떻게 만들어놓을것인가 (실험전략까지 고려)<br>  다른사람들이 checkout만으로 동일한 실험을 할 수 있게 하기 위함<br>  MAIN<ul><li>DEVELOP<pre><code>  유저편의성, 코드기능개선, 버그픽스  접두어/feature</code></pre></li><li>BASELINE<br>  접두어/feature x 실험 o<br>다른 태스크에 대한 것들이나 (실험위주)<br>  baseline/qa/1<br>  baseline/ner<br>  baseline/sentence classification</li></ul></li><li>너무 브랜치가 많아질 수 있다.<ul><li>config를 변경하면 브랜치를 분기하지 않을 수 있다.</li></ul></li></ul></li><li><p>(선택사항) 여유가 되면 자세하게 써놓기</p></li><li><p>커밋전략</p><ul><li>commit message 규칙이나 템플릿 정하기</li></ul></li><li><p>(작업단위와 리뷰에 대해서 더 생각해보기)</p></li><li><p>PR 템플릿 및 PR/Review 전략 구체화</p><ul><li>PR 올리는 타이밍(시점)<ul><li>성능향상에 의한 PR은 모두가 리뷰</li><li>자잘한 변화들은 책임자만 리뷰</li></ul></li><li>리뷰를 어떻게 할 건지? 리뷰어는 몇 명</li><li>속도? Merge속도에 대해서 데드라인이 있었음 좋겠다.</li></ul></li><li><p>그라운드룰 및 빠른 의사결정을 위한 협업가치 리스트업 및 우선순위 정하기</p><ul><li>ex) 안정성(예외처리), 구성원의만족, 가독성, 일관성, 객체지향성, 단순성, 외부유저의경험, 신속성(작업속도), 통제가능성, 학습가능성, 취업적용가능성, 실험가능성 등등</li></ul></li></ul></details><p>또한 이러한 사전논의를 통해 대회가 진행될수록 팀원들이 무엇을 원하는지 확실히 알 수 있었으며, 모두가 같은 그림을 그리고 같은 방향을 가지고 대회에 임할 수 있었습니다.</p><p>이러한 노력을 토대로 저희는 빈틈없이 기록을 이어가고, 실험을 공유하며 항상 새로운 실험아이디어를 얻을 수 있었습니다. 그 결과 하루에 10번씩 제출가능한 대회에서 12일 동안 다른 팀들 대비 압도적인 횟수인 <em><strong>99번의 실험결과를 제출</strong></em> 해볼 수 있었습니다.</p><h3 id="⚙️-협업툴"><a href="#⚙️-협업툴" class="headerlink" title="⚙️ 협업툴"></a>⚙️ 협업툴</h3><p>지난 대회 때 다양한 협업 채널을 구성하였을 때 오히려 혼란이 가중되고 모든 채널을 사용하기 힘들다는 점을 고려하여, 대부분의 협업을 <code>Notion</code>으로 진행했습니다. 또한 불가피한 상황에 대비한 연락수단으로 <code>kakaotalk</code>과 <code>slack</code>을 사용하였고, <code>git</code>은 사용가능한 기능을 최소한으로 하고 코드관리 수단으로만 사용하였습니다.</p><p><code>Notion</code>에서 일정관리, 문서관리, 실험관리 등 많은 기능을 사용하기 위해 엄밀하게 Template을 찾아보며 Dashboard를 구성하였고 그렇게 아래와 같은 페이지를 구성할 수 있었습니다.</p><p align="center">    <img src="/images/notion_main.png" style="display: inline" height="220px" width="32%">    <img src="/images/notion_kanban.png" style="display: inline" height="220px" width="32%">    <img src="/images/notion_schedule.png" style="display: inline" height="220px" width="32%"></p><p><strong>Main Dashboard</strong></p><ul><li><p>sub page들의 link와 zoom, wandb, github, drive 링크들</p></li><li><p>TODO와 실험관리를 위한 kanban 보드</p><ul><li><p>실험을 위한 Process별 Tag 부착</p>  <img src="/images/no_tag.png" height="30%" width="30%"></li><li><p>Assignee 할당</p></li></ul></li><li><p>일정관리를 위한 schedule</p><ul><li>알림기능 활성화</li><li>용도별 Tag 부착</li></ul></li><li><p>Reference와 Docs 링크들</p><ul><li>회의록</li><li>멘토링</li><li>연구일지 등</li></ul></li></ul><h3 id="💻-코드-관리"><a href="#💻-코드-관리" class="headerlink" title="💻 코드 관리"></a>💻 코드 관리</h3><p><img src="/images/git1.png"></p><p>초반에는 전체 코드를 관리하기 위해 PR-Merge 방법으로 진행하다가 Review가 늦어지거나, 작업시간이 오래걸리면 다른 팀원이 같은 작업을 하는 등 예상치 못한 병목이 발생하고 오히려 개인 실험에 방해요소로 작용할 수 있다는 판단을 하게 되었습니다.</p><style>  .linear_highlight {      background: linear-gradient(to top, #778899 10%, transparent 10%);  }</style><span class="linear_highlight">따라서 baseline으로 사용할 수 있는 코드에서 각자의 이름 혹은 실험 이름으로 분기를 나누어 개인 작업을 진행하면서, 사전에 논의했던 것처럼 score가 올랐을 경우에만 baseline 코드에 PR-Merge를 하고, 해당 score를 재현가능할 수 있게끔 버전업하기로 했습니다.</span><p><img src="/images/bran.png"></p><p>branch가 많아지긴 했지만, 실험에 실패했을 때 빠르게 Rollback할 수 있었고, 각자의 실험에서 확실한 성능향상 요소만을 합칠 수 있었습니다. Competition이라는 플랫폼의 특성상 7명의 팀원이 각자 작성한 모든 코드들을 Review하고 합치면서 작업을 이어나가기에는 많은 시간과 노력을 필요로 했습니다. 하지만 기준을 두고 필요할 때만 코드를 병합하니 실험은 실험대로 잘 이루어지고, 실험에 실패하더라도 가장 최신버전의 코드를 모두가 사용할 수 있었다는 점에서 많은 이점을 얻을 수 있었습니다.</p><h3 id="🧑‍🔬-체계적인-실험"><a href="#🧑‍🔬-체계적인-실험" class="headerlink" title="🧑‍🔬 체계적인 실험"></a>🧑‍🔬 체계적인 실험</h3><p><img src="/images/notion_kanban.png"></p><p>저희는 Kanban board를 사용하여 실험을 관리하였습니다.</p><p align="center">    <img src="/images/exp_tag.png" height="30%" width="30%"></p><p>Backlog, TO-DO, In progress, Completed 네 단계로 나누어 서로가 어떤 실험을 진행하고 있는지, 어떤 실험을 해야하는지 파악할 수 있게 하였으며 실험이 끝날 때마다 그때그때 갱신하는 작업을 진행하였습니다.</p><p>각각의 카테고리 별 책임은 이렇습니다.</p><ul><li>Backlog: 단순 실험 아이디어 및 건의사항, 수정사항</li><li>TO-DO: 꼭 적용해봐야 하는 실험</li><li>In progress: 현재 진행중인 실험</li><li>Completed: 완료된 실험</li></ul><p>Backlog에 진행해보고 싶은 실험카드가 생겼거나, 다른 실험 아이디어가 생긴 경우에는 Notion의 <code>Comment</code> 기능을 이용하여 이미 진행중인 실험이면 해당 실험의 진행상황이나 주의사항들을 더 자세하게 공유할 수 있게 하였습니다.</p><p align="center">    <img src="/images/notion_comm2.png">    <img src="/images/notion_comm.png"></p><p>실험카드가 <code>Completed</code>로 이동하게 되면 아래와 같이 실험기록표에 결과를 작성하고, 실험의 성공여부와 관계없이 그 실험에 대한 평가와 그런 결과가 나온 이유 혹은 주의사항 등을 기록하게 하였습니다.</p><p align="center">    <img src="/images/exp1.png">    <img src="/images/notion_exp.png"></p><span class="linear_highlight">이러한 시도는 팀원들 간 기술부채를 최대한 줄어들게 하였고 실패한 실험을 반복적으로 하지 않을 수 있게 하여 효율적으로 실험을 계획할 수 있게 해주었습니다.</span><h2 id="🛋-Data-Experiments"><a href="#🛋-Data-Experiments" class="headerlink" title="🛋 Data Experiments"></a>🛋 Data Experiments</h2><h3 id="👁-Data-EDA"><a href="#👁-Data-EDA" class="headerlink" title="👁 Data EDA"></a>👁 Data EDA</h3><p><img src="/images/eda.png"></p><p>데이터는 위와 같이 매우 불균형하게 분포되어 있었고, 9,000개가 넘는 no_relation과 달리 <code>per:place_of_death</code>처럼 약 40개 정도만 존재하는 label도 있었습니다.</p><p>이렇게 극단적인 Data Imbalancing을 잘 잡는 것이 이번 대회의 핵심이라고 생각하게 되었습니다.</p><p><img src="/images/dup.png" alt="김채은 캠퍼님의 토론게시판 글 중"></p><p>또한 sentence와 subject_entity, object_entity까지 전부 동일한 문장이 53개가 있는 등 중복된 데이터와 mislabeled 데이터들이 존재하였고, 이것들을 전부 제거하고 수정하여 데이터셋을 재구성하였습니다.</p><h3 id="💪-Data-Augmentation"><a href="#💪-Data-Augmentation" class="headerlink" title="💪 Data Augmentation"></a>💪 Data Augmentation</h3><p>이후에는 Data Imbalancing을 해결하기 위하여 여러 Augmentation 기법들로 실험을 이어나갔습니다.</p><ol><li><p><a href="https://github.com/toriving/KoEDA">EDA &amp; AEDA</a><br> KoEDA 라이브러리를 사용하여 EDA, AEDA 각각 전체 데이터셋에 대해 <code>n_aug=[1, 2, 4]</code> 비율로 augmentation 진행</p><ul><li>두 방법 모두 아무것도 하지 않았을 때보다 validation score가 낮았음.</li></ul></li><li><p><a href="https://imbalanced-learn.org/stable/">Undersampling &amp; Oversampling</a><br> imblearn 라이브러리를 사용하여 SMOTE로 Sampling 진행<br> <img src="/images/under.png" alt="Undersampling"></p><p> <img src="/images/over.png" alt="Oversampling"></p><ul><li>두 방법 모두 아무것도 하지 않았을 때보다 validation score가 낮았음.</li></ul></li><li><p>Back Translation<br> Crawler를 사용하여 papago 번역기 사용.<br> <code>klue/roberta-small</code> 모델 기준으로 score 상승이 있었지만 너무 늦게 시도해서 best 모델에 적용하지 못했음.</p><ul><li><code>ko -&gt; en -&gt; ja -&gt; ko</code>: 약 0.1 LB Score 하락</li><li><code>ko -&gt; ja -&gt; ko</code>: 약 0.5 LB Score 상승</li></ul></li><li><p>Target Augmentation subject &lt;-&gt; object label changing<br> kfold로 학습을 진행할 때 한 번이라도 틀린 data에 대해서 subject와 object entity를 변경함으로써 augmentation 진행</p><ul><li>약 0.05 LB Score 상승</li></ul></li></ol><p>하지만 이러한 augmentation 기법들에 대해서 많은 효과를 볼 수가 없었는데, <code>confusion matrix</code>를 통해 원인을 유추해볼 수 있었습니다.</p><p><img src="/images/conf.png"></p><p>처음 예상과 다르게 적은 label의 데이터를 생각보다 잘 맞추고 있었고, <em>오히려 데이터 수가 가장 많았던 <code>no_relation</code> 예측에서 많이 틀리고 있었기 때문</em> 이었습니다. 따라서 전체 데이터셋에 대해서 augmentation을 진행한 방식, 그리고 sampling 방식으로는 효과를 보지 못했다는 것을 알 수 있었습니다.</p><p>처음부터 이렇게 Confusion Matrix를 도입하여 현재 모델이 어떤 예측을 잘 수행하지 못하는지 등을 파악하여, <code>no_relation</code>에 대해서만 augmentation을 시도하는 등 디테일하게 augmentation 전략을 세웠으면 좋았을 것이란 아쉬움이 남습니다. 또한 4번 실험에서 <code>sub &lt;-&gt; obj</code> label만 변경하는 방식 말고 다른 augmentation 방법도 써봤으면 어땠을까 하는 아쉬움이 남습니다.</p><h3 id="🔧-Data-Preprocessing-amp-Tokenizer"><a href="#🔧-Data-Preprocessing-amp-Tokenizer" class="headerlink" title="🔧 Data Preprocessing &amp; Tokenizer"></a>🔧 Data Preprocessing &amp; Tokenizer</h3><ol><li><p>Dynamic Padding<br> Huggingface의 Tokenizer는 <code>max_length</code> 인자를 통해 기본적으로 <code>fixed padding</code> 방식을 사용합니다. 저희는 더 빠른 실험을 통해 <code>dynamic padding</code> 방식으로 변경하였고 그 결과 약 30%의 속도를 향상시킬 수 있었습니다.</p><p> <img src="/images/fixed.png" alt="fixed padding"></p><p> <img src="/images/dynamic.png" alt="dynamic padding"></p></li><li><p><a href="https://arxiv.org/pdf/2102.01373.pdf">An Improved Baseline for Sentence-level Relation Extraction</a><br> 문장의 Subject, Object Entity의 NER Type을 명시해주고, Entity의 위치를 사전학습에서 사용된 특수문자를 이용하여 표기하는 Typed Entity Marker를 적용했습니다.</p><p> <img src="/images/token.png"></p><ol><li>vanilla : 기본 베이스라인 input</li><li>special_ent : <code>기본 베이스라인 input + [sbj][sbj/] + [obj][obj/]</code></li><li>special_ent_without_prefix : 기본 베이스라인 input의 앞에있는 <code>subject [sep] object [sep] 부분을 제거</code>하고 <code>special token</code>을 사용 (4, 5번 역시 prefix를 제거함) </li><li>punct_ent : <code>@sbj@ #obj#</code> 식으로 special token 없이 entity 표현 </li><li>punct_typing_ent : <code>@*sbj_type*sbj@ #^obj_type^obj#</code> 식으로 entity type을 알려주며 표현결과: 3번 5번이 비교적 가장 우수한 성능을 보임, 동일 조건 하 validation f1 기준 1 정도의 성능 차이를 보임.</li></ol></li></ol><p>위의 두 방법을 사용하여 실험과 검증은 조금 더 빠르게 진행할 수 있었고, 성능 향상을 이끌어낼 수 있었습니다.</p><h2 id="🧘-Modeling"><a href="#🧘-Modeling" class="headerlink" title="🧘 Modeling"></a>🧘 Modeling</h2><p>Backbone이 되는 Model은 <code>klue/roberta-large</code>를 사용하였으며 Base 성능은 <code>avg. 71 (micro f1)</code> 정도를 기록하였습니다.</p><p><em><strong>여러가지 실험들</strong></em></p><ul><li><p>Entity Embedding<br>  아무것도 하지 않았을 때보다 validation score가 낮았음.</p><details>  <summary>  코드 보기  </summary>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RobertaEmbeddingsWithTokenEmbedding</span>(<span class="hljs-params">nn.Module</span>):</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">edit by 곽진성_T2011</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, model, config, pre_model_state_dict=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.word_embeddings = model.roberta.embeddings.word_embeddings<br>    self.position_embeddings = model.roberta.embeddings.position_embeddings<br>    self.token_type_embeddings = model.roberta.embeddings.token_type_embeddings<br><br>    self.entity_embeddings = nn.Embedding(<span class="hljs-number">9</span>, config.hidden_size, padding_idx=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">if</span> pre_model_state_dict:<br>        pre_weight = pre_model_state_dict[<span class="hljs-string">&#x27;roberta.embeddings.entity_embeddings.weight&#x27;</span>]<br>        self.entity_embeddings.weight = torch.nn.parameter.Parameter(pre_weight, requires_grad=<span class="hljs-literal">True</span>)<br><br>    self.LayerNorm = model.roberta.embeddings.LayerNorm<br>    self.dropout = model.roberta.embeddings.dropout<br>    self.position_embedding_type = <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">&quot;position_embedding_type&quot;</span>, <span class="hljs-string">&quot;absolute&quot;</span>)<br>    self.register_buffer(<span class="hljs-string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="hljs-string">&quot;1.6.0&quot;</span>):<br>        self.register_buffer(<br>            <span class="hljs-string">&quot;token_type_ids&quot;</span>,<br>            torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),<br>            persistent=<span class="hljs-literal">False</span>,<br>        )<br>    self.padding_idx = config.pad_token_id<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params"></span></span><br><span class="hljs-params"><span class="hljs-function">    self, input_ids=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, position_ids=<span class="hljs-literal">None</span>, inputs_embeds=<span class="hljs-literal">None</span>, past_key_values_length=<span class="hljs-number">0</span></span></span><br><span class="hljs-params"><span class="hljs-function"></span>):</span><br>    <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            position_ids = self.create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)<br>        <span class="hljs-keyword">else</span>:<br>            position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)<br><br>    <span class="hljs-keyword">if</span> input_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        input_shape = input_ids.size()<br>    <span class="hljs-keyword">else</span>:<br>        input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br><br>    seq_length = input_shape[<span class="hljs-number">1</span>]<br><br>    <span class="hljs-keyword">if</span> token_type_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&quot;token_type_ids&quot;</span>):<br>            buffered_token_type_ids = self.token_type_ids[:, :seq_length]<br>            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[<span class="hljs-number">0</span>], seq_length)<br>            token_type_ids = buffered_token_type_ids_expanded<br>        <span class="hljs-keyword">else</span>:<br>            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)<br><br>    <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        inputs_embeds = self.word_embeddings(input_ids)<br>    token_type_embeddings = self.token_type_embeddings(token_type_ids)<br><br>    entity_ids = self.create_entity_ids_from_input_ids(input_ids)<br>    entity_embeddings = self.entity_embeddings(entity_ids)<br><br>    embeddings = inputs_embeds + token_type_embeddings<br>    <span class="hljs-keyword">if</span> self.position_embedding_type == <span class="hljs-string">&quot;absolute&quot;</span>:<br>        position_embeddings = self.position_embeddings(position_ids)<br>        embeddings += position_embeddings<br>    <br>    embeddings += entity_embeddings<br>    <br>    embeddings = self.LayerNorm(embeddings)<br>    embeddings = self.dropout(embeddings)<br>    <span class="hljs-keyword">return</span> embeddings<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_position_ids_from_inputs_embeds</span>(<span class="hljs-params">self, inputs_embeds</span>):</span><br>    input_shape = inputs_embeds.size()[:-<span class="hljs-number">1</span>]<br>    sequence_length = input_shape[<span class="hljs-number">1</span>]<br><br>    position_ids = torch.arange(<br>        self.padding_idx + <span class="hljs-number">1</span>, sequence_length + self.padding_idx + <span class="hljs-number">1</span>, dtype=torch.long, device=inputs_embeds.device<br>    )<br>    <span class="hljs-keyword">return</span> position_ids.unsqueeze(<span class="hljs-number">0</span>).expand(input_shape)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_entity_ids_from_input_ids</span>(<span class="hljs-params">self, input_ids</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    map index 1~8 to the token that is related to sbj, obj entities</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    s_ids = torch.nonzero((input_ids == <span class="hljs-number">36</span>)) <span class="hljs-comment"># subject</span><br>    o_ids = torch.nonzero((input_ids == <span class="hljs-number">7</span>)) <span class="hljs-comment"># object</span><br>    <span class="hljs-comment"># entity type mapped into index 3 ~ 8</span><br>    type_map = &#123;<span class="hljs-number">4410</span> : <span class="hljs-number">3</span>, <span class="hljs-number">7119</span> : <span class="hljs-number">4</span>, <span class="hljs-number">3860</span> : <span class="hljs-number">5</span>, <span class="hljs-number">5867</span> : <span class="hljs-number">6</span>, <span class="hljs-number">12395</span> : <span class="hljs-number">7</span>, <span class="hljs-number">9384</span> : <span class="hljs-number">8</span>&#125;<br><br>    entity_ids = torch.zeros_like(input_ids)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(s_ids)):<br>        s_id = s_ids[i]<br>        o_id = o_ids[i]<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>            entity_ids[s_id[<span class="hljs-number">0</span>], s_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>] = type_map[input_ids[s_id[<span class="hljs-number">0</span>], s_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>].item()]<br>            entity_ids[o_id[<span class="hljs-number">0</span>], o_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>] = type_map[input_ids[o_id[<span class="hljs-number">0</span>], o_id[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>].item()]<br>        <span class="hljs-keyword">else</span>:<br>            prev_s_id = s_ids[i-<span class="hljs-number">1</span>]<br>            prev_o_id = o_ids[i-<span class="hljs-number">1</span>]<br>            entity_ids[s_id[<span class="hljs-number">0</span>], prev_s_id[<span class="hljs-number">1</span>]+<span class="hljs-number">4</span>:s_id[<span class="hljs-number">1</span>]] = <span class="hljs-number">1</span><br>            entity_ids[o_id[<span class="hljs-number">0</span>], prev_o_id[<span class="hljs-number">1</span>]+<span class="hljs-number">4</span>:o_id[<span class="hljs-number">1</span>]] = <span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> entity_ids<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_position_ids_from_input_ids</span>(<span class="hljs-params">self, input_ids, padding_idx, past_key_values_length=<span class="hljs-number">0</span></span>):</span><br>    mask = input_ids.ne(padding_idx).<span class="hljs-built_in">int</span>()<br>    incremental_indices = (torch.cumsum(mask, dim=<span class="hljs-number">1</span>).type_as(mask) + past_key_values_length) * mask<br>    <span class="hljs-keyword">return</span> incremental_indices.long() + padding_idx<br></code></pre></td></tr></table></figure></details></li><li><p><a href="https://github.com/monologg/R-BERT">R-BERT</a><br>  본 구조에서 BERT를 RoBERTa로 변경. LB 기준 71.362의 micro f1 score 달성.</p><details>  <summary>  코드 보기  </summary>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RBERT</span>(<span class="hljs-params">RobertaPreTrainedModel</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    orgin code: https://github.com/monologg/R-BERT</span><br><span class="hljs-string">    edit by 문하겸_T2076</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, config, model_name</span>):</span><br>        <span class="hljs-built_in">super</span>(RBERT, self).__init__(config)<br>        self.roberta = RobertaModel.from_pretrained(<br>            model_name)  <span class="hljs-comment"># Load pretrained bert</span><br><br>        self.num_labels = config.num_labels<br><br>        self.cls_fc_layer = FCLayer(<br>            config.hidden_size, config.hidden_size, <span class="hljs-number">0.1</span>)<br>        self.entity_fc_layer = FCLayer(<br>            config.hidden_size, config.hidden_size, <span class="hljs-number">0.1</span>)<br>        self.label_classifier = FCLayer(<br>            config.hidden_size * <span class="hljs-number">3</span>,<br>            config.num_labels,<br>            <span class="hljs-number">0.1</span>,<br>            use_activation=<span class="hljs-literal">False</span>,<br>        )<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">entity_average</span>(<span class="hljs-params">hidden_output, e_mask</span>):</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Average the entity hidden state vectors (H_i ~ H_j)</span><br><span class="hljs-string">        :param hidden_output: [batch_size, j-i+1, dim]</span><br><span class="hljs-string">        :param e_mask: [batch_size, max_seq_len]</span><br><span class="hljs-string">                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]</span><br><span class="hljs-string">        :return: [batch_size, dim]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        e_mask_unsqueeze = e_mask.unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [b, 1, j-i+1]</span><br>        length_tensor = (e_mask != <span class="hljs-number">0</span>).<span class="hljs-built_in">sum</span>(<br>            dim=<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># [batch_size, 1]</span><br><br>        <span class="hljs-comment"># [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -&gt; [b, dim]</span><br>        sum_vector = torch.bmm(e_mask_unsqueeze.<span class="hljs-built_in">float</span>(),<br>                               hidden_output).squeeze(<span class="hljs-number">1</span>)<br>        avg_vector = sum_vector.<span class="hljs-built_in">float</span>() / length_tensor.<span class="hljs-built_in">float</span>()  <span class="hljs-comment"># broadcasting</span><br>        <span class="hljs-keyword">return</span> avg_vector<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids=<span class="hljs-literal">None</span>, attention_mask=<span class="hljs-literal">None</span>, token_type_ids=<span class="hljs-literal">None</span>, labels=<span class="hljs-literal">None</span>, e1_mask=<span class="hljs-literal">None</span>, e2_mask=<span class="hljs-literal">None</span></span>):</span><br>        outputs = self.roberta(<br>            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)<br>        sequence_output = outputs[<span class="hljs-number">0</span>]<br>        pooled_output = outputs[<span class="hljs-number">1</span>]<br><br>        <span class="hljs-comment"># Average</span><br>        e1_h = self.entity_average(sequence_output, e1_mask)<br>        e2_h = self.entity_average(sequence_output, e2_mask)<br><br>        <span class="hljs-comment"># Dropout -&gt; tanh -&gt; fc_layer (Share FC layer for e1 and e2)</span><br>        pooled_output = self.cls_fc_layer(pooled_output)<br>        e1_h = self.entity_fc_layer(e1_h)<br>        e2_h = self.entity_fc_layer(e2_h)<br><br>        <span class="hljs-comment"># Concat -&gt; fc_layer</span><br>        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-<span class="hljs-number">1</span>)<br>        logits = self.label_classifier(concat_h)<br><br>        <span class="hljs-comment"># add hidden states and attention if they are here</span><br>        outputs = (logits,) + outputs[<span class="hljs-number">2</span>:]<br><br>        <span class="hljs-comment"># Softmax</span><br>        <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.num_labels == <span class="hljs-number">1</span>:<br>                loss_fct = nn.MSELoss()<br>                loss = loss_fct(logits.view(-<span class="hljs-number">1</span>), labels.view(-<span class="hljs-number">1</span>))<br>            <span class="hljs-keyword">else</span>:<br>                loss_type = <span class="hljs-string">&quot;focal&quot;</span><br>                beta = <span class="hljs-number">0.9999</span><br>                gamma = <span class="hljs-number">2.0</span><br><br>                loss_fct = CB_loss(beta=beta, gamma=gamma)<br>                loss = loss_fct(logits.view(-<span class="hljs-number">1</span>, self.num_labels),<br>                                labels.view(-<span class="hljs-number">1</span>), loss_type)<br><br>            outputs = (loss,) + outputs<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure></details></li><li><p>Model split &amp; combine<br>  <code>no_relation</code>만 구분하도록 학습시킨 모델, <code>relation</code>만 구분하도록 학습시킨 모델, <code>전체 데이터로 학습시킨 모델</code> 세 가지 <code>klue/roberta-large</code> 모델의 가중치를 freezing 하고 classifier만 학습시킨 것. LB 기준 73.251의 micro f1 score 달성.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CombineModels</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 이요한_T2166</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(CombineModels, self).__init__()<br><br>        c1 = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>, num_labels=<span class="hljs-number">2</span>)<br>        c2 = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>, num_labels=<span class="hljs-number">29</span>)<br>        c3 = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>, num_labels=<span class="hljs-number">30</span>)<br><br>        self.roberta1 = AutoModelForSequenceClassification.from_pretrained(<br>            <span class="hljs-string">&quot;split_model_no_rel_large&quot;</span>, config=c1)<br>        self.roberta2 = AutoModelForSequenceClassification.from_pretrained(<br>            <span class="hljs-string">&quot;split_model_rel_large&quot;</span>, config=c2)<br>        self.roberta3 = AutoModelForSequenceClassification.from_pretrained(<br>            <span class="hljs-string">&quot;sota_kfold&quot;</span>, config=c3)<br><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.roberta1.parameters():<br>            p.requires_grad = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.roberta2.parameters():<br>            p.requires_grad = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.roberta3.parameters():<br>            p.requires_grad = <span class="hljs-literal">False</span><br><br>        self.fc1 = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">768</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">29</span>, <span class="hljs-number">768</span>)<br>        self.fc3 = nn.Linear(<span class="hljs-number">30</span>, <span class="hljs-number">768</span>)<br><br>        self.classifier = nn.Sequential(<br>            nn.Dropout(p=<span class="hljs-number">0.1</span>),<br>            nn.Linear(<span class="hljs-number">768</span> * <span class="hljs-number">15</span>, <span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>),<br>            nn.Tanh(),<br>            nn.Dropout(p=<span class="hljs-number">0.1</span>),<br>            nn.Linear(<span class="hljs-number">768</span>, <span class="hljs-number">30</span>, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):</span><br>        logits_1 = self.roberta1(<br>            input_ids.clone(), attention_mask=attention_mask).get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br>        logits_2 = self.roberta2(<br>            input_ids.clone(), attention_mask=attention_mask).get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br>        logits_3 = self.roberta3(<br>            input_ids.clone(), attention_mask=attention_mask).get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br><br>        logits_1 = self.fc1(logits_1)<br>        logits_2 = self.fc2(logits_2)<br>        logits_3 = self.fc1(logits_3)<br><br>        concatenated_vectors = torch.cat((<br>            logits_1, logits_2, logits_3), dim=-<span class="hljs-number">1</span>)<br><br>        output = self.classifier(concatenated_vectors)<br>        outputs = SequenceClassifierOutput(logits=output)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>  </details></li><li><p>FC Layer -&gt; LSTM<br>  너무 늦게 도입하여 제출 실패.</p><details>  <summary>  코드 보기  </summary>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RobertaAddLSTM</span>(<span class="hljs-params">RobertaPreTrainedModel</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 정희영_T2210</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, config, *args, **kwargs</span>):</span><br>            <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>            self.bert = RobertaModel.from_pretrained(<span class="hljs-string">&quot;klue/roberta-large&quot;</span>)<br><br>            self.lstm = nn.LSTM(<span class="hljs-number">1024</span>, <span class="hljs-number">256</span>, batch_first=<span class="hljs-literal">True</span>, bidirectional=<span class="hljs-literal">True</span>)<br>            self.linear = nn.Linear(<span class="hljs-number">256</span>*<span class="hljs-number">2</span>, <span class="hljs-number">30</span>)<br>            self.dropout = nn.Dropout(<span class="hljs-number">0.5</span>)<br>            self.tanh = nn.Tanh()<br>            self.linear2 = nn.Linear(<span class="hljs-number">30</span>, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, input_ids, attention_mask</span>):</span><br>            output = self.bert(input_ids, attention_mask=attention_mask)<br><br>            lstm_output, (h,c) = self.lstm(output[<span class="hljs-number">0</span>]) <span class="hljs-comment">## extract the 1st token&#x27;s embeddings</span><br>            hidden = torch.cat((lstm_output[:,-<span class="hljs-number">1</span>, :<span class="hljs-number">256</span>],lstm_output[:,<span class="hljs-number">0</span>, <span class="hljs-number">256</span>:]),dim=-<span class="hljs-number">1</span>)<br>            linear_output = self.linear(hidden.view(-<span class="hljs-number">1</span>,<span class="hljs-number">256</span>*<span class="hljs-number">2</span>))<br>            x = self.tanh(linear_output)<br>            x = self.dropout(x)<br>            outputs = SequenceClassifierOutput(logits=x)<br><br>            <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure></details></li><li><p>TAPT - <a href="https://arxiv.org/pdf/2004.10964.pdf">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a><br>  주어진 학습데이터로 사전학습된 모델에 TAPT 를 적용해보았을 때 약 0.5 정도의 validation f1 score 향상이 있었으나, 시간문제로 논문에서 제안된 epochs만큼 학습을 진행하지 못했음. koelectra와 roberta-base로 리더보드에 제출해본 결과 큰 성능향상이 없었기 때문에 large 모델에 적용해볼 수 없었음.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">edit by 정진원_T2206</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, RobertaForMaskedLM, ElectraForMaskedLM, BertForMaskedLM, AutoConfig, DataCollatorWithPadding, DataCollatorForLanguageModeling<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LineByLineTextDataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer, TrainingArguments<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EarlyStoppingCallback<br><br><span class="hljs-comment"># fetch pretrained model for MaskedLM training </span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>)<br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br>model = BertForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;klue/roberta-large&#x27;</span>)<br>model.to(device)<br><br><span class="hljs-comment"># Read txt file which is consisted of sentences from train.csv</span><br>dataset = LineByLineTextDataset(<br>    tokenizer=tokenizer,<br>    file_path=<span class="hljs-string">&#x27;data/train.txt&#x27;</span>,<br>    block_size=<span class="hljs-number">514</span> <span class="hljs-comment"># block size needs to be modified to max_position_embeddings</span><br>)<br><br>data_collator = DataCollatorForLanguageModeling( <br>    tokenizer=tokenizer, mlm=<span class="hljs-literal">True</span>, mlm_probability=<span class="hljs-number">0.2</span> <br>)<br><br><span class="hljs-comment"># need to change arguments </span><br>training_args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&quot;./klue-roberta-retrained&quot;</span>,<br>    overwrite_output_dir=<span class="hljs-literal">True</span>,<br>    learning_rate=<span class="hljs-number">5e-05</span>,<br>    num_train_epochs=<span class="hljs-number">200</span>, <br>    per_device_train_batch_size=<span class="hljs-number">16</span>,<br>    save_steps=<span class="hljs-number">100</span>,<br>    save_total_limit=<span class="hljs-number">2</span>,<br>    seed=<span class="hljs-number">30</span>,<br>    save_strategy=<span class="hljs-string">&#x27;epoch&#x27;</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">8</span>,<br>    logging_steps=<span class="hljs-number">100</span>,<br>    evaluation_strategy=<span class="hljs-string">&#x27;epoch&#x27;</span>,<br>    resume_from_checkpoint=<span class="hljs-literal">True</span>,<br>    fp16=<span class="hljs-literal">True</span>,<br>    fp16_opt_level=<span class="hljs-string">&#x27;O1&#x27;</span>,<br>    load_best_model_at_end=<span class="hljs-literal">True</span><br>) <br><br>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    data_collator=data_collator,<br>    train_dataset=dataset,<br>    eval_dataset=dataset,<br>    callbacks = [EarlyStoppingCallback(early_stopping_patience=<span class="hljs-number">3</span>)]<br>)<br><br>trainer.train()<br>trainer.save_model(<span class="hljs-string">&quot;./klue-roberta-retrained&quot;</span>)<br></code></pre></td></tr></table></figure>  </details></li></ul><p><em><strong>loss function</strong></em></p><ul><li><p>CB Loss<br>  R-BERT를 제외하고는 큰 성능향상을 못봤음.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyTrainer</span>(<span class="hljs-params">Trainer</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 문하겸_T2076</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, disable_wandb=<span class="hljs-literal">True</span>, *args, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        self.disable_wandb = disable_wandb<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss</span>(<span class="hljs-params">self, model, inputs, return_outputs=<span class="hljs-literal">False</span></span>):</span><br>        labels = inputs.get(<span class="hljs-string">&quot;labels&quot;</span>)<br>        outputs = model(**inputs)<br>        logits = outputs.get(<span class="hljs-string">&quot;logits&quot;</span>)<br>        beta = <span class="hljs-number">0.9999</span><br>        gamma = <span class="hljs-number">2.0</span><br><br>        criterion = CB_loss(beta, gamma)<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            criterion.cuda()<br>        loss_fct = criterion(logits, labels)<br><br>        <span class="hljs-keyword">return</span> (loss_fct, outputs) <span class="hljs-keyword">if</span> return_outputs <span class="hljs-keyword">else</span> loss_fct<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluation_loop</span>(<span class="hljs-params">self, *args, **kwargs</span>):</span><br>        eval_loop_output = <span class="hljs-built_in">super</span>().evaluation_loop(*args, **kwargs)<br><br>        pred = eval_loop_output.predictions<br>        label_ids = eval_loop_output.label_ids<br><br>        self.draw_confusion_matrix(pred, label_ids)<br>        <span class="hljs-keyword">return</span> eval_loop_output<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CB_loss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, beta, gamma, epsilon=<span class="hljs-number">0.1</span></span>):</span><br>        <span class="hljs-built_in">super</span>(CB_loss, self).__init__()<br>        self.beta = beta<br>        self.gamma = gamma<br>        self.epsilon = epsilon<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, logits, labels</span>):</span><br>        <span class="hljs-comment"># self.epsilon = 0.1 #labelsmooth</span><br>        beta = self.beta<br>        gamma = self.gamma<br><br>        no_of_classes = logits.shape[<span class="hljs-number">1</span>]<br>        samples_per_cls = torch.Tensor(<br>            [<span class="hljs-built_in">sum</span>(labels == i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(logits.shape[<span class="hljs-number">1</span>])])<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            samples_per_cls = samples_per_cls.cuda()<br><br>        effective_num = <span class="hljs-number">1.0</span> - torch.<span class="hljs-built_in">pow</span>(beta, samples_per_cls)<br>        weights = (<span class="hljs-number">1.0</span> - beta) / ((effective_num) + <span class="hljs-number">1e-8</span>)<br><br>        weights = weights / torch.<span class="hljs-built_in">sum</span>(weights) * no_of_classes<br>        labels = labels.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>        weights = torch.tensor(weights.clone().detach()).<span class="hljs-built_in">float</span>()<br><br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            weights = weights.cuda()<br>            labels_one_hot = torch.zeros(<br>                <span class="hljs-built_in">len</span>(labels), no_of_classes).cuda().scatter_(<span class="hljs-number">1</span>, labels, <span class="hljs-number">1</span>).cuda()<br><br>        labels_one_hot = (<span class="hljs-number">1</span> - self.epsilon) * labels_one_hot + \<br>            self.epsilon / no_of_classes<br>        weights = weights.unsqueeze(<span class="hljs-number">0</span>)<br>        weights = weights.repeat(labels_one_hot.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>) * labels_one_hot<br>        weights = weights.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br>        weights = weights.unsqueeze(<span class="hljs-number">1</span>)<br>        weights = weights.repeat(<span class="hljs-number">1</span>, no_of_classes)<br><br>        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)<br>        <span class="hljs-keyword">return</span> cb_loss<br></code></pre></td></tr></table></figure>  </details></li><li><p><a href="https://arxiv.org/pdf/1906.07413.pdf">LDAM</a></p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LDAMLossTrainer</span>(<span class="hljs-params">Trainer</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    edit by 김민수_T2025</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, *args, **kwargs</span>):</span><br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br>        self.n_per_labels = self.train_dataset.get_n_per_labels()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_loss</span>(<span class="hljs-params">self, model, inputs, return_outputs=<span class="hljs-literal">False</span></span>):</span><br>        labels = inputs.get(<span class="hljs-string">&#x27;labels&#x27;</span>)<br>        outputs = model(**inputs)<br>        logits = outputs.get(<span class="hljs-string">&#x27;logits&#x27;</span>)<br><br>        betas = [<span class="hljs-number">0</span>, <span class="hljs-number">0.99</span>]<br>        beta_idx = self.state.epoch &gt;= <span class="hljs-number">2</span><br>        n_per_labels = self.n_per_labels<br><br>        effective_num = <span class="hljs-number">1.0</span> - np.power(betas[beta_idx], n_per_labels)<br>        cls_weights = (<span class="hljs-number">1.0</span> - betas[beta_idx]) / np.array(effective_num)<br>        cls_weights = cls_weights / np.<span class="hljs-built_in">sum</span>(cls_weights) * <span class="hljs-built_in">len</span>(n_per_labels)<br>        cls_weights = torch.FloatTensor(cls_weights)<br><br>        criterion = LDAMLoss(cls_num_list=n_per_labels, max_m=<span class="hljs-number">0.5</span>, s=<span class="hljs-number">30</span>, weight=cls_weights)<br>        <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>            criterion.cuda()<br><br>        loss_fct = criterion(logits, labels)<br>        <span class="hljs-keyword">return</span> (loss_fct, outputs) <span class="hljs-keyword">if</span> return_outputs <span class="hljs-keyword">else</span> loss_fct<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LDAMLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, cls_num_list, max_m=<span class="hljs-number">0.5</span>, weight=<span class="hljs-literal">None</span>, s=<span class="hljs-number">30</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        m_list = <span class="hljs-number">1.0</span> / np.sqrt(np.sqrt(cls_num_list))<br>        m_list = m_list * (max_m / np.<span class="hljs-built_in">max</span>(m_list))<br>        m_list = torch.cuda.FloatTensor(m_list)<br>        self.m_list = m_list<br>        <span class="hljs-keyword">assert</span> s &gt; <span class="hljs-number">0</span><br>        self.s = s<br>        self.weight = weight<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, target</span>):</span><br>        index = torch.zeros_like(x, dtype=torch.<span class="hljs-built_in">bool</span>)<br>        index.scatter_(<span class="hljs-number">1</span>, target.data.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), <span class="hljs-number">1</span>)<br><br>        index_float = index.<span class="hljs-built_in">type</span>(torch.cuda.FloatTensor)<br>        batch_m = torch.matmul(self.m_list[<span class="hljs-literal">None</span>, :], index_float.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>        batch_m = batch_m.view((-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        x_m = x - batch_m<br><br>        output = torch.where(index, x_m, x)<br>        <span class="hljs-keyword">return</span> F.cross_entropy(self.s * output.to(<span class="hljs-string">&#x27;cuda&#x27;</span>), target.to(<span class="hljs-string">&#x27;cuda&#x27;</span>), weight=self.weight.to(<span class="hljs-string">&#x27;cuda&#x27;</span>))<br></code></pre></td></tr></table></figure>  </details></li></ul><p>결과적으론 backbone 모델의 kfold score를 넘어서지 못했습니다. 그 이유로는 public leaderboard와 간극이 적은 적절한 validation dataset을 구성하지 못했기 때문입니다. </p><p>차선책으로 미리 stratified하게 0.1 비율로 split한 train, validation dataset을 고정시켜놓고 사용하였지만, 해당 데이터셋의 validation score는 public leaderboard와 <em><strong>평균적으로 15점 이상, 극단적인 경우 30점까지 차이가 존재</strong></em> 했습니다. 이러한 validation dataset으로만 실험을 했기 때문에 validation score를 신뢰하기 어려웠고, model이 좋은지 나쁜지 직접 제출해보기 전에는 알 수 없었습니다.</p><p>따라서 제출횟수가 한정적이므로 대부분 위의 validation score로만 검증을 했고, 성능향상 가능성이 없다고 판단해 추가적인 실험을 진행하지 못했습니다. 하지만 막상 1, 2등의 solution을 보니 저희가 진행했던 실험들로 점수를 올렸기에 속이 쓰렸습니다… :(</p><p>다음 대회에서는 public leaderboard와의 간극이 적은 적절한 validation dataset을 구성할 필요가 있으며, 또한 교차검증을 위한 validation dataset 역시 고정시켜놓을 필요가 있다고 느꼈습니다.</p><h2 id="🍜-Ensemble"><a href="#🍜-Ensemble" class="headerlink" title="🍜 Ensemble"></a>🍜 Ensemble</h2><ul><li><p>K-fold</p><p>  <code>klue/roberta-large</code> 모델을 사용하여 kfold(k=5)를 사용하여 성능을 개선하였습니다. single fold 기준 public leaderboard에서 약 71의 micro f1 score를 기록하였고, 5 fold ensemble을 통해 public leaderboard에서 73.5의 micro f1 score를 기록할 수 있었습니다.</p></li></ul><h3 id="👨‍🎨-앙상블-깎는-노인과-기도메타"><a href="#👨‍🎨-앙상블-깎는-노인과-기도메타" class="headerlink" title="👨‍🎨 앙상블 깎는 노인과 기도메타"></a>👨‍🎨 앙상블 깎는 노인과 기도메타</h3><p><img src="/images/soft_voting.jpg" alt="soft voting"></p><p>저희는 public leaderboard 기준 73 정도의 micro f1 score를 기록하였지만 예측 분포가 다른 결과들을 <code>soft voting</code>하여 public leaderboard 기준 74.306의 micro f1 score를 기록할 수 있었습니다.</p><p align="center">    <img src="/images/기도1.png" style="display: inline" height="100px">    <img src="/images/기도2.png" style="display: inline" height="100px">    <img src="/images/기도3.png" style="display: inline" height="100px">    <img src="/images/기도4.png" style="display: inline" height="100px"></p><p align="center">    <img src="/images/기도5.png" height="100px"></p><p>    최종적으로 제출된 결과는 Ensemble된 것들 중 public leaderboard 기준 AUPRC가 가장 높은 결과이며, private leaderboard 에서 다른 팀 대비 점수 하락폭이 적어서     <span class="linear_highlight">    순위가 9등 -> 5등으로 상승했습니다.    </span></p><p>결과적으로 모델링 실험에서 압도적인 성능 향상을 이루진 못했지만, 비슷한 점수의 다른 분포를 가지는 결과들을 많이 만들어놓았던 것이 앙상블에서 더욱 일반화된 결과를 얻을 수 있었던 요인이었다고 생각합니다.</p><p><img src="/images/final.png"></p><h2 id="💯-Good-Practice"><a href="#💯-Good-Practice" class="headerlink" title="💯 Good Practice"></a>💯 Good Practice</h2><p>저희 팀만의 Good Practice는 체계적으로 Notion에 실험관리를 한 것도 있지만, <code>huggingface</code>의 다양한 기능들을 사용해봤다는 것이고 그 중에서 좋은 효과를 낸 것으로는 <code>fp16</code>과 <code>hyperparameter_search</code>가 있습니다.</p><ul><li><p>fp16</p><p>  <img src="/images/fp16.png"><br>  fp16은 <code>Mixed-Precision Training</code>으로 32-bit Floating Point가 아닌 16-bit Floating Point를 사용하는 방식입니다. 이 방식을 통해 모델을 학습시킬 때 성능은 비슷하지만 약 60% 가량의 향상된 속도로 학습을 진행할 수 있었습니다.<br>  <code>TrainingArguments</code>에 <code>fp16=True</code>, <code>fp16_opt_level=&#39;O1&#39;</code>만 추가하면 바로 사용할 수 있어서 간단하게 다양한 실험을 진행할 수 있었습니다.</p></li></ul><ul><li><p>hyperparameter_search</p><p>  <code>hyperparameter_search</code>는 <code>Trainer</code>에 존재하는 method로 <code>raytune</code>, <code>optuna</code>, <code>SigOpt</code> 세 가지 중 자신의 환경에 설치되어 있는 라이브러리를 이용하여 적절한 hyperparameter를 탐색해주는 유용한 함수입니다. 저희는 <code>hyperparameter_search</code>로 public leaderboard 기준 4점 이상의 f1 score 향상을 기록할 수 있었습니다.</p>  <details>      <summary>      코드 보기      </summary>      <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, Trainer, TrainingArguments<br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> load your tokenizer &amp; dataset</span><br><span class="hljs-comment"># tokenizer = ...</span><br><span class="hljs-comment"># dataset = ...</span><br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> change your pretrained model path</span><br>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;YOUR_MODEL_PATH&quot;</span>)<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model_init</span>():</span><br>    <span class="hljs-keyword">return</span> AutoModelForSequenceClassification.from_pretrained(<br>        model_path, config=config)<br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> fill it your training arguments</span><br>training_args = TrainingArguments(...)<br><br><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> fill it your trainer arguments</span><br>trainer = Trainer(<br>    model_init=model_init, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> 반드시 model_init 함수로 모델을 불러와야합니다.</span><br>    args=training_args,<br>    ...<br>)<br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> optuna</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">optuna_hp_space</span>(<span class="hljs-params">trial</span>):</span><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;learning_rate&quot;</span>: trial.suggest_float(<span class="hljs-string">&quot;learning_rate&quot;</span>, <span class="hljs-number">5e-6</span>, <span class="hljs-number">5e-4</span>, log=<span class="hljs-literal">True</span>),<br>        <span class="hljs-string">&quot;num_train_epochs&quot;</span>: trial.suggest_int(<span class="hljs-string">&quot;num_train_epochs&quot;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>),<br>        <span class="hljs-string">&quot;seed&quot;</span>: trial.suggest_int(<span class="hljs-string">&quot;seed&quot;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">42</span>),<br>    &#125;<br><br><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> ray tune</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ray_hp_space</span>():</span><br>    <span class="hljs-keyword">from</span> ray <span class="hljs-keyword">import</span> tune<br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;learning_rate&quot;</span>: tune.loguniform(<span class="hljs-number">5e-6</span>, <span class="hljs-number">5e-4</span>),<br>        <span class="hljs-string">&quot;num_train_epochs&quot;</span>: tune.choice(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>)),<br>        <span class="hljs-string">&quot;seed&quot;</span>: tune.choice(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">42</span>)),<br>    &#125;<br><br>trainer.hyperparameter_search(<br>    direction=<span class="hljs-string">&quot;maximize&quot;</span>, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> or direction=&quot;minimize&quot;</span><br>    hp_space=ray_hp_space, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> if you wanna use optuna, change it to optuna_hp_space</span><br>    backend=<span class="hljs-string">&quot;ray&quot;</span>, <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> if you wanna use optuna, remove this argument</span><br>)<br></code></pre></td></tr></table></figure>  </details><p>  <img src="/images/%EC%9A%A9%EB%9F%89%EA%BD%89%EC%B0%B8.png" alt="단점 - 용량꽉참"></p></li></ul><p>또한 <code>W&amp;B</code>를 팀으로 만들어서 팀원들이 실험하는 결과들을 전부 공유할 수 있게 만들었습니다. 덕분에 실패한 실험이나 성공한 실험들에 대해서 chart를 통해 더욱 쉽고 직관적으로 모델을 검증할 수 있었고, 팀원간 더 빠른 결과 공유가 가능했습니다. </p><p align="center">    <img src="/images/w0.png" height="300px"></p><p align="center">    <img src="/images/w1.png" style="display: inline" height="120px">    <img src="/images/w2.png" style="display: inline" height="120px"></p><p align="center">    <img src="/images/w3.png" style="display: inline" height="120px">    <img src="/images/w4.png" style="display: inline" height="120px"></p><h2 id="💌-Thanks-to"><a href="#💌-Thanks-to" class="headerlink" title="💌 Thanks to"></a>💌 Thanks to</h2><p><em><strong>청계산셰르파의 비밀병기, 이유경 멘토님.</strong></em></p><p align="center">    <img src="/images/notion_mentoring.png" style="display: inline" height="360px">    <img src="/images/notion_qna.png" style="display: inline" height="360px"></p><p>정말 바쁘신 와중에도 많은 것을 알려주시려고 열심히 찾아보시고, 따로 공부도 해가시면서 저희에게 많은 도움을 주셨습니다. 저희의 등반일지에 가장 큰 기여를 하신 이유경 멘토님께 다시한번 감사의 말씀 전해드리고 싶습니다.</p><br><p><em><strong>청계산셰르파 팀명에 항상 불만을 가지시는 성예닮 멘토님</strong></em><br><img src="/images/1.png"></p><p>유경멘토님의 사생팬답게 저희팀에도 많은 관심가져주시고 지켜봐주셔서 정말 든든합니다. 항상 말씀못드리는게 죄송할정도로, 많은 도움 주시고 알려주셔서 정말 감사합니다.</p><h2 id="마지막으로"><a href="#마지막으로" class="headerlink" title="마지막으로"></a>마지막으로</h2><p>저희가 처음에 계획했던 <code>기록</code>과 <code>공유</code>라는 가치에 있어서만큼은 전반적으로 팀원 모두가 만족할 수 있었던 프로젝트였습니다.</p><p>처음 합을 맞춤에도 불구하고 팀원 모두가 다음 프로젝트에서는 어떠한 역할을 수행하고, 어떤 식으로 협업을 하는 것이 효과적일지 스스로 깨우칠 수 있었다는 점에서 굉장히 고무적이며, 많은 깨달음을 얻을 수 있었던 경험이었습니다.</p><p>너무 좋은 팀원분들과 함께할 수 있어서 정말 좋았고 다음 대회가 너무나도 기다려지네요.</p><p>저희의 이야기가 조금이라도 도움이 되었길 바라면서 마치겠습니다.</p><p>긴 글 읽어주셔서 감사합니다.</p><h2 id="🤜-부록-팀원들-한마디"><a href="#🤜-부록-팀원들-한마디" class="headerlink" title="🤜 부록: 팀원들 한마디"></a>🤜 부록: 팀원들 한마디</h2><h3 id="좋았던-점"><a href="#좋았던-점" class="headerlink" title="좋았던 점"></a>좋았던 점</h3><ul><li>요한: 철저하게 기록을 하고, 실험결과를 공유했던 점이 가장 잘 한 것 같습니다. 특히 다양한 협업툴을 두고 사용한 것보다 그 기능들을 노션에다가 전부 통합하여 사용한게 혼란이 적어 잘 된 것 같습니다.</li><li>하겸: 실험 자체를 다양하게 시도하고, 실험 공유가 잘 됬던 것 같습니다. 이전 실험의 결론에서 다음 실험은 어떻게 할지를 정한 것도 좋았습니다.</li><li>준영: 팀원 간에 공유가 잘되서 좋았습니다. 기록이 잘되다보니 제가 하지 않은 실험에서도 아이디어를 가져올 수 있었습니다.</li><li>진원: 팀원들끼리 결과 공유와 기록이 잘 이루어졌습니다. 열심히 한 만큼 최종적으로도 괜찮은 결과를 얻어서 만족합니다.</li><li>민수: 실험을 많이 했던 것. 제출 횟수를 꽉 채워서 쓴게 좋았습니다.</li><li>희영: 의견 공유가 잘 되어서 내가 하지 않은 실험에서도 지식을 얻을 수 있어서 좋았습니다. 팀원들과 멘토님이 지식이 많아서 진짜 빠르게 배웠습니다.</li><li>진성 : 각자 실험에 있어 통제변인 설정을 철저히 하고 기록을 세세하게 해, 실험의 효과를 공유하기 좋았습니다.</li></ul><h3 id="아쉬운-점"><a href="#아쉬운-점" class="headerlink" title="아쉬운 점"></a>아쉬운 점</h3><ul><li>요한: 여러 실험 아이디어들이 있었지만, 막판으로 갈 수록 성능에 집착하여 큰 모델로만 실험을 하느라 모든 아이디어들에 대해서 실험을 하지 못했던 것이 아쉽습니다.</li><li>하겸 : 간극이 적은 validation set을 결국은 못찾았다. → 어떻게 찾을 수 있을지는 아직도 모르겠습니다. 성능면에서도 큰 도움이 되지 않아서 아쉬웠습니다.</li><li>준영: 모델 작업을 하지 못했습니다. 데이터 관련해서 많은 인사이트를 찾아보고 싶었지만 결과를 제대로 내지 못했습니다.</li><li>진원: 모델을 수정하는 작업을 많이 하지 못해 아쉬웠습니다. 다양한 실험을 진행하였지만, 모델의 성능을 올리는데 크게 기여하지 못했습니다.</li><li>민수: 모델을 태스크에 맞게 보다 적극적으로 변형하려는 시도를 하지 않았습니다.</li><li>희영: 대회 초기 생활 스케줄이 꼬여서 시간 낭비를 많이 했습니다. 생활 습관을 잘 잡고 시작하는 게 중요할 것 같습니다.</li><li>진성 : 수동적으로 할 일을 받아서 하거나, 다른 사람의 branch에 덧붙여서 작은 실험들만 했습니다. 다음 대회에서는 나도 적극적으로 논문 등에서 아이디어를 얻어와 큼직하게(빠르게) 정확하게 구현하는 연습을 해보자.</li></ul><h3 id="개선할-점"><a href="#개선할-점" class="headerlink" title="개선할 점"></a>개선할 점</h3><ul><li>요한: 성능은 어차피 오를 것이기 때문에, 비슷한 실험을 여러번 하는 것보다는 떠올린 아이디어들에 대해 전부 실험할 수 있도록 해봐야겠습니다.</li><li>하겸: 일단 강의부터 들어서 맨땅에 헤딩하지 않기</li><li>준영: 강의 빠르게 듣기. 데이터 빠르게 훑어보기. 모델 뜯어보기.</li><li>진원: 모델을 수정해보는 실험을 진행하고, 확실하게 성능 향상을 이루기 위해 실험을 진행할 때 조금 더 명확한 근거나 방법을 미리 조사해보고 진행하면 좋을 것 같습니다 (e.g. 논문 읽어서 train 횟수 정하기).</li><li>민수: 어떤식의 변형이 해당 태스크에 적합한지 단순 점수와 느낌 이외에 정량적으로 검증할 수 있는 방법론을 생각해보면 좋을 것 같습니다.</li><li>희영: 첫날 진짜 빠르게 강의 다 듣기. 10시~12시 사이 시간 잘 이용하기. 이동 후 바로 시작하기, 12시 넘으면 집에서 공부할 생각하지 말기. EDA를 너무 오래 하고 raw 데이터를 너무 많이 보면 시간 낭비일수도.</li><li>진성 : 후반부에 기록을 하는 데에 신경을 많이 못썼습니다. 결국 남는건 기록이니 다음 대회 긴 기간동안에도 꾸준히 기록하자.</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
