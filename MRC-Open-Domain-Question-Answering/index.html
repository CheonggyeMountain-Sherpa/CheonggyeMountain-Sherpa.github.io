

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/2.png">
  <link rel="icon" href="/images/2.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Yohan Lee">
  <meta name="keywords" content="">
  <meta name="description" content="대회 소개 Question Answering (QA) 은 다양한 종류의 질문에 대해 대답하는 인공지능을 만드는 연구 분야입니다. 다양한 QA 시스템 중, Open-Domain Question Answering (ODQA) 은 주어지는 지문이 따로 존재하지 않고 사전에 구축되어 있는 Knowledge resource 에서 질문에 대답할 수 있는 문서를 찾아">
<meta property="og:type" content="article">
<meta property="og:title" content="MRC - Open Domain Question Answering">
<meta property="og:url" content="https://cheonggyemountain-sherpa.github.io/MRC-Open-Domain-Question-Answering/index.html">
<meta property="og:site_name" content="청계산셰르파">
<meta property="og:description" content="대회 소개 Question Answering (QA) 은 다양한 종류의 질문에 대해 대답하는 인공지능을 만드는 연구 분야입니다. 다양한 QA 시스템 중, Open-Domain Question Answering (ODQA) 은 주어지는 지문이 따로 존재하지 않고 사전에 구축되어 있는 Knowledge resource 에서 질문에 대답할 수 있는 문서를 찾아">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211106210830331.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211106222959110.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211106223121104.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211109124548155.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211109060404922.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/Top-k_Retrieval_Acc.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211107042614732.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/sparse_passage_ret.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/facebookresearch_dpr_contributing.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211107045109925.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211109062628253.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211109095217407.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211109101344173.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211107051016138.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211107051439508.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211107050716799.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211107050457499.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110040706772.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110070044172.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110071600955.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110091444688.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110162348848.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110200555562-6542356">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110155315446.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110201240878.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110155322213.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110155406916.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110155258759.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110214338773.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110155032798.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110155211405.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110224114767.png">
<meta property="og:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211110234505636.png">
<meta property="article:published_time" content="2021-11-10T14:47:09.000Z">
<meta property="article:modified_time" content="2021-11-10T15:01:27.063Z">
<meta property="article:author" content="Yohan Lee">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cheonggyemountain-sherpa.github.io/images/image-20211106210830331.png">
  
  <title>MRC - Open Domain Question Answering - 청계산셰르파</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"cheonggyemountain-sherpa.github.io","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>청계산셰르파</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="MRC - Open Domain Question Answering">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-11-10 23:47" pubdate>
        November 10, 2021 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      약 16k 단어
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      49분만에 읽기
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">MRC - Open Domain Question Answering</h1>
            
            <div class="markdown-body">
              <h1 id="대회-소개"><a href="#대회-소개" class="headerlink" title="대회 소개"></a>대회 소개</h1><p><img src="/images/image-20211106210830331.png" srcset="/img/loading.gif" lazyload alt="image-20211106210830331"></p>
<p>Question Answering (QA) 은 다양한 종류의 질문에 대해 대답하는 인공지능을 만드는 연구 분야입니다. 다양한 QA 시스템 중, Open-Domain Question Answering (ODQA) 은 주어지는 지문이 따로 존재하지 않고 사전에 구축되어 있는 Knowledge resource 에서 질문에 대답할 수 있는 문서를 찾아 질문에 대한 답을 하는 과제로 일반적인 QA 보다 Challenging한 과제입니다. </p>
<p><img src="/images/image-20211106222959110.png" srcset="/img/loading.gif" lazyload alt="image-20211106222959110"></p>
<p><img src="/images/image-20211106223121104.png" srcset="/img/loading.gif" lazyload alt="image-20211106223121104"></p>
<p>대표적인 ODQA 시스템의 예시로는 Google 검색엔진이 있습니다. Google에 질문을 입력하게 되면 Wikipedia 등에서 Knowledge Resource에서 질문에 답을 할 수 있는 Retrieval이 동작하여 관련 문서를 찾고, Reader가 그 문서 속에서 정답을 찾아 답을 알려주는 시스템입니다.</p>
<p>이번 대회는 이 ODQA 시스템 중 질문과 관련있는 문서를 찾아주는 Retrieval과, 관련된 문서 중에서 적절한 답변을 찾는 Reader의 두 가지 stage를 구현하고 통합하여 정확한 답변을 많이 내주는 모델을 만드는 대회였습니다.</p>
<p>저희 팀은 4주동안 다른 팀 대비 175회라는 압도적인 제출횟수를 기록하며 굉장히 많은 실험을 이어갔고, 저는 그 중에서도 110회의 제출을 하며 대부분의 실험과 성능 향상을 이끌었으며 Private Leaderboard에서 최종 3위를 기록하였습니다.</p>
<h1 id="Strategy-amp-Planning"><a href="#Strategy-amp-Planning" class="headerlink" title="Strategy &amp; Planning"></a>Strategy &amp; Planning</h1><p>이번 대회는 4주라는 긴 시간동안 진행되었기 때문에, 그 시간들을 헛되이 쓰지 않게 하기 위해서는 적절한 대회전략을 세울 필요가 있었고, 그에 걸맞는 계획을 수립해야 했습니다.</p>
<p>이번 대회에는 아래와 같은 제약사항이 있었습니다.</p>
<ol>
<li>외부 dataset 사용 금지.</li>
<li>Pretrained Model weights의 경우 상업적 이용이 가능한 License를 가지고 있으며 누구에게나 공개된 모델이어야 함.</li>
<li>Reader Model을 학습시키는 것이 아닌, 제공된 학습 Data를 Generation Model 등을 이용하여 Augmentation 시키는 것을 목적으로 한다면 상업적 이용이 불가능하더라도 Pretrained Model Weights 사용 가능.</li>
</ol>
<p>지금까지의 경험과 위의 제약사항들을 토대로 보다 strict 하게 계획을 세우기로 하였고, 아래의 Timeline 대로 작업을 진행하기로 했습니다.</p>
<p><img src="/images/image-20211109124548155.png" srcset="/img/loading.gif" lazyload alt="image-20211109124548155"></p>
<p>첫 1주차에는 주어진 baseline 코드가 어렵게 작성되어 있었기 때문에 MRC라는 Task에 대한 공부를 바탕으로 baseline 코드 분석 및 refactoring을 진행했습니다. 2주차부터 본격적으로 EDA와 성능평가를 기반으로 baseline 의 어떤 부분을 보완하면 성능을 향상시킬 수 있을지를 집중적으로 탐색해보았습니다. 그리고 그렇게 탐색한 결과를 바탕으로 3주차에는 data augmentation 과 retrieval, reader 의 성능을 향상시키기 위한 아이디어들을 실제로 구현하고 실험하며 성능을 최대한 끌어올렸고, 그렇게 얻은 모델들을 마지막 4주차 때 앙상블하여 최종 결과를 도출했습니다.</p>
<h1 id="Data-EDA"><a href="#Data-EDA" class="headerlink" title="Data EDA"></a>Data EDA</h1><p><img src="/images/image-20211109060404922.png" srcset="/img/loading.gif" lazyload alt="image-20211109060404922"></p>
<p>주어진 학습 데이터의 샘플은 3,952개로 매우 적은 양이었습니다. 사전학습된 모델로 Fine-tuning을 진행했을 때 쉽게 Overfitting이 일어날 거란 판단을 하게 되었고, Augmentation에 집중하여 좀 더 Robust한 모델이 될 수 있도록 하고 다양한 Augmentation들을 진행한 후 모델에 쉽게 공급하기 위해서 해당 파이프라인을 구축하는 것에 집중할 수 있도록 계획을 세웠습니다.</p>
<p>이후에는 Retrieval과 Reader 두 가지 성능이 모두 중요했기 때문에 성능을 올리기 위한 방법을 찾고 구현하는 쪽에 집중하며, 최종적으로는 다양한 Ensemble을 통해 모델의 성능을 극대화시키고자 했습니다.</p>
<h1 id="Retrieval"><a href="#Retrieval" class="headerlink" title="Retrieval"></a>Retrieval</h1><p><img src="/images/Top-k_Retrieval_Acc.png" srcset="/img/loading.gif" lazyload alt="Top-k Retrieval Acc."></p>
<p><img src="/images/image-20211107042614732.png" srcset="/img/loading.gif" lazyload alt="image-20211107042614732"></p>
<p><code>Retrieval</code>은 질문에 맞는 알맞은 문서를 찾아주는 모델을 말합니다. 문서를 찾는 방법은 대표적으로 Sparse Matrix를 이용한 TF-IDF, BM25 와 같은 방식과 질문과 문서의 Dense Embedding을 활용한 방법이 있습니다.</p>
<h2 id="Sparse-Passage-Retrieval"><a href="#Sparse-Passage-Retrieval" class="headerlink" title="Sparse Passage Retrieval"></a>Sparse Passage Retrieval</h2><p>TF-IDF는 단어의 등장 빈도와 단어가 제공하는 정보의 양을 고려한 알고리즘이며, BM25는 TF-IDF의 개념을 바탕으로 문서의 길이까지 고려하여 점수를 산출하는 알고리즘입니다. BM25의 수식은 아래와 같으며 평균적인 문서의 길이보다 더 작은 문서에서 단어가 매칭된 경우 그 문서에 대해 가중치를 부여하는 방식으로, 실제 검색엔진과 추천 시스템에서 아직까지도 많이 사용되는 알고리즘입니다.</p>
<p><img src="/images/sparse_passage_ret.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>Test dataset의 question에 대해서 Retrieval을 통하여 유사한 Top-k 개의 passage를 가져오도록 한 뒤 해당 passage들을 question 뒤에 concat 하여 reader 모델로 inference 를 진행합니다. k를 늘릴 수록 Sparse Passage Retrieval의 성능은 향상되었으나, reader 모델의 혼란이 가중된다는 문제가 있었기에, Retrieval의 성능을 개선할 필요가 있었습니다.</p>
<h2 id="Dense-Passage-Retrieval-DPR"><a href="#Dense-Passage-Retrieval-DPR" class="headerlink" title="Dense Passage Retrieval (DPR)"></a>Dense Passage Retrieval (DPR)</h2><p>Sparse Embedding을 사용하면 검색하고자 하는 모든 문서를 이루고 있는 word에 대해 해당 문서가 가지고 있는 word를 표시해야 하므로 차원의 수가 매우 커지고 대부분의 특성이 0이 되는 문제를 가지고 있습니다. 따라서 Sparse한 Matrix를 더 적은 차원의 고밀도 벡터로 만들어 사용하는 방법론이 <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/DPR">DPR</a>입니다. 논문에 따르면 Sparse Passage Retrieval보다 성능면에서 뒤쳐졌지만, 두 개의 Bert Encoder를 사용하여 query와 passage를 따로 훈련시키게 되면서 그 성능이 비약적으로 발전하게 되었습니다.</p>
<p>DPR을 위한 두 개의 모델을 훈련시킬 때, 하나의 질문에 대한 ground truth passage를 positive passage로 두고, batch안에 존재하는 다른 질문들에 대한 passage를 negative passage로 두는 in-batch negative 방식을 사용합니다. </p>
<p><img src="/images/facebookresearch_dpr_contributing.png" srcset="/img/loading.gif" lazyload alt="facebookresearch_dpr_contributing"></p>
<h1 id="Reader"><a href="#Reader" class="headerlink" title="Reader"></a>Reader</h1><p><code>Reader</code>는 주어진 질문에 대해 Retrieval이 가져온 문서를 읽고 정답을 찾아내는 모델입니다. Reader의 경우 <code>Extractive</code> 와 <code>Generative</code> 방식이 존재하는데, Extractive Reader는 문서 내에서 모델이 정답일 것으로 예측하는 <code>토큰의 시작과 마지막 인덱스</code>의 확률(logit)을 구하고 이들의 합이 최대가 되는 문자열을 정답으로 예측합니다. 반면 Generative Reader는 문서를 읽고 정답일 것으로 예측되는 <code>토큰을 생성</code>하여 만들어낸 문자열을 정답으로 예측합니다.</p>
<p>학습데이터의 양이 매우 적었기 때문에 <code>사전학습된 한국어 모델을 활용</code>하고자 했으며, Extractive 방식에 집중하고자 했습니다. 영어의 경우 일반적으로 Generative Reader가 성능이 좋다고 알려져 있습니다. 왜냐하면 Extractive Reader의 경우 정답이 반드시 문서 내에 존재해야 한다는 단점이 있는 반면, Generative Reader는 문서 내에 정답이 존재하지 않아도 충분히 정답이 될 토큰을 생성해낼 수 있기 때문입니다. 하지만 Huggingface Hub에 공개되어 있는 사전학습된 한국어 생성 모델중에 중에 저작권 문제 없는 모델들은</p>
<ul>
<li><code>hyunwoongko/kobart</code> (seq2seq)</li>
<li><code>google/mt-5</code> 계열 (seq2seq)</li>
<li><code>kykim/gpt3-kor-small_based_on_gpt2</code> (gpt2)</li>
</ul>
<p>위의 세 가지 정도였고, Extractive Reader로 사용할 수 있는 Language Model들과는 다르게 모델의 크기가 적고 실제 Baseline 성능평가 당시 사전학습된 Extractive Reader로 Fine-tuning 하는 것보다 성능이 낮았기 때문에 Generative 방식은 사용하지 않았습니다.</p>
<p>저희는 Reader 의 성능 개선을 위한 모든 아이디어들에 대하여 실험을 진행하였고, 실험 결과들에 의해서만 좋은 방법들을 취사선택하였습니다.</p>
<h2 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h2><p><img src="/images/image-20211107045109925.png" srcset="/img/loading.gif" lazyload alt="image-20211107045109925"></p>
<p>Extractive 방식으로 사용할 수 있는 여러 가지 사전학습 모델로 성능평가를 진행했고, 테스트 결과 validation EM 기준 가장 점수가 높았던 <code>klue/roberta-large</code> 모델을 backbone 으로 삼게 되었습니다.</p>
<h2 id="Performance-evaluation-of-backbone-according-to-retrieval"><a href="#Performance-evaluation-of-backbone-according-to-retrieval" class="headerlink" title="Performance evaluation of backbone according to retrieval"></a>Performance evaluation of backbone according to retrieval</h2><p>사전학습된 Backbone 모델의 정확한 성능평가를 위하여 baseline 에 주어진 inference 코드로 evaluation 을 진행했습니다. 처음에 주어졌던 retrieval 방식은 TF-IDF 를 이용하여 질문에 대한 Top-k 개의 문서를 가져온 뒤 아래와 같이 불러온 문서를 모두 붙여서 정답의 시작과 마지막 인덱스를 예측하도록 이루어져 있었습니다. 질문에 Top-k 개의 문서를 붙인 뒤 Inference를 하기 위한 모델의 input 형식은 아래와 같습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">question + [SEP] + passage_1 + <span class="hljs-string">&#x27; &#x27;</span> + passage_2 + <span class="hljs-string">&#x27; &#x27;</span> + ... + passage_k<br></code></pre></td></tr></table></figure>

<p><img src="/images/image-20211109062628253.png" srcset="/img/loading.gif" lazyload alt="image-20211109062628253"></p>
<p>K의 개수를 늘릴 수록 public leaderboard 와 validation score 의 간극이 줄어드는 것을 확인하였고, 이러한 retrieval 방식에 착안하여 다음 실험을 계획할 수 있었습니다.</p>
<h2 id="Concat-Top-k-Negative-Samples"><a href="#Concat-Top-k-Negative-Samples" class="headerlink" title="Concat Top-k Negative Samples"></a>Concat Top-k Negative Samples</h2><p>해당 방법의 아이디어를 얻은 논문은 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a> 입니다. 해당 논문에서는 Retrieval을 위한 Dense Embedding을 만들 때 </p>
<blockquote>
<p> Our best model uses gold passages from the same mini-batch and one BM25 negative passage.</p>
</blockquote>
<p>BM25를 통해 가져온 negative sample을 추가하여 같이 학습시켰을 때가 가장 높은 퍼포먼스를 내는 모델이었다고 언급합니다. 이 실험에 착안하여 Reader 모델 학습 때 역시 negative sample을 추가하여 학습시키면 성능이 향상될 것이란 기대를 하게 되었습니다.</p>
<p>해당 실험을 통해 성능이 향상될 것이란 기대를 하게된 근거는 아래와 같습니다.</p>
<ol>
<li>현재 Inference 는 Retrieval 로부터 질문과 유사한 Top-k 개의 passage를 불러온 뒤 concat 하고 그 안에서 Extractive Reader 가 정답을 찾게 하는 과정으로 이루어진다. Inference 과정처럼 Reader 모델 학습 때도 여러 passage를 concat 해놓고 정답을 찾게 하는식으로 최대한 비슷한 환경을 조성해 놓으면 예측 때 모델의 혼란이 줄어들 것이다.</li>
<li>Retrieval 이 upgrade 되더라도 실험의 전제가 변하지 않을 것이고, 동일하게 학습시켜놓았던 모델로 더 좋은 결과를 얻을 수 있을 것이다.</li>
<li>answer 키워드가 반복될 수 있는 positive sample 을 사용하면 오히려 모델의 혼란이 가중될 것이다.</li>
</ol>
<p><img src="/images/image-20211109095217407.png" srcset="/img/loading.gif" lazyload alt="image-20211109095217407"></p>
<p>실험결과 예상과 같이 여러 passage를 concat 하였을 때 기존 validation score 보다 높은 점수를 기록하였고, possitive sample 보다는 negative sample 을 concat 하여 Reader 를 학습시켰을 때 훨씬 높은 점수를 기록할 수 있었다.</p>
<p>Negative sample 을 선택하는 방법에 따라서도 Reader 의 성능이 천차만별이었습니다. 특히 Retrieval과 Tokenizer 의 영향을 가장 많이 받을 것으로 예상했고, 두 가지에 집중하여 가장 좋은 성능을 내는 negative sample 을 뽑을 수 있도록 하였습니다.</p>
<p>사용할 수 있는 Retrieval 은 BM25, Elastic search, Hybrid retrieval 이 있었으며, Tokenizer 는 Elastic search 에 사용되는 Nori Tokenizer 와 BERT Tokenizer 가 있었습니다. 저희는 이 조합들을 사용하여 validation 을 측정해보았고 실험결과는 아래와 같습니다.</p>
<p><img src="/images/image-20211109101344173.png" srcset="/img/loading.gif" lazyload alt="image-20211109101344173"></p>
<p>결국, 최종적으로 사용한 Negative Sample 기준은 아래와 같습니다.</p>
<ul>
<li>Elastic Search 와 Nori Tokenizer 를 사용하며, 정답을 포함하지 않는 상위 4개의 전처리된 passage</li>
</ul>
<p>처음에는 Retrieval 성능이 올라갈수록 모델이 더 혼동하기 쉬운 Negative Sample 을 많이 포함시켜 Reader 의 성능이 오를 것으로 예상하였으나, 막상 실험을 해보니 Dense Embedding 을 혼합한 Retrieval 보다는 BM25 계열의 Sparse Embedding 을 활용하여 negative sampling 을 진행했을 때 결과가 더 좋았습니다. Train dataset 에 대하여 Top-k (k=4) 개의 문서를 합쳤을 때 총 19,760 개의 wiki passage 를 보게 되는데, retrieval 성능이 좋을 수록 합치는 negative sample 중에서도 중복되는 문서들이 많았기 때문입니다.</p>
<p>결과적으로 명확한 원인은 찾아내지 못했지만 중복되는 문서들이 많았기 때문으로 많았을 것으로 예상할 수 있ㅇ습니다. 정량적인 분석 도구와 NLP에서의 EDA 에 대한 공부를 추가적으로 하고 싶었다.</p>
<p>![Chart Title (1)](/images/Chart Title (1’).pn)</p>
<p>기존 Train dataset 에는 약 500 건 정도의 ground truth passage 가 중복되어 있고, 저희가 사용하는 wiki 에는 같은 title 의 문서이지만 문단이 다른 경우가 </p>
<p>총 60613, 중복 제외 56,737, 중복 제외 후 title 겹치는거 31,726 (중복 제거 전에 타이틀 중복 31,755)</p>
<p>sparse embedding 인 tf-idf 기반의 bm25 계열에서는 question 에 등장한 단어와 유사한 것을 기준으로 판별하고, hybrid retrieval 에서는 그뿐만 아니라 symantic 의미론적으로 유사하게 사용되는 동의어나 대체어 뭐 이런 단어들에 대한 문서도 가져왔음 다만 이런 문서들이 쓰임새 등도 비슷하기 때문에 Task 난이도가 너무 높아졌다? 모델의 학습이 제대로 이루어지지 않았으며, 부가적으로 중복도 많았음. 총 31,726 개의 위키를 보는데 이 때 중복만 4,448개 . 이미 쉽게 과적합이 되는 상태에서 embedding 이 겹치는 passage 들에 과적합되어 버리기 때문에 Negative sample 로서의 기대했던 역할을 제대로 수행하지 못했다고 생각하게 되었음. </p>
<h2 id="hyperparameter-tuning"><a href="#hyperparameter-tuning" class="headerlink" title="hyperparameter tuning"></a>hyperparameter tuning</h2><p>위의 실험들을 바탕으로 Elastic Search 로부터 Top-k 개의 negative sample 을 포함하여 데이터셋을 재구축하였고, 그 중 가장 성능이 높았던 k=4 를 기준으로 hyperparameter tuning 을 진행했습니다. Huggingface Trainer 에 존재하는 hyperparameter_search 를 사용하였고, batch size, learning rate, seed, gradient accumulation step, weight decay 다섯 개에 대하여 validation EM score 를 maximize 할 수 있도록 search 를 진행했습니다. 그 결과 learning rate 의 영향이 상당히 중요했으며, V100 을 사용중이었기 때문에 하드웨어적인 제약으로 gradient accumulation step 을 사용하여 batch 의 크기를 키워보았고, batch size * gradient accumulation step 이 128일 때에 학습이 가장 잘 이루어진다는 것을 알 수 있었다.</p>
<ul>
<li>Optuna study 활용 (huggingface trainer.hyperparameter_search())</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_hp_space</span>(<span class="hljs-params">trial</span>):</span><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;learning_rate&quot;</span>: trial.suggest_float(<span class="hljs-string">&quot;learning_rate&quot;</span>, <span class="hljs-number">1e-6</span>, <span class="hljs-number">1e-4</span>, log=<span class="hljs-literal">True</span>),<br>        <span class="hljs-string">&quot;seed&quot;</span>: trial.suggest_int(<span class="hljs-string">&quot;seed&quot;</span>, <span class="hljs-number">1</span>, <span class="hljs-number">123</span>),<br>        <span class="hljs-string">&quot;per_device_train_batch_size&quot;</span>: trial.suggest_categorical(<br>            <span class="hljs-string">&quot;per_device_train_batch_size&quot;</span>, [<span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">12</span>, <span class="hljs-number">16</span>]<br>        ),<br>        <span class="hljs-string">&quot;weight_decay&quot;</span>: trial.suggest_float(<span class="hljs-string">&quot;weight_decay&quot;</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.3</span>),<br>        <span class="hljs-string">&quot;gradient_accumulation_steps&quot;</span>: trial.suggest_categorical(<br>            <span class="hljs-string">&quot;gradient_accumulation_steps&quot;</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>]<br>        ),<br>    &#125;<br></code></pre></td></tr></table></figure>

<p><img src="/images/image-20211107051016138.png" srcset="/img/loading.gif" lazyload alt="image-20211107051016138"></p>
<p><img src="/images/image-20211107051439508.png" srcset="/img/loading.gif" lazyload alt="image-20211107051439508"></p>
<p><img src="/images/image-20211107050716799.png" srcset="/img/loading.gif" lazyload alt="image-20211107050716799"></p>
<p>20번의 trial 을 거쳤으며 W&amp;B의 sweep 기능을 이용하여 가장 좋은 조합을 찾았다.</p>
<p><img src="/images/image-20211107050457499.png" srcset="/img/loading.gif" lazyload alt="image-20211107050457499"></p>
<p>결과 public leaderboard 상 5 이상의 score 향상을 얻을 수 있었다.</p>
<h2 id="Custom-Heads"><a href="#Custom-Heads" class="headerlink" title="Custom Heads"></a>Custom Heads</h2><p>실험을 이어가면서 Test dataset 에 대해 Inference 를 하고 나온 결과를 살펴보던 중 아래의 예시들을 통해 의문을 가지게 되었습니다.</p>
<ol>
<li>예측한 정답에 형용사, 조사 등이 포함되어 있는 경우 - <code>대통령인 빌헬름 미클라스</code>, <code>&lt;자유&gt;지에</code></li>
<li>예측한 정답의 substring 이 반복되는 경우 - <code>주노해변 주노해변</code></li>
<li>예측한 정답이 문장인 경우 - <code>남자아이 빈을 데려다 양자로 삼는다는 것에서..</code></li>
</ol>
<p>Extractive 방식에 따라 문서 내에 존재하는 토큰의 위치를 찾게 되는데, 위의 예시처럼 <code>형용사</code> 를 포함해야 할 것인지, <code>반복</code> 되는 키워드의 경우 어떻게 잘라내야 할 것인지, <code>문장</code> 이 정답이라면 문장 전체를 정답으로 할 지, 아니면 <code>명사구</code> 가 될 수 있도록 end position 을 이동시켜야 할 지 등에 대한 고민을 하게 되었습니다. 그리고 이러한 문제가 발생한 원인은 모델이 ‘일관적이지 않게’ 정답을 추출해내고 있기 때문일 것으로 예상하고 사용하고 있던 huggingface 의 QuestionAnswering 소스 코드를 살펴보았습니다.</p>
<p><img src="/images/image-20211110040706772.png" srcset="/img/loading.gif" lazyload alt="image-20211110040706772"></p>
<p>Huggingface 의 Extractive Reader 방식으로 사용되는 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/_modules/transformers/models/roberta/modeling_roberta.html#RobertaForQuestionAnswering">QuestionAnswering</a> 모델의 head 는 backbone 의 sequence output shape 인 hidden_size 를 2(start_logit, end_logit) 로 축소한 Linear layer 가 사용됩니다. 저희가 backbone 으로 사용한 <code>klue/roberta-large</code> 의 경우 hidden_size 가 1,024 로 매우 컸는데, 실험을 이어가다 보니 이렇게 큰 사이즈의 벡터를 바로 두 개의 logit 으로 축소한다는 것에서 위의 예시와 같은 문제점들이 생겨날 수 있을 것으로 판단하게 되었고 다양한 head 를 사용하고 튜닝함으로써 모델이 보다 일관적으로 정답을 예측하고, 이러한 문제를 해결할 수 있을 것으로 기대하게 되었습니다. 또한 최종 제출 전략인 Ensemble 을 고려하였을 때 head 에 다양성을 준 모델의 퍼포먼스가 낮아도 Ensemble 의 좋은 재료가 될 수 있을 것이란 기대로, 최대한 다양한 head 를 실험해보기로 하였습니다.</p>
<p>Head 딴에 custom layer 를 추가하기 위해 아래 논문과 블로그에서 아이디어를 얻게 되었습니다.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15718571.pdf">Research of LSTM Additions on Top of SQuAD BERT Hidden Transform Layers</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.09694v4.pdf">Retrospective Reader for Machine Reading Comprehension</a></li>
<li><a target="_blank" rel="noopener" href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/">Implementing a CNN for Text Classification in TensorFlow</a></li>
<li><a target="_blank" rel="noopener" href="https://ratsgo.github.io/natural%20language%20processing/2017/03/19/CNN/">CNN으로 문장 분류하기</a></li>
</ul>
<p>저희가 실험에 사용했던 Custom Layer 와 전체적인 Architecture 는 아래와 같습니다.</p>
<p><img src="/images/image-20211110070044172.png" srcset="/img/loading.gif" lazyload alt="image-20211110070044172"></p>
<p>RNN Family 를 사용한 head 의 모델에서는 정답이 포함된 Sequence 에 조금 더 집중할 수 있기를 기대했고, CNN 은 정답과 조금 더 유사한 feature 들, Transformer Encoder 에서는 사전학습된 Backbone 의 representation 보다 주어진 학습 데이터에 집중하여 보다 성능을 개선할 수 있기를 기대했습니다.</p>
<p><img src="/images/image-20211110071600955.png" srcset="/img/loading.gif" lazyload alt="image-20211110071600955"></p>
<p>막상 실험을 진행하니, 현재 backbone 도 과적합이 쉽게 일어나는 sensitive 한 상황이었기 때문에 head 에서 복잡도를 키우게 되니 validation score 는 backbone 보다 하락하였습니다. 특히 Conv1d layer 에서는 out_feature 가 작을수록, RNN Family 에서는 hidden_size 가 클수록 좋은 성능을 보였으며 전체적으로 custom layer 의 개수를 늘릴 수록 학습이 제대로 진행되지 않았습니다.</p>
<p>Custom head 를 부착한 모델들로 inference 를 진행한 후 나온 nbest_predictions 를 정성적으로 살펴보았을 때 각각의 head 들의 특징을 대략적으로나마 알 수 있었는데</p>
<p><img src="/images/image-20211110091444688.png" srcset="/img/loading.gif" lazyload alt="image-20211110091444688"></p>
<p>위의 예시에서 Conv1d 는 정답인 <code>사락사라</code> 와 비슷한 의미를 가진 ‘행성’, ‘지구’, ‘베텔게우스’ 등의 단어에 집중하는 모습을 보였고, Bi-LSTM과 Bi-GRU 는 정답인 <code>사락사라</code> 가 포함되어 있는 문장인 <code>40억년전 지구에서, 사락사라의 사람들이 자신들만의 낙원을 만들기 위해 지구에 착륙하였다.</code> 라는 시퀀스에 집중하였으며, Transformer Encoder 의 경우 Conv1d 와 RNN Family 의 특성을 모두 가지고 있는 듯한 경향을 보였습니다.</p>
<p>다만 각각의 모델들은 backbone 보다 public leaderboard score 가 비슷하거나 조금 더 낮은 점수를 기록하였습니다. 하지만 점수가 비슷함에도 예측한 정답들이 매우 달랐기 때문에 Ensemble 의 좋은 재료로 활용할 수 있을 것으로 예상할 수 있었습니다.</p>
<h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><p>다음으로 시도한 것은 Data Augmentation 입니다. 학습 데이터가 너무 적었기 때문에 더욱 robust 한 모델을 만들기 위해서는 여러 방면으로 데이터를 증강한 뒤 다양한 데이터셋을 만들 필요가 있다고 생각했습니다. 크게 AEDA, Shuffling, Question Generation 세 가지 방법으로 Augmentation 을 진행하였으며, 해당 방법들로 약 10배 이상의 데이터를 만들 수 있었습니다.</p>
<h3 id="AEDA"><a href="#AEDA" class="headerlink" title="AEDA"></a>AEDA</h3><p><img src="/images/image-20211110162348848.png" srcset="/img/loading.gif" lazyload alt="image-20211110162348848"></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.13230.pdf">AEDA(An Easier Data Augmentation Technique for Text Classification)</a> 는 Original Text 의 random 한 위치에 punctuation 을 삽입하는 Augmentation 방법입니다. 해당 논문에서는 <code>Text Classification</code> Task 에서 큰 성능 향상을 보았는데, 각각의 Token 사이에 노이즈를 추가함으로써 모델의 과적합을 방지하고 서로 붙어있는 Token 간의 의존성을 낮추어 일반화 성능이 좋은 모델을 만들어 낼 수 있지 않았을까 라는 생각을 하게 되었습니다. 이 생각을 바탕으로 학습데이터에 존재하는 질문들에다가 AEDA Augmentation 을 사용하면 짧은 Sequence 라고 하더라도 질문의 의도나 Representation 을 잘 파악할 수 있을 것이란 예상을 하게 되었습니다. 저희는 하나의 Question 에 대하여 1개, 2개, 4개, 8개의 augmentation sentence 를 생성하였습니다. 질문의 마지막에는 항상 <code>?</code> 가 포함되어 있었기 때문에 마지막 인덱스를 제외한 text 에 <code>[&quot;.&quot;, &quot;,&quot;, &quot;!&quot;, &quot;;&quot;, &quot;:&quot;]</code> 의 punctuation 들을 삽입했습니다. 빠른 작업속도를 위해 Mecab Tokenizer 를 사용하였고, 삽입하는 확률을 0.2 로 설정함으로써 punctuation 이 너무 자주 삽입되는 것을 방지했습니다.</p>
<p>하지만 <code>Question + [SEP] + Ground Truth Passage + &#39; &#39; + Negative Samples</code> 로 이루어지는 input 형태에서 Question 에만 변형을 주는 것은 좋은 성능으로 이어지지 않았습니다. 그 이유를 예상해보았을 때 Question 에 대한 학습에는 도움이 되었을 지 몰라도, 아래 두 가지를 떠올려볼 수 있었습니다.</p>
<ol>
<li>AEDA 를 수행한 sentence 가 많을수록 SEP Token 을 기준으로 합치는 Passage 들이 매우 많았기 때문에 실제 정답을 찾아내는 수행 능력만큼은 큰 차이가 없었을 것이다.</li>
<li>오히려 겹치는 Passage 에 대해서 보다 쉽게 overfitting 이 발생했기 때문에 inference 에서 기존보다 예측 결과가 나빴을 것이다.</li>
</ol>
<p>따라서 AEDA 를 제외한 다른 Augmentation 방법에 집중하게 되었습니다.</p>
<h3 id="Sentence-Shuffle"><a href="#Sentence-Shuffle" class="headerlink" title="Sentence Shuffle"></a>Sentence Shuffle</h3><p>다음으로 시도한 Augmentation 은 Sentence 를 뒤섞는 방법입니다.</p>
<p><img src="/images/image-20211110200555562-6542356'.pn" srcset="/img/loading.gif" lazyload alt="image-20211110200555562"></p>
<p>위의 예시를 다시 보면, <code>유령은 어느 행성에서 지구로 왔는가?</code> 에 대한 정답은 <code>사락사라</code> 입니다. 이 때 정답이 포함된 문장은 Passage 의 가장 첫 부분에 나타나고 있습니다. 또한 이 질문의 경우 앞, 뒷문장 또는 문서의 맥락과 상관 없이 정답이 포함된 문장만 보더라도 정답을 예측할 수 있습니다. 이렇게 여러 문장과의 관계를 살피지 않고 한 문장만 보고 정답을 맞출 수 있는 쉬운 질문의 경우 다른 문장은 학습에 오히려 <code>방해요소로 작용할 것</code>이란 생각을 하게 되었습니다. 그 생각을 바탕으로 문장들을 뒤섞는 아이디어를 떠올리게 되었고 <a target="_blank" rel="noopener" href="https://www.google.co.kr/amp/s/neptune.ai/blog/data-augmentation-nlp/amp">Data-Augmentation-NLP</a> 해당 블로그를 참조하여 성능 향상 가능성을 보게 되었습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(train_dataset))):<br>    split_sentences = kss.split_sentences(train_dataset[i][<span class="hljs-string">&#x27;context&#x27;</span>])<br>    split_sentences_table = &#123;s: i <span class="hljs-keyword">for</span> i, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(split_sentences)&#125;<br>    answer_start = train_dataset[i][<span class="hljs-string">&#x27;answers&#x27;</span>][<span class="hljs-string">&#x27;answer_start&#x27;</span>][<span class="hljs-number">0</span>]<br><br>    ...<br><br>    random.shuffle(split_sentences)<br>    shuffle_index = [split_sentences_table[sentence] <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> split_sentences]<br>    shuffle_answer_sentence_index = shuffle_index.index(answer_sentence_index)<br></code></pre></td></tr></table></figure>

<p>kss 라이브러리의 <code>split_sentences</code> 함수를 사용하여 passage 를 문장단위로 분리한 이후 random 으로 shuffle 을 진행하였습니다. 이 때 정답의 시작 인덱스가 변경되기 때문에, 분리한 문장 배열에서 원래 정답이 속해있던 문장의 index 를 hash_table 에 저장한 뒤 변경된 문장의 위치에서 올바른 정답을 찾아갈 수 있는 처리를 해주었습니다.</p>
<p><img src="/images/image-20211110155315446.png" srcset="/img/loading.gif" lazyload alt="image-20211110155315446"></p>
<p><img src="/images/image-20211110201240878.png" srcset="/img/loading.gif" lazyload alt="image-20211110201240878"></p>
<p>결과는 위의 사진과 같습니다. 문장의 위치를 섞어줌으로써 Backbone 에게 어려운 Sample 을 더 많이 보여주어 성능 향상을 기대해볼 수 있었습니다.</p>
<h3 id="Passage-Shuffle"><a href="#Passage-Shuffle" class="headerlink" title="Passage Shuffle"></a>Passage Shuffle</h3><p>기존에는 Ground Truth 에다가 Negative Sample 4개를 이어붙여서 학습을 진행했었는데 sentence shuffle 을 진행하면서 이어붙이는 passage 들도 뒤섞는 것에 대한 아이디어를 떠올리게 되었습니다. 왜냐하면 Ground Truth 가 항상 맨 앞에 등장하기 때문에 Reader 모델이 Passage 들의 앞부분에만 집중할 수 있다고 생각했기 때문입니다. 또한 inference phase 에서 Retrieval 의 score 대로 passage 를 이어붙이게 되는데, 이 때 정답을 찾을 수 있는 passage 가 맨 앞에 오지 않는다면, Reader 의 성능이 급격히 떨어질 수도 있을 것이란 생각을 하게 되었습니다.</p>
<p><img src="/images/image-20211110155322213.png" srcset="/img/loading.gif" lazyload alt="image-20211110155322213"></p>
<p>이러한 아이디어에 착안하여 위와 같이 Ground Truth 와 Negative Sample 의 순서를 random 하게 섞어줌으로써 Reader 의 성능 향상을 기대해볼 수 있었습니다.</p>
<h3 id="Sentence-Shuffle-Passage-Shuffle"><a href="#Sentence-Shuffle-Passage-Shuffle" class="headerlink" title="Sentence Shuffle + Passage Shuffle"></a>Sentence Shuffle + Passage Shuffle</h3><p><img src="/images/image-20211110155406916.png" srcset="/img/loading.gif" lazyload alt="image-20211110155406916"></p>
<p>또한 앞의 두 아이디어와 비슷한 기대를 하며 Passage 들을 섞고 난 뒤 각각의 Passage 의 문장들까지 섞어놓은 학습데이터까지 구축하게 되었습니다. 이렇게 위의 세 가지 방식을 사용하여 처음보다 3배의 데이터를 추가로 확보할 수 있었습니다.</p>
<h3 id="Question-Generation"><a href="#Question-Generation" class="headerlink" title="Question Generation"></a>Question Generation</h3><p>다음으로 시도한 Augmentation 은 Question Generation 입니다. 외부데이터셋 사용이 제한된 상태에서 사용할 수 있는 Augmentation 중 가장 기대를 많이했던 방법입니다. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.06705">Conditional BERT Contextual Augmentation</a> 논문을 보고 생성모델을 사용한 augmentation 기법에 아이디어를 얻게 되었고, 아래의 문서와 레포지토리를 참고하여 간단하게 Question Generation을 수행할 수 있었습니다.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://kakaobrain.github.io/pororo/seq2seq/qg.html">Pororo Question Generation</a></li>
<li><a target="_blank" rel="noopener" href="https://kakaobrain.github.io/pororo/tagging/ner.html">Pororo Named Entity Recognition</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/codertimo/KorQuAD-Question-Generation/tree/main/korquad_qg">KorQuAD-Question-Generation</a></li>
</ul>
<p>질문을 생성하기 위해서는 Target 이 되는 Answer 가 필요했기 때문에 NER 을 활용하여 Entity 인 것들을 Target 으로하는 질문 생성과, Wiki 문서의 제목이 문서 본문에 포함되면, 제목을 Target 으로 하는 질문을 생성하는 두 가지 접근법을 생각해 냈습니다. 그리고 사전학습된 생성모델인 SKT-AI 의 KoGPT-2 와 Kakaobrain 의 KoBART 를 사용하여 새로운 질문을 생성해냈고, 학습 데이터와 유사한 형태의 데이터를 만들어낼 수 있었습니다.</p>
<ol>
<li><p>NER + Question Generation</p>
<p><img src="/images/image-20211110155258759.png" srcset="/img/loading.gif" lazyload alt="image-20211110155258759"></p>
<p>Pororo 의 NER 은 Entity 가 없으면 <code>&#39;O&#39;</code> 를 반환합니다. 따라서 ‘O’ 가 부착되지 않은 Entity 를 정답으로 하는 질문을 생성해냈고, 약 14만 건의 데이터를 확보할 수 있었습니다. 다만 질문의 퀄리티가 상당히 좋지 않았기 때문에 filtering 을 거칠 필요가 있었습니다. 정성적으로 살펴본 결과 entity 의 길이가 짧은 것들 대부분에 의존명사가 포함되어 있었고, 질문이 이러한 entity 를 제대로 표현할 수 없겠다는 생각이 들었습니다. 학습 데이터셋의 정답 길이의 평균이 6 이상이었기 때문에 길이가 6보다 적은 entity 를 제거하였고, 약 2만 3천 개 정도의 질문이 남게 되었습니다. 또한 다른 target 에 대하여 동일한 질문이 곳곳에 포함되어 있었고, target 과 전혀 상관없는 질문이 생성되기도 하였습니다. 그래서 저희는 <a target="_blank" rel="noopener" href="https://kakaobrain.github.io/pororo/text_cls/sts.html">Pororo Semantic Textual Similarity</a> 를 사용하여 질문과 답변의 유사도가 0.5 이상인 것들만 추려서 최종적으로 약 2천 개의 데이터를 확보하게 되었습니다.</p>
</li>
</ol>
<ul>
<li><ol start="2">
<li>Wiki Title question generation</li>
</ol>
<p><img src="/images/image-20211110214338773.png" srcset="/img/loading.gif" lazyload alt="image-20211110214338773"></p>
<p>Named Entity 를 Target 으로 하여 생성해낸 질문들을 필터링하니 데이터가 별로 남지 않아서 다른 방법을 고안하게 되었습니다. Wiki의 title 을 Target 으로 질문을 생성해내면 문서를 대표하는 정답으로 질좋은 질문들을 생성해낼 수 있다고 판단했기 때문입니다. 이 때 주어진 Wiki 에서 중복을 제거하고, Title 이 겹치는 문서들도 제거한 31,726 개의 Wiki 문서에서 Title 이 문서 내에 포함되어 있는 문서의 개수는 약 17,000 개였습니다. 저희는 KoGPT-2 를 사용하여 17,000 여개의 정답을 Target 으로 하는 질문들을 생성했습니다.</p>
</li>
</ul>
<p>그렇게 총 약 3만 개의 질의응답 데이터셋을 구축할 수 있었고, 이렇게 구축한 데이터셋들을 서로 다른 조합으로 하여 학습을 이어갔습니다. 하지만, Augmentation 한 데이터셋으로 학습하여도 기존의 리더보드 점수를 따라잡을 수 없었습니다.</p>
<blockquote>
<p>Pororo 는 기본적으로 batchify 를 지원하지 않기 때문에 빠른 속도로 augmentation 을 진행할 수 없었습니다. 단순히 컴퓨터를 켜놓고 대기할 수 밖에 없었는데, 다음에는 batchify 방식을 구현하여 보다 빠른 작업을 진행해야 겠다는 생각을 하게 되었습니다.</p>
</blockquote>
<h2 id="Curriculum-learning"><a href="#Curriculum-learning" class="headerlink" title="Curriculum learning"></a>Curriculum learning</h2><p>Augmentation 을 진행했는데 성능 개선을 못 본 것에 대하여 아래와 같은 이유들을 생각해볼 수 있었습니다.</p>
<blockquote>
<ol>
<li>생성해낸 질문의 퀄리티가 생각보다 좋지 않아서 오히려 모델의 학습을 방해했다.</li>
<li>증강한 질의응답의 난이도가 너무 어렵다.</li>
</ol>
</blockquote>
<p>정성적인 평가를 진행했을 때 생성해낸 질문들 중에 이상하거나 중복된 질문들을 찾지 못했습니다. 또한 Sentence 와 Passage 를 모두 shuffle 한 학습데이터의 경우 Validation EM Score 가 40점을 넘기지 못했습니다. 때문에 두 번째 이유에 집중하여 모델의 학습 방식을 개선해보기로 하였습니다. 그 중에서도 <a target="_blank" rel="noopener" href="https://hwiyong.tistory.com/327">Curriculum Learning - 대학원생이 쉽게 설명해보기</a> 블로그를 참고하게 되었고 쉬운 샘플부터 점점 난이도를 높여가며 학습시키는 것이 일반화와 더 빠른 수렴에 도움이 된다는 것을 알게 되었습니다.</p>
<p>따라서 Level 1, 2, 3 세 단계를 두고 Augmented Dataset, Passage Shuffled Dataset, Sentence and Passage Shuffled Dataset 난이도별로 학습을 시켜보게 되었습니다.</p>
<p><img src="/images/image-20211110155032798.png" srcset="/img/loading.gif" lazyload alt="image-20211110155032798"></p>
<p><img src="/images/image-20211110155211405.png" srcset="/img/loading.gif" lazyload alt="image-20211110155211405"></p>
<p><img src="/images/image-20211110224114767.png" srcset="/img/loading.gif" lazyload alt="image-20211110224114767"></p>
<p>기존 가장 높은 점수를 기록했던 모델보다 약 3점의 스코어 향상이 있었고, Curriculum Learning 을 통해 얻은 결과들을 Ensemble 했을 때는 약 5점의 스코어 향상을 기록할 수 있었습니다.</p>
<h2 id="Combine-Models"><a href="#Combine-Models" class="headerlink" title="Combine Models"></a>Combine Models</h2><p>이 외에도 여러 모델을 결합한 형태로 실험을 이어갔습니다. <a target="_blank" rel="noopener" href="https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4">Combining numerical and text features in deep neural networks</a> 해당 글에서 아이디어를 얻었으며, Single model 보다는 여러 모델의 hidden state 를 concat 하여 end-to-end 로 학습시킬 때 더 좋은 퍼포먼스를 기대할 수 있지 않을까라는 생각 때문이었습니다.</p>
<p>BERT 계열의 비슷한 모델을 Combine 하는 것은 단순 Layer 를 두 배로 늘리는 것 정도의 효과밖에는 기대하지 않았기 때문에, 최대한 Model 의 Architecture 가 다른 모델들을 가지고 실험을 해보고자 했습니다.</p>
<ul>
<li>Roberta-Electra</li>
<li>Roberta-Bigbird</li>
</ul>
<p>최종적으로는 위의 두 가지 조합을 사용하게 되었습니다. 위의 조합을 선택한 이유는 아래와 같습니다.</p>
<blockquote>
<ol>
<li>Electra 의 경우 Generator 는 BERT 의 MLM 구조와 동일하지만, Discriminator 에 의해 RoBERTa 와는 다른 Representation 을 기대할 수 있을 것이다.</li>
<li>Bigbird 의 경우 Block sparse attention 을 이용하여 Roberta 보다 긴 sequence 에 강력할 것이다.</li>
</ol>
</blockquote>
<p>하지만, 단순히 hidden state 를 concat 한 뒤 정답의 position 을 추출하는 방식만으로는 backbone 의 성능을 따라갈 수 없었습니다. 서로의 Representation 들이 다르기 때문에 이미 어느정도의 Features 를 학습한 사전학습 모델을 결합하는 것으로는 좋은 효과를 도출해낼 수 없었던 것으로 예상됩니다. Scratch 부터 학습을 진행했으면 더 좋은 성능을 기대해볼 수 있었을 것으로 기대합니다.</p>
<h2 id="Focussing-on-surrounding-sentences-of-the-answer"><a href="#Focussing-on-surrounding-sentences-of-the-answer" class="headerlink" title="Focussing on surrounding sentences of the answer"></a>Focussing on surrounding sentences of the answer</h2><p>Data Augmentation 과 Custom Head 를 부착한 방식으로 이미 큰 성능 향상을 보았지만, 모델 구조를 더욱 개선할 필요를 느끼게 되었습니다. 현재 Inference 시 Retrieval 로 20 개의 질문과 유사한 문서를 찾아오도록 하는데, 이 문서들의 순서를 바꿀 때마다, 그리고 20개보다 더 많은 문서를 찾아오도록 할 때마다 모델이 다른 정답들을 예측했기 때문입니다. 따라서 Reader 가 더욱 일관된 답변을 예측할 수 있도록 하기 위해 고민하던 중 아래의 문서와 논문에서 아이디어를 얻을 수 있었습니다.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.sbert.net/">SBERT</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.01603">Bidirectional Attention Flow for Machine Comprehension</a></li>
</ul>
<blockquote>
<p>Reader 가 정답을 예측할 때 정답인 것으로 예측되는 문장과, 주변 sentence 에 집중할 수 있도록 하면 Retrieval 이 아무리 많은 문서를 가져오더라도 오답률이 줄어들 것이다.</p>
</blockquote>
<p>이 아이디어를 구현하기 위해 두 가지 방법을 떠올리게 되었습니다.</p>
<ol>
<li>Sentence BERT 를 사용하여 Question 과 Passage 에 대한 Embedding 을 RoBERTa Embedding 에 더해주자.</li>
<li>Question 과 Passage 에 대한 Embedding 으로 Cosine Similarity Score 를 구하고, 해당 Score 를 weight 로 하여 RoBERTa 의 last hidden state 에 곱해주자.</li>
</ol>
<p><img src="/images/image-20211110234505636.png" srcset="/img/loading.gif" lazyload alt="image-20211110234505636"></p>
<ul>
<li>Similarity Embedding layer 추가</li>
<li>Similarity Attention Net (with sentence-transformer’s bi-encoder question embedding, passage embedding)</li>
</ul>
<h2 id="KFold"><a href="#KFold" class="headerlink" title="KFold"></a>KFold</h2><ul>
<li><p>5folds - start, end logits mean</p>
</li>
<li></li>
</ul>
<h2 id="시도했으나-잘-안된-것들"><a href="#시도했으나-잘-안된-것들" class="headerlink" title="시도했으나 잘 안된 것들"></a>시도했으나 잘 안된 것들</h2><p>APE (Adaptive Passage Encoding)</p>
<ul>
<li>T5 Base</li>
</ul>
<p>Retrieval rerank using cross-encoder (Bi-encoder embedding -&gt; Cross-encoder Rerank)</p>
<p>Retro reader</p>
<ul>
<li> No Answer에 대한 데이터셋을 따로 구축했어야 했는데, 이 때 데이터의 스케일을 어떻게 조정해야할지 정하지 못했음. 단순히 Train set에 전체 질문들에 대해 * ground truth를 제외한 나머지 Passage를 no answer label로 만들 수 있었기 때문에 3952 * 3951 개 까지 늘릴 수 있었음. 그치만 이렇게 되면 학습하게 되는 Train passage에 대해서 너무 과적합될것 같다는 생각을 함. 그렇다고 train question에 대해 wiki에서 정답 passage를 제외한 뒤 random으로 뽑아오는 방식 도 있긴 했으나, 생각만하고 시도해보지 않았음. 충분히 가치가 있었던 것 같긴 하다. 왜냐하면 question generation 등도 있었기 때문. 다만 안한 이유를 돌이켜보면, qg, aeda 등으로 몇 배를 늘린 데이터셋에서 그렇게 만흥ㄴ 효과를 보지 못했기 때문이지 않을까 싶기도 함.</li>
</ul>
<h1 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h1><h2 id="Max-prob"><a href="#Max-prob" class="headerlink" title="Max prob"></a>Max prob</h2><h2 id="Hard-Voting"><a href="#Hard-Voting" class="headerlink" title="Hard Voting"></a>Hard Voting</h2><h2 id="Candidates"><a href="#Candidates" class="headerlink" title="Candidates"></a>Candidates</h2>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0</a>
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/KLUE_RE/">
                        <span class="hidden-mobile">KLUE - Relation Extraction</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'CheonggyeMountain-Sherpa/CheonggyeMountain-Sherpa.github.io');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
